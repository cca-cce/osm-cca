{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"module03-analyzing-text-content-natural-language-processing-30pct/3-2-3-computer-lab.qmd\"\n",
        "---\n",
        "\n",
        "## inferential analysis (part 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import os\n",
        "\n",
        "# Load sentence_df from the TSV file\n",
        "input_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\n",
        "input_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\n",
        "sentence_df = pd.read_csv(input_file_path, sep='\\t')\n",
        "\n",
        "# Load the spaCy model (small English model is used here)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize an empty list to store token data\n",
        "token_data = []\n",
        "\n",
        "# Iterate over the sentences in the sentence_df DataFrame\n",
        "for index, row in sentence_df.iterrows():\n",
        "    doc = nlp(row['sentence_text'])  # Process the sentence text with spaCy\n",
        "    \n",
        "    # Iterate over the tokens in the sentence\n",
        "    for j, token in enumerate(doc):\n",
        "        token_data.append({\n",
        "            'id': row['id'],                    # Original text ID\n",
        "            'sentence_number': row['sentence_number'],  # Sentence number\n",
        "            'token_number': j + 1,              # Token number (starting from 1)\n",
        "            'token_text': token.text,           # Token text\n",
        "            'token_lemma': token.lemma_,        # Token lemma\n",
        "            'token_pos': token.pos_,            # Token part of speech\n",
        "            'token_entity': token.ent_type_     # Token entity type (if any)\n",
        "        })\n",
        "\n",
        "# Create a new DataFrame with the token data\n",
        "token_df = pd.DataFrame(token_data)\n",
        "\n",
        "# Save the token_df DataFrame as a TSV file\n",
        "output_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\n",
        "output_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\n",
        "token_df.to_csv(output_file_path, sep='\\t', index=False)\n",
        "\n",
        "# Display the token DataFrame\n",
        "print(token_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}