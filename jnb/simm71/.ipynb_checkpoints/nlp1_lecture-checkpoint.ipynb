{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "470wj9-5cXSL"
   },
   "source": [
    "**Sustainability Communication and Greenwashing: Definitions and Analysis**\n",
    "\n",
    "*Sustainability communication* refers to how organizations convey their environmental efforts and commitments to stakeholders, including customers, investors, and the public. It encompasses the strategies and messages used to inform about sustainable practices, goals, and achievements.\n",
    "\n",
    "*Greenwashing* is the practice where an organization gives a false impression or provides misleading information about how its products are more environmentally sound. It's a form of deceptive marketing intended to present an eco-friendly image that doesn't align with reality.\n",
    "\n",
    "**The Relationship Between Sustainability Communication and Greenwashing**\n",
    "\n",
    "While sustainability communication aims to transparently share genuine environmental initiatives, greenwashing undermines this by distorting the truth. Effective sustainability communication builds trust and accountability, whereas greenwashing can damage an organization's reputation and lead to consumer skepticism.\n",
    "\n",
    "**Case Study: Preem and Vattenfall**\n",
    "\n",
    "Consider two organizations:\n",
    "\n",
    "- **Preem**: A Swedish petroleum company primarily involved in refining and selling fossil fuels.\n",
    "- **Vattenfall**: A Swedish power company engaged in energy production, including renewables like wind and hydroelectric power, as well as traditional sources.\n",
    "\n",
    "Both companies engage in sustainability communication to highlight their environmental efforts. However, their core businesses differ, which might influence the content and tone of their messages.\n",
    "\n",
    "**Using spaCy NLP to Analyze Sustainability Communication**\n",
    "\n",
    "Natural Language Processing (NLP) tools like **spaCy** can help analyze the language used in sustainability reports, press releases, and other communications from these organizations. By examining tokens (words) and parts of speech (POS), we can gain insights into their messaging strategies.\n",
    "\n",
    "Here's how we can proceed:\n",
    "\n",
    "1. **Tokenization and POS Tagging**: Using spaCy, we tokenize the text and assign POS tags to each token, identifying nouns, verbs, adjectives, etc.\n",
    "\n",
    "2. **Frequency Analysis**:\n",
    "   - **Adjectives**: Analyze the use of descriptive words such as \"sustainable,\" \"green,\" \"renewable,\" or \"clean.\" A high frequency of positive environmental adjectives might be used to enhance the company's eco-friendly image.\n",
    "   - **Nouns and Verbs**: Examine the key nouns and verbs to understand what actions the company emphasizes (e.g., \"reducing emissions,\" \"investing,\" \"innovating\").\n",
    "\n",
    "3. **Comparative Analysis**:\n",
    "   - **Preem**: As a fossil fuel company, Preem might focus on terms like \"transition,\" \"carbon capture,\" or \"efficiency.\" The language may highlight efforts to mitigate environmental impact while continuing core operations.\n",
    "   - **Vattenfall**: With a significant stake in renewables, Vattenfall's communication might frequently include words like \"wind power,\" \"sustainability,\" and \"renewable energy,\" reflecting a strategic shift toward greener energy sources.\n",
    "\n",
    "4. **Identifying Potential Greenwashing Indicators**:\n",
    "   - **Discrepancies**: Look for a mismatch between the frequency of positive environmental language and the company's actual environmental performance or core activities.\n",
    "   - **Vagueness and Jargon**: Excessive use of buzzwords without concrete actions or data might indicate greenwashing.\n",
    "   - **Overemphasis on Minor Initiatives**: Highlighting small sustainable projects while major operations remain environmentally harmful.\n",
    "\n",
    "**Determining Levels of Greenwashing**\n",
    "\n",
    "By quantitatively analyzing the language used, we can infer the level of greenwashing:\n",
    "\n",
    "- **High Level**: Predominant use of positive environmental adjectives with little mention of substantive actions or acknowledgment of environmental challenges.\n",
    "- **Moderate Level**: Balanced use of positive language and discussion of both achievements and ongoing issues.\n",
    "- **Low Level**: Transparent communication with specific data, acknowledging areas for improvement, and outlining concrete steps being taken.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Applying spaCy NLP to sustainability communications allows us to dissect the linguistic patterns and assess the authenticity of an organization's environmental messaging. By examining the parts of speech and token frequency, we can gain insights into whether companies like Preem and Vattenfall are engaging in genuine sustainability efforts or potentially greenwashing their practices.\n",
    "\n",
    "This analysis not only sheds light on their communication strategies but also promotes greater accountability and encourages more truthful and transparent sustainability reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4HjjV_8wrmo",
    "outputId": "b5a74e83-76f2-4936-83f4-89a224538f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'osm-cca-nlp'...\n",
      "remote: Enumerating objects: 89, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
      "remote: Total 89 (delta 24), reused 83 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (89/89), 17.05 MiB | 18.62 MiB/s, done.\n",
      "Resolving deltas: 100% (24/24), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cca-cce/osm-cca-nlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ORXsc2awraJ",
    "outputId": "d7aadb0c-d4ad-4656-df2a-703aeb6e775d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/5.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/5.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/5.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q pdfminer.six\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "908s_iDq9Luy",
    "outputId": "90709b0b-e42c-4dad-cfeb-ac63dfddcbdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /content/osm-cca-nlp/res/pdf/preem/raw-materials-sustainability-preem.pdf to /content/osm-cca-nlp/res/pdf/preem/raw-materials-sustainability-preem.txt\n",
      "Converted /content/osm-cca-nlp/res/pdf/preem/towards-a-climate-neutral-value-chain.pdf to /content/osm-cca-nlp/res/pdf/preem/towards-a-climate-neutral-value-chain.txt\n",
      "Converted /content/osm-cca-nlp/res/pdf/preem/carbon-capture-and-storage-preem.pdf to /content/osm-cca-nlp/res/pdf/preem/carbon-capture-and-storage-preem.txt\n",
      "Converted /content/osm-cca-nlp/res/pdf/preem/hvo-100-and-saf-sustainable-aviation-fuels-icr-project-preem.pdf to /content/osm-cca-nlp/res/pdf/preem/hvo-100-and-saf-sustainable-aviation-fuels-icr-project-preem.txt\n",
      "Converted /content/osm-cca-nlp/res/pdf/preem/production-of-renewable-diesel-synsat-project-preem.pdf to /content/osm-cca-nlp/res/pdf/preem/production-of-renewable-diesel-synsat-project-preem.txt\n",
      "Converted /content/osm-cca-nlp/res/pdf/vattenfall/un-s-sustainable-development-goals-vattenfall.pdf to /content/osm-cca-nlp/res/pdf/vattenfall/un-s-sustainable-development-goals-vattenfall.txt\n",
      "Converted /content/osm-cca-nlp/res/pdf/vattenfall/vattenfall-expands-circular-targes-on-wind-turbine-components-vattenfall.pdf to /content/osm-cca-nlp/res/pdf/vattenfall/vattenfall-expands-circular-targes-on-wind-turbine-components-vattenfall.txt\n",
      "Converted /content/osm-cca-nlp/res/pdf/vattenfall/fossil-free-fertilisers-a-key-to-cutting-carbon-emissions-vattenfall.pdf to /content/osm-cca-nlp/res/pdf/vattenfall/fossil-free-fertilisers-a-key-to-cutting-carbon-emissions-vattenfall.txt\n",
      "Converted /content/osm-cca-nlp/res/pdf/vattenfall/shifting-the-climate-narrative-vattenfall.pdf to /content/osm-cca-nlp/res/pdf/vattenfall/shifting-the-climate-narrative-vattenfall.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Directories containing the PDFs\n",
    "directories = ['organization1', 'organization2']\n",
    "directories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n",
    "\n",
    "for directory in directories:\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                text_path = os.path.splitext(pdf_path)[0] + '.txt'\n",
    "\n",
    "                try:\n",
    "                    text = extract_text(pdf_path)\n",
    "                    with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(text)\n",
    "                    print(f\"Converted {pdf_path} to {text_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to convert {pdf_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljz_yMUy0NRz"
   },
   "source": [
    "**Importing Necessary Libraries**\n",
    "\n",
    "The code begins by importing essential modules. It imports `os` for interacting with the operating system's file system and `extract_text` from `pdfminer.high_level` for extracting text content from PDF files.\n",
    "\n",
    "---\n",
    "\n",
    "**Defining the Directories Containing PDFs**\n",
    "\n",
    "Two lists named `directories` are defined. The first is a placeholder with `['organization1', 'organization2']`, and the second specifies the actual paths to the directories containing the PDF files:\n",
    "- `/content/osm-cca-nlp/res/pdf/preem`\n",
    "- `/content/osm-cca-nlp/res/pdf/vattenfall`\n",
    "\n",
    "---\n",
    "\n",
    "**Iterating Over Each Directory**\n",
    "\n",
    "The code uses a `for` loop to iterate through each directory specified in the `directories` list. This allows the program to process multiple directories sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "**Walking Through Directory Trees**\n",
    "\n",
    "Within each directory, the `os.walk(directory)` function traverses the directory tree. It yields a tuple containing the `root` path, a list of `dirs` (subdirectories), and a list of `files` in each directory.\n",
    "\n",
    "---\n",
    "\n",
    "**Identifying PDF Files**\n",
    "\n",
    "For every file in the `files` list, the code checks if the file name ends with `.pdf` (case-insensitive) using `file.lower().endswith('.pdf')`. This ensures that only PDF files are processed.\n",
    "\n",
    "---\n",
    "\n",
    "**Constructing File Paths**\n",
    "\n",
    "The full path to the PDF file is constructed using `os.path.join(root, file)`. The corresponding text file path is created by replacing the `.pdf` extension with `.txt` using `os.path.splitext(pdf_path)[0] + '.txt'`.\n",
    "\n",
    "---\n",
    "\n",
    "**Extracting Text from PDFs**\n",
    "\n",
    "A `try` block is initiated to attempt text extraction. The `extract_text(pdf_path)` function reads the content of the PDF file and stores it in the variable `text`.\n",
    "\n",
    "---\n",
    "\n",
    "**Writing Extracted Text to Files**\n",
    "\n",
    "If text extraction is successful, the code opens a new text file at `text_path` in write mode with UTF-8 encoding. It writes the extracted text into this file and then closes it, ensuring the text is saved next to the original PDF.\n",
    "\n",
    "---\n",
    "\n",
    "**Logging Successful Conversions**\n",
    "\n",
    "After successfully writing the text file, the code prints a message indicating the PDF file has been converted, using:\n",
    "```python\n",
    "print(f\"Converted {pdf_path} to {text_path}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Handling Exceptions**\n",
    "\n",
    "An `except` block catches any exceptions that occur during the extraction or writing process. If an error occurs, it prints a failure message with the path of the PDF file and the exception details:\n",
    "```python\n",
    "print(f\"Failed to convert {pdf_path}: {e}\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLkKh1xC_qcT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Directories containing the text files\n",
    "directories = ['organization1', 'organization2']\n",
    "directories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n",
    "\n",
    "data = []\n",
    "text_index = 1\n",
    "\n",
    "# Allowed characters: alphabetic, punctuation, and whitespace\n",
    "allowed_chars = set(string.ascii_letters + string.punctuation + string.whitespace)\n",
    "\n",
    "for directory in directories:\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                folder_name = os.path.basename(root)\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    raw_text = f.read()\n",
    "\n",
    "                # Keep only allowed characters\n",
    "                clean_text = ''.join(c for c in raw_text if c in allowed_chars)\n",
    "\n",
    "                # Replace sequences of whitespace with a single space\n",
    "                clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "\n",
    "                # Trim leading and trailing whitespace\n",
    "                clean_text = clean_text.strip()\n",
    "\n",
    "                data.append({\n",
    "                    'text_index': text_index,\n",
    "                    'file_path': file_path,\n",
    "                    'folder_name': folder_name,\n",
    "                    'raw_text': raw_text,\n",
    "                    'clean_text': clean_text\n",
    "                })\n",
    "\n",
    "                text_index += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df_texts = pd.DataFrame(data, columns=['text_index', 'file_path', 'folder_name', 'raw_text', 'clean_text'])\n",
    "\n",
    "# Save DataFrame to TSV file\n",
    "df_texts.to_csv('df_texts.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rQuvNkf1Dqw"
   },
   "source": [
    "**Importing Additional Libraries**\n",
    "\n",
    "The code imports `pandas` for data manipulation, `re` for regular expressions, and `string` for accessing string constants. These libraries provide functionalities for data storage, text processing, and character handling.\n",
    "\n",
    "---\n",
    "\n",
    "**Initializing Data Structures**\n",
    "\n",
    "An empty list named `data` is initialized to store dictionaries of processed text information. A variable `text_index` is set to `1` to assign a unique numerical index to each text file processed.\n",
    "\n",
    "---\n",
    "\n",
    "**Defining Allowed Characters**\n",
    "\n",
    "A set called `allowed_chars` is created containing all alphabetic characters, punctuation marks, and whitespace characters. This set is used to filter out unwanted characters from the text content.\n",
    "\n",
    "---\n",
    "\n",
    "**Processing Text Files**\n",
    "\n",
    "The code iterates through the directories, searching for files that end with `.txt`. For each text file found, it performs the following steps:\n",
    "\n",
    "---\n",
    "\n",
    "**Reading Raw Text**\n",
    "\n",
    "The text file is opened using UTF-8 encoding, and its content is read into the variable `raw_text`. This variable holds the original text extracted from the file.\n",
    "\n",
    "---\n",
    "\n",
    "**Cleaning Text Content**\n",
    "\n",
    "1. **Filtering Allowed Characters:** The code constructs `clean_text` by including only characters present in `allowed_chars`, effectively removing any non-alphabetic, non-punctuation, and non-whitespace characters.\n",
    "\n",
    "2. **Normalizing Whitespace:** It uses a regular expression to replace multiple whitespace characters (`\\s+`) with a single space, ensuring consistent spacing in the text.\n",
    "\n",
    "3. **Trimming Whitespace:** Leading and trailing whitespace are removed from `clean_text` using the `strip()` method.\n",
    "\n",
    "---\n",
    "\n",
    "**Collecting Processed Data**\n",
    "\n",
    "A dictionary containing the following keys is appended to the `data` list:\n",
    "- `'text_index'`: The unique index number for the text file.\n",
    "- `'file_path'`: The full path to the text file.\n",
    "- `'folder_name'`: The name of the folder containing the file.\n",
    "- `'raw_text'`: The original text content from the file.\n",
    "- `'clean_text'`: The cleaned and processed text content.\n",
    "\n",
    "After appending, `text_index` is incremented to prepare for the next file.\n",
    "\n",
    "---\n",
    "\n",
    "**Creating a DataFrame**\n",
    "\n",
    "The list `data` is converted into a pandas DataFrame named `df_texts`. The DataFrame includes the specified columns: `'text_index'`, `'file_path'`, `'folder_name'`, `'raw_text'`, and `'clean_text'`.\n",
    "\n",
    "---\n",
    "\n",
    "**Saving DataFrame to TSV File**\n",
    "\n",
    "The DataFrame `df_texts` is saved to a TSV (Tab-Separated Values) file named `'df_texts.tsv'`. The `index=False` parameter ensures that the DataFrame's index is not written to the file, keeping the output clean.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGyJ6MfZ17XC"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1MSz01fAP4v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "data = []\n",
    "\n",
    "for idx, row in df_texts.iterrows():\n",
    "    text_index = row['text_index']\n",
    "    folder_name = row['folder_name']\n",
    "    clean_text = row['clean_text']\n",
    "\n",
    "    # Process the clean text to identify sentences\n",
    "    doc = nlp(clean_text)\n",
    "\n",
    "    sentence_index = 1\n",
    "    for sent in doc.sents:\n",
    "        sentence_text = sent.text\n",
    "\n",
    "        # Squeeze, trim, and convert sequences of whitespace to single spaces\n",
    "        sentence_text = re.sub(r'\\s+', ' ', sentence_text).strip()\n",
    "\n",
    "        # Check if the sentence exceeds five words\n",
    "        if len(sentence_text.split()) > 5:\n",
    "            data.append({\n",
    "                'text_index': text_index,\n",
    "                'folder_name': folder_name,\n",
    "                'sentence_index': sentence_index,\n",
    "                'sentence_text': sentence_text\n",
    "            })\n",
    "            sentence_index += 1\n",
    "\n",
    "# Create the DataFrame\n",
    "df_sentences = pd.DataFrame(data, columns=['text_index', 'folder_name', 'sentence_index', 'sentence_text'])\n",
    "\n",
    "# Save the DataFrame to a TSV file\n",
    "df_sentences.to_csv('df_sentences.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQaUN57F1eLe"
   },
   "source": [
    "**Iterating Over Cleaned Text Data**\n",
    "\n",
    "The code begins by iterating over each row in the `df_texts` DataFrame using `df_texts.iterrows()`. For every entry, it extracts the `text_index`, `folder_name`, and `clean_text` fields. This setup allows the script to process each cleaned text from the dataset individually.\n",
    "\n",
    "---\n",
    "\n",
    "**Processing Text with spaCy to Identify Sentences**\n",
    "\n",
    "For each `clean_text`, the code applies the spaCy English language model to create a `doc` object:\n",
    "```python\n",
    "doc = nlp(clean_text)\n",
    "```\n",
    "This object contains linguistic annotations, enabling the extraction of sentences (`doc.sents`) from the text.\n",
    "\n",
    "---\n",
    "\n",
    "**Extracting and Cleaning Sentences**\n",
    "\n",
    "The script initializes a `sentence_index` to keep track of sentence positions. It then iterates over each sentence in `doc.sents`. For every sentence, it retrieves the sentence text using `sent.text` and performs cleaning operations:\n",
    "- **Normalizing Whitespace:** Uses a regular expression to replace multiple whitespace characters with a single space:\n",
    "  ```python\n",
    "  sentence_text = re.sub(r'\\s+', ' ', sentence_text).strip()\n",
    "  ```\n",
    "- **Trimming:** Removes leading and trailing whitespace with `.strip()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Filtering Sentences by Word Count**\n",
    "\n",
    "Before including a sentence in the dataset, the code checks if it contains more than five words:\n",
    "```python\n",
    "if len(sentence_text.split()) > 5:\n",
    "```\n",
    "This condition filters out short sentences that may not be informative, ensuring that only substantial sentences are included in the analysis.\n",
    "\n",
    "---\n",
    "\n",
    "**Collecting Processed Sentence Data**\n",
    "\n",
    "If a sentence meets the word count criterion, the code appends a dictionary to the `data` list containing:\n",
    "- `'text_index'`: The identifier of the original text from which the sentence was extracted.\n",
    "- `'folder_name'`: The name of the folder containing the source text.\n",
    "- `'sentence_index'`: A sequential number indicating the sentence's order within the text.\n",
    "- `'sentence_text'`: The cleaned and processed sentence text.\n",
    "\n",
    "After appending, the `sentence_index` is incremented to prepare for the next sentence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tvh3zfvmVtir",
    "outputId": "1e55f595-2e54-411a-bd1b-4793794c850d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data = []\n",
    "\n",
    "for idx, row in df_sentences.iterrows():\n",
    "    text_index = row['text_index']\n",
    "    folder_name = row['folder_name']\n",
    "    sentence_index = row['sentence_index']\n",
    "    sentence_text = row['sentence_text']\n",
    "\n",
    "    doc = nlp(sentence_text)\n",
    "\n",
    "    token_index = 1\n",
    "    for token in doc:\n",
    "        # Exclude tokens that are not actual words and are not stopwords\n",
    "        if token.is_alpha and token.text.lower() not in stop_words:\n",
    "            token_text = token.text\n",
    "            token_lemma = token.lemma_\n",
    "            token_pos = token.pos_\n",
    "            token_entity = token.ent_type_ if token.ent_type_ else 'O'  # 'O' for no entity\n",
    "\n",
    "            data.append({\n",
    "                'text_index': text_index,\n",
    "                'folder_name': folder_name,\n",
    "                'sentence_index': sentence_index,\n",
    "                'token_index': token_index,\n",
    "                'token_text': token_text,\n",
    "                'token_lemma': token_lemma,\n",
    "                'token_pos': token_pos,\n",
    "                'token_entity': token_entity\n",
    "            })\n",
    "\n",
    "            token_index += 1\n",
    "\n",
    "# Create the DataFrame\n",
    "df_tokens = pd.DataFrame(data, columns=[\n",
    "    'text_index', 'folder_name', 'sentence_index',\n",
    "    'token_index', 'token_text', 'token_lemma', 'token_pos', 'token_entity'\n",
    "])\n",
    "\n",
    "# Save the DataFrame to a TSV file\n",
    "df_tokens.to_csv('df_tokens.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx1QfmJU18jH"
   },
   "source": [
    "**Importing NLTK and Downloading Stopwords**\n",
    "\n",
    "The code imports `nltk` and `stopwords` from `nltk.corpus`. It then calls `nltk.download('stopwords')` to ensure that the NLTK stopwords corpus is downloaded and available. This step is crucial for filtering out common words (like \"the\", \"and\", \"is\") that may not be meaningful for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "**Getting English Stopwords from NLTK**\n",
    "\n",
    "A set of English stopwords is created by calling `stopwords.words('english')` and converting it to a set for efficient lookup. This set, stored in `stop_words`, will be used to filter out common English words during tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "**Iterating Over Sentences in df_sentences**\n",
    "\n",
    "The code iterates over each row in the `df_sentences` DataFrame using `df_sentences.iterrows()`. For every sentence, it extracts relevant information such as `text_index`, `folder_name`, `sentence_index`, and `sentence_text`. This setup prepares the data for tokenization at the sentence level.\n",
    "\n",
    "---\n",
    "\n",
    "**Processing Sentences with spaCy to Extract Tokens**\n",
    "\n",
    "Each `sentence_text` is processed using the spaCy English model to create a `doc` object:\n",
    "```python\n",
    "doc = nlp(sentence_text)\n",
    "```\n",
    "This object contains tokens with rich linguistic annotations, enabling detailed analysis of each word in the sentence.\n",
    "\n",
    "---\n",
    "\n",
    "**Filtering Tokens**\n",
    "\n",
    "The code iterates over each `token` in the `doc` object. It applies filters to include only tokens that:\n",
    "- Consist of alphabetic characters (`token.is_alpha`).\n",
    "- Are not in the set of English stopwords (`token.text.lower() not in stop_words`).\n",
    "\n",
    "This ensures that only meaningful, non-common words are included in the analysis.\n",
    "\n",
    "---\n",
    "\n",
    "**Extracting Token Attributes**\n",
    "\n",
    "For each token that passes the filters, the code extracts various linguistic attributes:\n",
    "- `token_text`: The original text of the token.\n",
    "- `token_lemma`: The lemmatized form of the token, which reduces the word to its base or dictionary form.\n",
    "- `token_pos`: The part-of-speech tag, indicating the grammatical role of the token (e.g., noun, verb).\n",
    "- `token_entity`: The named entity type if the token is part of a recognized entity; otherwise, it defaults to `'O'` for \"no entity.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Collecting Token Data**\n",
    "\n",
    "A dictionary containing the extracted token information is appended to the `data` list. The dictionary includes:\n",
    "- `'text_index'`: The identifier of the original text.\n",
    "- `'folder_name'`: The name of the folder containing the source text.\n",
    "- `'sentence_index'`: The position of the sentence within the text.\n",
    "- `'token_index'`: A sequential number indicating the token's order within the sentence.\n",
    "- `'token_text'`, `'token_lemma'`, `'token_pos'`, `'token_entity'`: The linguistic attributes of the token.\n",
    "\n",
    "After appending, `token_index` is incremented to prepare for the next token.\n",
    "\n",
    "---\n",
    "\n",
    "**Creating df_tokens DataFrame**\n",
    "\n",
    "The list `data` is converted into a pandas DataFrame named `df_tokens`. The DataFrame includes the specified columns:\n",
    "```python\n",
    "['text_index', 'folder_name', 'sentence_index', 'token_index', 'token_text', 'token_lemma', 'token_pos', 'token_entity']\n",
    "```\n",
    "This structured format allows for easy manipulation and analysis of the token data.\n",
    "\n",
    "---\n",
    "\n",
    "**Saving df_tokens DataFrame to a TSV File**\n",
    "\n",
    "The DataFrame `df_tokens` is saved to a TSV (Tab-Separated Values) file named `'df_tokens.tsv'`. The `index=False` parameter ensures that the DataFrame's index is not written to the file, keeping the output clean and focused on the relevant data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB-QpuoZt_lu"
   },
   "source": [
    "By following these steps, we've cleaned, processed, and structured the text data from both organizations. The named entity recognition analysis has provided insights into the differences in their sustainability communications, highlighting unique entities and focal points for each organization."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
