{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cED93ePCoeSv"
   },
   "source": [
    "# 3-1-2: Reading text into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zoj-n7Gy9aw"
   },
   "source": [
    "## Download text data and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXAXc4b84LTA",
    "outputId": "30bfee0d-1255-4020-a252-2bbaddf05e44"
   },
   "outputs": [],
   "source": [
    "!rm -rf *.zip osm-cca-* 2>/dev/null\n",
    "!git clone https://github.com/cca-cce/osm-cca-nlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PhxVeSu4XBa",
    "outputId": "6327adab-0d30-4709-9672-d2038cb281f6"
   },
   "outputs": [],
   "source": [
    "!pip install -q pdfminer.six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMfc2YnHzNRM"
   },
   "source": [
    "## Convert PDF to plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hq69QTqq4dUg",
    "outputId": "56adaafa-4678-4ade-edb9-9ff52055a2ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Directories containing the PDFs\n",
    "directories = ['organization1', 'organization2']\n",
    "directories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n",
    "\n",
    "for directory in directories:\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                text_path = os.path.splitext(pdf_path)[0] + '.txt'\n",
    "\n",
    "                try:\n",
    "                    text = extract_text(pdf_path)\n",
    "                    with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(text)\n",
    "                    print(f\"Converted {pdf_path} to {text_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to convert {pdf_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGAf9EiMzVxc"
   },
   "source": [
    "## Read plain text to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "abeW3jYooeSw",
    "outputId": "74b237a8-d77f-4421-e52e-0498a8f2d4a0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Directories containing the text files\n",
    "directories = ['organization1', 'organization2']\n",
    "directories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n",
    "\n",
    "data = []\n",
    "text_index = 1\n",
    "\n",
    "# Allowed characters: alphabetic, punctuation, and whitespace\n",
    "allowed_chars = set(string.ascii_letters + string.punctuation + string.whitespace)\n",
    "\n",
    "for directory in directories:\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                folder_name = os.path.basename(root)\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    raw_text = f.read()\n",
    "\n",
    "                # Keep only allowed characters\n",
    "                clean_text = ''.join(c for c in raw_text if c in allowed_chars)\n",
    "\n",
    "                # Replace sequences of whitespace with a single space\n",
    "                clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "\n",
    "                # Trim leading and trailing whitespace\n",
    "                clean_text = clean_text.strip()\n",
    "\n",
    "                data.append({\n",
    "                    'text_index': text_index,\n",
    "                    'file_path': file_path,\n",
    "                    'folder_name': folder_name,\n",
    "                    'raw_text': raw_text,\n",
    "                    'clean_text': clean_text\n",
    "                })\n",
    "\n",
    "                text_index += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df_texts = pd.DataFrame(data, columns=['text_index', 'file_path', 'folder_name', 'raw_text', 'clean_text'])\n",
    "\n",
    "# Save DataFrame to TSV file\n",
    "df_texts.to_csv('df_texts.tsv', sep='\\t', index=False)\n",
    "\n",
    "# View text dataframe\n",
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1eozWoToeSx"
   },
   "source": [
    "# 3-1-3: Descriptive text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41gT1lTV0r0l"
   },
   "source": [
    "## Count texts, words, and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "EgHzY6bS-DYT",
    "outputId": "ca322238-e913-4e29-ad5b-aa47a60a3464"
   },
   "outputs": [],
   "source": [
    "# Add new columns for text statistics\n",
    "df_texts['text_name'] = df_texts['file_path'].apply(os.path.basename)\n",
    "\n",
    "# Word and character counts\n",
    "df_texts['word_count'] = df_texts['clean_text'].apply(lambda x: len(x.split()))\n",
    "df_texts['char_count'] = df_texts['clean_text'].apply(len)\n",
    "\n",
    "# Mean characters per word (avoid division by zero)\n",
    "df_texts['mean_chars_per_word'] = df_texts.apply(\n",
    "    lambda row: row['char_count'] / row['word_count'] if row['word_count'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Select relevant columns for descriptive output\n",
    "df_stats = df_texts[['text_name', 'word_count', 'char_count', 'mean_chars_per_word']]\n",
    "\n",
    "# View the stats table\n",
    "df_stats\n",
    "\n",
    "# Optional: summary of word and character stats\n",
    "#summary = df_stats[['word_count', 'char_count', 'mean_chars_per_word']].describe()\n",
    "#print(\"\\nDescriptive summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6yuTtq20XQy"
   },
   "source": [
    "## Download pre-trained English language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83QGAzuAoeSx",
    "outputId": "ab71660b-676b-4af8-99d4-191279cb215e"
   },
   "outputs": [],
   "source": [
    "# for more languages, check https://spacy.io/models\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-33FgoGoeSx"
   },
   "source": [
    "# 3-2-2: Inferential text analysis, tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv4wlV_X0c6u"
   },
   "source": [
    "## Split text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sot2-He0_MoE",
    "outputId": "50a34139-9a83-44a3-9f1a-be675157e7e4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "data = []\n",
    "\n",
    "for idx, row in df_texts.iterrows():\n",
    "    text_index = row['text_index']\n",
    "    folder_name = row['folder_name']\n",
    "    clean_text = row['clean_text']\n",
    "\n",
    "    # Process the clean text to identify sentences\n",
    "    doc = nlp(clean_text)\n",
    "\n",
    "    sentence_index = 1\n",
    "    for sent in doc.sents:\n",
    "        sentence_text = sent.text\n",
    "\n",
    "        # Squeeze, trim, and convert sequences of whitespace to single spaces\n",
    "        sentence_text = re.sub(r'\\s+', ' ', sentence_text).strip()\n",
    "\n",
    "        # Check if the sentence exceeds five words\n",
    "        if len(sentence_text.split()) > 5:\n",
    "            data.append({\n",
    "                'text_index': text_index,\n",
    "                'folder_name': folder_name,\n",
    "                'sentence_index': sentence_index,\n",
    "                'sentence_text': sentence_text\n",
    "            })\n",
    "            sentence_index += 1\n",
    "\n",
    "# Create the DataFrame\n",
    "df_sentences = pd.DataFrame(data, columns=['text_index', 'folder_name', 'sentence_index', 'sentence_text'])\n",
    "\n",
    "# Save the DataFrame to a TSV file\n",
    "df_sentences.to_csv('df_sentences.tsv', sep='\\t', index=False)\n",
    "\n",
    "# View sentence dataframe\n",
    "df_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmMw8RP-0jk8"
   },
   "source": [
    "## Split sentences into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "LRBxoKnAoeSy",
    "outputId": "cc66493d-e7df-41b4-9d2a-e9a3f9ce9e43"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get NLTK English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 1: Extract base token data (text + lemma)\n",
    "data = []\n",
    "\n",
    "for idx, row in df_sentences.iterrows():\n",
    "    text_index = row['text_index']\n",
    "    folder_name = row['folder_name']\n",
    "    sentence_index = row['sentence_index']\n",
    "    sentence_text = row['sentence_text']\n",
    "\n",
    "    doc = nlp(sentence_text)\n",
    "\n",
    "    token_index = 1\n",
    "    for token in doc:\n",
    "        if token.is_alpha and token.text.lower() not in stop_words:\n",
    "            data.append({\n",
    "                'text_index': text_index,\n",
    "                'folder_name': folder_name,\n",
    "                'sentence_index': sentence_index,\n",
    "                'token_index': token_index,\n",
    "                'token_text': token.text,\n",
    "                'token_lemma': token.lemma_\n",
    "            })\n",
    "            token_index += 1\n",
    "\n",
    "# Create base token dataframe\n",
    "df_tokens = pd.DataFrame(data)\n",
    "\n",
    "# Preview\n",
    "df_tokens.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNKulZnsoeSy"
   },
   "source": [
    "# 3-2-3: Inferential text analysis, POS and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6KjPWgP1j8f"
   },
   "source": [
    "## Part of speech analysis (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "yY9wyMBc_OOm",
    "outputId": "2d0ea242-f08c-4faf-b66b-495ecef1b9ab"
   },
   "outputs": [],
   "source": [
    "# Step 2: Add POS tags\n",
    "pos_tags = []\n",
    "\n",
    "for idx, row in df_sentences.iterrows():\n",
    "    doc = nlp(row['sentence_text'])\n",
    "    pos_tags.extend([\n",
    "        token.pos_\n",
    "        for token in doc\n",
    "        if token.is_alpha and token.text.lower() not in stop_words\n",
    "    ])\n",
    "\n",
    "# Append POS column\n",
    "df_tokens['token_pos'] = pos_tags\n",
    "\n",
    "# Preview\n",
    "df_tokens.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKJFa8yK1oJ_"
   },
   "source": [
    "## Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8V77FufWoeSy",
    "outputId": "859bb369-0a91-4bc7-b15b-83650cabf86f"
   },
   "outputs": [],
   "source": [
    "# Step 3: Add entity types\n",
    "entity_types = []\n",
    "\n",
    "for idx, row in df_sentences.iterrows():\n",
    "    doc = nlp(row['sentence_text'])\n",
    "    entity_types.extend([\n",
    "        token.ent_type_ if token.ent_type_ else 'O'\n",
    "        for token in doc\n",
    "        if token.is_alpha and token.text.lower() not in stop_words\n",
    "    ])\n",
    "\n",
    "# Append entity column\n",
    "df_tokens['token_entity'] = entity_types\n",
    "\n",
    "# Save full token dataframe\n",
    "df_tokens.to_csv('df_tokens.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Preview\n",
    "df_tokens.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE2jIyFRoeSz"
   },
   "source": [
    "# 3-3-2: Summarizing results of text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utQtmJpr2H9U"
   },
   "source": [
    "## Summarize data (select, filter, aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "ZgFII_CzoeSz",
    "outputId": "20507700-b3b8-4bf7-c675-ac89ae6b68dd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the POS tags we're interested in\n",
    "pos_tags = ['NOUN', 'PROPN', 'VERB', 'ADJ']\n",
    "\n",
    "# Filter df_tokens to include only the desired POS tags\n",
    "filtered_df = df_tokens[df_tokens['token_pos'].isin(pos_tags)]\n",
    "\n",
    "# Group by folder_name, token_pos, and token_lemma, and count occurrences\n",
    "grouped = (\n",
    "    filtered_df.groupby(['folder_name', 'token_pos', 'token_lemma'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "# For each folder_name and token_pos, get the top 10 most frequent token_lemmas\n",
    "df_tokens_frequency = (\n",
    "    grouped.groupby(['folder_name', 'token_pos'])\n",
    "    .apply(lambda x: x.nlargest(10, 'count'))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Optionally, sort the dataframe for clarity\n",
    "df_tokens_frequency = df_tokens_frequency.sort_values(\n",
    "    ['folder_name', 'token_pos', 'count'], ascending=[True, True, False]\n",
    ")\n",
    "\n",
    "# Save df_tokens_frequency to TSV file\n",
    "df_tokens_frequency.to_csv('df_tokens_frequency.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Display the result\n",
    "df_tokens_frequency.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVYnOAotoeSz"
   },
   "source": [
    "# 3-3-3: Visualizing results of text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC29YdNm2PAU"
   },
   "source": [
    "## Visualize results (stacked bar plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_RumX6PoeS0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load or reuse the frequency data\n",
    "# df_tokens_frequency = pd.read_csv('df_tokens_frequency.tsv', sep='\\t')\n",
    "\n",
    "# Increase default font size\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "# Store unique folder names\n",
    "folder_names = df_tokens_frequency['folder_name'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7m1fKKzyB0RC"
   },
   "outputs": [],
   "source": [
    "def plot_token_pos_frequencies(df, token_pos_label):\n",
    "    df_tp = df[df['token_pos'] == token_pos_label]\n",
    "    unique_tokens = sorted(df_tp['token_lemma'].unique())[:20]\n",
    "    if len(unique_tokens) < 20:\n",
    "        unique_tokens.extend([''] * (20 - len(unique_tokens)))\n",
    "\n",
    "    plot_data = pd.DataFrame({'token_lemma': unique_tokens})\n",
    "\n",
    "    for folder_name in folder_names:\n",
    "        df_folder = df_tp[df_tp['folder_name'] == folder_name]\n",
    "        token_counts = df_folder.set_index('token_lemma')['count']\n",
    "        plot_data[folder_name] = plot_data['token_lemma'].map(token_counts).fillna(0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ind = np.arange(len(unique_tokens))\n",
    "    bottom = np.zeros(len(unique_tokens))\n",
    "    colors = plt.cm.Set2.colors\n",
    "    color_cycle = colors[:len(folder_names)]\n",
    "\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    for i, folder_name in enumerate(folder_names):\n",
    "        counts = plot_data[folder_name].values\n",
    "        bars = ax.bar(ind, counts, bottom=bottom, color=color_cycle[i], label=folder_name)\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2,\n",
    "                    bar.get_y() + height / 2,\n",
    "                    int(height),\n",
    "                    ha='center',\n",
    "                    va='center',\n",
    "                    fontsize=16\n",
    "                )\n",
    "        bottom += counts\n",
    "\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(plot_data['token_lemma'], rotation=90)\n",
    "    ax.set_title(f\"Token Frequencies for {token_pos_label}\", fontsize=28)\n",
    "    ax.set_xlabel('Token Lemma', fontsize=24)\n",
    "    ax.set_ylabel('Frequency', fontsize=24)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.legend(title='Folder Name', fontsize=20, title_fontsize=22)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"df_tokens_frequency_{token_pos_label}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "AHycHrH8CnfL",
    "outputId": "514bc700-8614-4a51-c72c-ed8fc9b8482f"
   },
   "outputs": [],
   "source": [
    "plot_token_pos_frequencies(df_tokens_frequency, \"NOUN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "xH5pBSeKCr-R",
    "outputId": "4b02e924-7588-4f16-e8d8-ac943bf4f5e6"
   },
   "outputs": [],
   "source": [
    "plot_token_pos_frequencies(df_tokens_frequency, \"VERB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "5IFIf8nYCyqO",
    "outputId": "cb81401e-3a8d-4547-9da4-a4173f94445e"
   },
   "outputs": [],
   "source": [
    "plot_token_pos_frequencies(df_tokens_frequency, \"ADJ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "gUq5fnbYC3Em",
    "outputId": "5b3323c9-492c-41f7-8412-40d864e2109d"
   },
   "outputs": [],
   "source": [
    "plot_token_pos_frequencies(df_tokens_frequency, \"PROPN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8QIOcm5DNgS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
