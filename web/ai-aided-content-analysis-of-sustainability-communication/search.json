[
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#sustainability-communication-content",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#sustainability-communication-content",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Sustainability communication content",
    "text": "Sustainability communication content\n\nDefine sustainability content both formally and functionally\nFormal content reflects environmental or social responsibility themes\nFunctional content supports branding, compliance, and stakeholder engagement\nCommunication influences perceptions of credibility and authenticity\nDual analysis reveals intent, structure, and audience impact"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#online-content-sampling",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#online-content-sampling",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Online content sampling",
    "text": "Online content sampling\n\nSampling method influences research reliability and validity\nRandom sampling is ideal but hard to apply in web contexts\nPurposive sampling targets relevant or contrasting examples\nStrategic sampling balances focus and generalizability\nDesign choices must minimize bias in content selection"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#expectations-about-sustainability-communication",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#expectations-about-sustainability-communication",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Expectations about sustainability communication",
    "text": "Expectations about sustainability communication\n\nChoose between qualitative, quantitative, or mixed approaches\nDescriptive analysis explores patterns; explanatory seeks causes\nLinguistic analysis compares tone, terms, and rhetorical strategies\nResearch begins with well-formed questions and hypotheses\nMethodological clarity improves interpretability and rigor"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#find-online-sustainability-communication",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#find-online-sustainability-communication",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Find online sustainability communication",
    "text": "Find online sustainability communication\n\nSelect organizations aligned with your research objectives\nStudy one or compare multiple for contrast and depth\nTarget official websites as curated communication sources\nIdentify and extract sustainability-related URLs\nBuild a dataset from key pages like “CSR” or “Sustainability”"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#convert-multimodal-web-pages-to-pdf",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#convert-multimodal-web-pages-to-pdf",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Convert multimodal web pages to PDF",
    "text": "Convert multimodal web pages to PDF\n\nUse browser print-to-PDF to capture entire web pages\nPreserves images, layout, and static design elements\nEnables analysis of multimodal communication forms\nQuick, user-friendly method requiring no programming\nLimitation: videos and interactive elements not captured"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#setting-up-an-initial-content-database",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#setting-up-an-initial-content-database",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Setting up an initial content database",
    "text": "Setting up an initial content database\n\nEnsure all collected content is stored in an accessible format\nLocal storage is easy but lacks shareability and backup\nCloud solutions like Google Drive support collaboration\nGitHub adds version control and traceability benefits\nA well-structured archive supports long-term analysis"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#organize-communication-content-in-folders",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#organize-communication-content-in-folders",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Organize communication content in folders",
    "text": "Organize communication content in folders\n\nSeparate folders allow comparisons between organizations\nNormalize filenames and paths for consistent processing\nFolder structure simplifies automation and batch analysis\nPDFs often need OCR or text extraction for further use\nGood organization supports transparency and reproducibility"
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture.html",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture.html",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nCompared to NLP—where tokens provide explicit, discrete units—images present an inference problem because pixels carry no intrinsic semantics. Computer vision must learn meaning from spatial patterns of color and intensity that are only indirectly related to concepts like “tree,” “factory,” or “logo.” The advantage is that pixels are naturally ordered in two (or more) dimensions, so convolutional and attention-based models can exploit locality and shape to identify structures that would be opaque to bag-of-words text models. In practice, classification maps an image (or region) to one or more labels, while object detection jointly locates and names multiple instances, turning raw arrays into interpretable units that can be aggregated for social-scientific analysis.\n\n\n\nEmpirical material often arrives as images extracted from PDFs, scraped from websites, or sampled as frames from YouTube videos; these can be analyzed to study visual framing in sustainability communication. Visual greenwashing can be investigated by quantifying cues such as nature imagery, color palettes suggestive of “green,” and the co-occurrence of ecological symbols with weak or non-verifiable claims. Beyond (or alongside) such quantitative indicators, computer vision can support qualitative constructs like problem–solution framing, risk vs. opportunity emphasis, or the prominence of corporate vs. community actors. While face recognition, emotion detection, and demographic inference (e.g., via packages like DeepFace) are technically feasible, they raise significant ethical and legal considerations—consent, bias, and fairness—that must be addressed in research design and reporting.\n\n\n\nGoogle Colab enables rapid prototyping without local setup, offering managed GPUs/TPUs and ephemeral environments that are ideal for teaching and exploratory work with large models. Through Hugging Face, students can access a wide range of pretrained unimodal and multimodal models (e.g., vision encoders, vision–language models) with a few lines of code. This facilitates experiments with content embeddings and text–image similarity—linking images to semantic prompts (e.g., “offshore wind,” “carbon capture”) to test hypotheses about visual themes. Although Colab’s sessions are temporary and resource-limited, the trade-off favors reproducibility and accessibility: notebooks capture dependencies, code, and outputs in a shareable, open-science workflow.\n\n\n\nImage classification assigns one or more labels to an input: binary classification distinguishes presence vs. absence (e.g., “dog vs. not-dog”), multiclass chooses a single label from many, and multilabel allows multiple simultaneous categories. This task is analogous to supervised text labeling but operates over spatial features rather than tokens. For sustainability datasets, classifiers can distinguish natural objects (trees, turbines, smoke plumes) from graphical elements (icons, logos, infographics) to separate photographic evidence from designed symbolism. Careful curation of training data, class balance, and threshold calibration is required to ensure that predicted labels reflect communicative content rather than spurious correlations.\n\n\n\nA practical pipeline iterates over collections of images—folders of campaign assets or frames extracted from video at fixed intervals—classifying each item and storing results in a dataframe. Each row can record filename, timestamp (for video frames), top-k predicted labels, and confidence scores, along with metadata such as source organization or platform. This tabular structure enables filtering, grouping, and statistical comparison across campaigns and time. Challenges include multi-label images (e.g., turbines and corporate logos in the same frame) and long-tail classes; solutions involve per-label thresholds, hierarchical taxonomies, and aggregation rules that preserve nuance while keeping the analysis tractable.\n\n\n\nObject detection extends classification by localizing multiple instances within a single image, outputting bounding boxes (or masks) with class labels. This is crucial when scenes contain many relevant elements—wind turbines, solar panels, people, vehicles—whose counts, sizes, and positions carry communicative meaning. However, dense scenes, occlusions, and small objects stress detectors, and class imbalance can skew results toward frequent categories. From a social-science perspective, detection enables measures like the salience of nature vs. industrial artifacts, co-presence of actors (e.g., company representatives with communities), and spatial arrangements that suggest responsibility, agency, or impact.\n\n\n\nLocalization can be represented by bounding boxes (rectangles around objects) or by pixel-accurate segmentation masks; the latter improves measurement of size, overlap, and shape but is computationally heavier. In video or live streams, tracking adds temporal continuity, allowing researchers to analyze persistence and transitions of visual motifs across shots. Confidence scores quantify model uncertainty and should be monitored and filtered (e.g., via class-specific thresholds, non-maximum suppression settings) to reduce false positives. Reporting confidence distributions and quality checks—inter-annotator validation, spot audits—supports transparent, replicable claims about visual patterns in sustainability communication.",
    "crumbs": [
      "module 4",
      "lesson 4.2",
      "video lecture 4.2.1"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture.html#lesson-4.2-image-classification-and-object-detection",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture.html#lesson-4.2-image-classification-and-object-detection",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nCompared to NLP—where tokens provide explicit, discrete units—images present an inference problem because pixels carry no intrinsic semantics. Computer vision must learn meaning from spatial patterns of color and intensity that are only indirectly related to concepts like “tree,” “factory,” or “logo.” The advantage is that pixels are naturally ordered in two (or more) dimensions, so convolutional and attention-based models can exploit locality and shape to identify structures that would be opaque to bag-of-words text models. In practice, classification maps an image (or region) to one or more labels, while object detection jointly locates and names multiple instances, turning raw arrays into interpretable units that can be aggregated for social-scientific analysis.\n\n\n\nEmpirical material often arrives as images extracted from PDFs, scraped from websites, or sampled as frames from YouTube videos; these can be analyzed to study visual framing in sustainability communication. Visual greenwashing can be investigated by quantifying cues such as nature imagery, color palettes suggestive of “green,” and the co-occurrence of ecological symbols with weak or non-verifiable claims. Beyond (or alongside) such quantitative indicators, computer vision can support qualitative constructs like problem–solution framing, risk vs. opportunity emphasis, or the prominence of corporate vs. community actors. While face recognition, emotion detection, and demographic inference (e.g., via packages like DeepFace) are technically feasible, they raise significant ethical and legal considerations—consent, bias, and fairness—that must be addressed in research design and reporting.\n\n\n\nGoogle Colab enables rapid prototyping without local setup, offering managed GPUs/TPUs and ephemeral environments that are ideal for teaching and exploratory work with large models. Through Hugging Face, students can access a wide range of pretrained unimodal and multimodal models (e.g., vision encoders, vision–language models) with a few lines of code. This facilitates experiments with content embeddings and text–image similarity—linking images to semantic prompts (e.g., “offshore wind,” “carbon capture”) to test hypotheses about visual themes. Although Colab’s sessions are temporary and resource-limited, the trade-off favors reproducibility and accessibility: notebooks capture dependencies, code, and outputs in a shareable, open-science workflow.\n\n\n\nImage classification assigns one or more labels to an input: binary classification distinguishes presence vs. absence (e.g., “dog vs. not-dog”), multiclass chooses a single label from many, and multilabel allows multiple simultaneous categories. This task is analogous to supervised text labeling but operates over spatial features rather than tokens. For sustainability datasets, classifiers can distinguish natural objects (trees, turbines, smoke plumes) from graphical elements (icons, logos, infographics) to separate photographic evidence from designed symbolism. Careful curation of training data, class balance, and threshold calibration is required to ensure that predicted labels reflect communicative content rather than spurious correlations.\n\n\n\nA practical pipeline iterates over collections of images—folders of campaign assets or frames extracted from video at fixed intervals—classifying each item and storing results in a dataframe. Each row can record filename, timestamp (for video frames), top-k predicted labels, and confidence scores, along with metadata such as source organization or platform. This tabular structure enables filtering, grouping, and statistical comparison across campaigns and time. Challenges include multi-label images (e.g., turbines and corporate logos in the same frame) and long-tail classes; solutions involve per-label thresholds, hierarchical taxonomies, and aggregation rules that preserve nuance while keeping the analysis tractable.\n\n\n\nObject detection extends classification by localizing multiple instances within a single image, outputting bounding boxes (or masks) with class labels. This is crucial when scenes contain many relevant elements—wind turbines, solar panels, people, vehicles—whose counts, sizes, and positions carry communicative meaning. However, dense scenes, occlusions, and small objects stress detectors, and class imbalance can skew results toward frequent categories. From a social-science perspective, detection enables measures like the salience of nature vs. industrial artifacts, co-presence of actors (e.g., company representatives with communities), and spatial arrangements that suggest responsibility, agency, or impact.\n\n\n\nLocalization can be represented by bounding boxes (rectangles around objects) or by pixel-accurate segmentation masks; the latter improves measurement of size, overlap, and shape but is computationally heavier. In video or live streams, tracking adds temporal continuity, allowing researchers to analyze persistence and transitions of visual motifs across shots. Confidence scores quantify model uncertainty and should be monitored and filtered (e.g., via class-specific thresholds, non-maximum suppression settings) to reduce false positives. Reporting confidence distributions and quality checks—inter-annotator validation, spot audits—supports transparent, replicable claims about visual patterns in sustainability communication.",
    "crumbs": [
      "module 4",
      "lesson 4.2",
      "video lecture 4.2.1"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-2",
    "href": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-2",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "lesson 4-2",
    "text": "lesson 4-2\n\nvideo-lecture.qmd:Image classification and object detection\n\n\ncomputer-lab.qmd:Inferential image analysis, classification\n\n\ncomputer-lab.qmd:Inferential image analysis, object detection",
    "crumbs": [
      "module 4"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-3",
    "href": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-3",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "lesson 4-3",
    "text": "lesson 4-3\n\nvideo-lecture.qmd:Interpreting the results of CV analysis\n\n\ncomputer-lab.qmd:Summarizing results of image analysis\n\n\ncomputer-lab.qmd:Visualizing results of image analysis",
    "crumbs": [
      "module 4"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#image-classification-and-object-detection",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#image-classification-and-object-detection",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Image classification and object detection",
    "text": "Image classification and object detection\n\nUnlike NLP tokens with explicit semantics, image pixels lack intrinsic meaning.\nComputer vision must infer meaning from spatial patterns of color, intensity, and shape.\nPixel ordering in two dimensions lets convolution and attention exploit locality and structure.\nClassification assigns one or more labels to an image or region based on learned patterns.\nObject detection jointly locates and names multiple instances to produce analyzable units."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#applications-in-sustainability-communication",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#applications-in-sustainability-communication",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Applications in sustainability communication",
    "text": "Applications in sustainability communication\n\nResearch images often come from PDFs, websites, and sampled YouTube frames.\nVisual greenwashing can be assessed by quantifying nature cues and symbolic green color palettes.\nComputer vision can operationalize qualitative frames such as problem–solution or risk versus opportunity.\nAnalyses can measure the prominence of corporate versus community actors in visuals.\nFace, affect, and demographic inference are feasible but raise consent, bias, and fairness concerns."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#install-computer-vision-models-in-colab",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#install-computer-vision-models-in-colab",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Install computer vision models in Colab",
    "text": "Install computer vision models in Colab\n\nColab provides managed GPUs and zero setup for fast prototyping with large models.\nHugging Face offers pretrained vision and vision–language models accessible with minimal code.\nThese tools enable experiments with content embeddings and text–image similarity to test themes.\nNotebooks capture code, dependencies, and outputs to support shareable open-science workflows.\nEphemeral sessions and resource limits are trade-offs that favor accessibility and reproducibility."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#inferential-image-analysis-classification",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#inferential-image-analysis-classification",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Inferential image analysis, classification",
    "text": "Inferential image analysis, classification\n\nImage classification assigns binary, multiclass, or multilabel categories to an input image.\nThe task parallels supervised text labeling but relies on spatial features instead of token sequences.\nSustainability classifiers can separate natural objects from graphical elements to distinguish evidence from symbolism.\nRobust training requires balanced datasets, careful curation, and calibrated decision thresholds.\nValid labels should reflect communicative content rather than spurious correlations or artifacts."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#iterate-classification-to-dataframe",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#iterate-classification-to-dataframe",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Iterate classification to dataframe",
    "text": "Iterate classification to dataframe\n\nBuild pipelines that iterate over image collections or video frames sampled at fixed intervals.\nStore per-item results in a dataframe with filenames, timestamps, labels, and confidence scores.\nTabular outputs enable filtering, grouping, and statistical comparisons across campaigns and time.\nMulti-label scenes and long-tail classes require per-label thresholds and hierarchical taxonomies.\nAggregation rules should preserve nuance while keeping analyses tractable and interpretable."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#inferential-image-analysis-object-detection",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#inferential-image-analysis-object-detection",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Inferential image analysis, object detection",
    "text": "Inferential image analysis, object detection\n\nObject detection localizes and labels multiple instances within an image using boxes or masks.\nCounting and sizing detected elements provides indicators of salience and composition in sustainability scenes.\nDense scenes, occlusion, and small objects remain challenging and can degrade recall and precision.\nClass imbalance can bias detectors toward frequent categories unless mitigated in training and postprocessing.\nDetection supports measures such as co-presence of actors and spatial arrangements of industry versus nature."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#object-localization-and-confidence-scores",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#object-localization-and-confidence-scores",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Object localization and confidence scores",
    "text": "Object localization and confidence scores\n\nBounding boxes provide rectangular localization while segmentation masks deliver pixel-accurate shapes.\nVideo and streaming analyses benefit from tracking to capture persistence and transitions over time.\nConfidence scores quantify uncertainty and should be filtered with class-specific thresholds and NMS.\nReporting confidence distributions and validation checks increases transparency and reproducibility.\nFiltering low-confidence detections reduces false positives and improves the reliability of conclusions."
  },
  {
    "objectID": "0-2-resources.html",
    "href": "0-2-resources.html",
    "title": "Resources",
    "section": "",
    "text": "Weder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "0-2-resources.html#literature",
    "href": "0-2-resources.html#literature",
    "title": "Resources",
    "section": "",
    "text": "Weder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "0-2-resources.html#websites-and-apps",
    "href": "0-2-resources.html#websites-and-apps",
    "title": "Resources",
    "section": "websites and apps",
    "text": "websites and apps\n\nhttps://www.python.org/downloads/\nhttps://wiki.python.org/moin/BeginnersGuide\nhttps://www.anaconda.com/products/individual\nhttps://code.visualstudio.com/download\nhttps://github.com/jupyterlab/jupyterlab_app#download\nhttps://trinket.io/home",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "0-2-resources.html#online-articles",
    "href": "0-2-resources.html#online-articles",
    "title": "Resources",
    "section": "online articles",
    "text": "online articles\n\nsome text here Kedia and Rasu (2020)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "0-1-course-intro.html",
    "href": "0-1-course-intro.html",
    "title": "Welcome",
    "section": "",
    "text": "script slides",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "0-1-course-intro.html#mooc-welcome-address",
    "href": "0-1-course-intro.html#mooc-welcome-address",
    "title": "Welcome",
    "section": "mooc welcome address",
    "text": "mooc welcome address\nWelcome to AI-Aided Content Analysis of Sustainability Communication—an online course designed to equip you with cutting-edge tools to explore one of the most pressing challenges of our time: how sustainability is communicated, interpreted, and shaped through digital content.",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "0-1-course-intro.html#personal-presentation",
    "href": "0-1-course-intro.html#personal-presentation",
    "title": "Welcome",
    "section": "personal presentation",
    "text": "personal presentation\nMy name is Nils Holmberg and i work at the Dept. of Communication at Lund University, where i teach and research on the topics of computational content analysis and cognitive communication effects.",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "0-1-course-intro.html#paragraph-1",
    "href": "0-1-course-intro.html#paragraph-1",
    "title": "Welcome",
    "section": "paragraph 1",
    "text": "paragraph 1\nWhether you’re a Lund University student advancing your coursework in media studies, environmental science, or digital methods, or a researcher or learner joining us from across the globe, this course opens new doors into how we can see, read, and interpret sustainability communication at scale. And if you’re a curious member of the public—perhaps with an interest in climate issues, artificial intelligence, or media literacy—you’ll find this course a window into how machines can help us make sense of the messages that shape our collective future.",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "0-1-course-intro.html#paragraph-2",
    "href": "0-1-course-intro.html#paragraph-2",
    "title": "Welcome",
    "section": "paragraph 2",
    "text": "paragraph 2\nFrom the very start, you’ll get hands-on experience building your own content database—learning how to find, collect, and structure sustainability communication from real-world websites. Through accessible, low-code Python notebooks running in Google Colab, you’ll be empowered to run your first analyses with minimal technical barriers. Want to see how organizations talk about climate goals? Curious how different companies portray sustainability through visuals? You’ll learn to read and analyze both text and image content, using powerful tools from natural language processing and computer vision.",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "0-1-course-intro.html#paragraph-3",
    "href": "0-1-course-intro.html#paragraph-3",
    "title": "Welcome",
    "section": "paragraph 3",
    "text": "paragraph 3\nFor Lund University students, this course offers specific methodological training that can be applied directly to thesis work or project assignments. You’ll gain not only technical fluency but also a critical understanding of how to document your methods and communicate your findings transparently—skills that align with open science values and interdisciplinary research goals.",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "0-1-course-intro.html#paragraph-5",
    "href": "0-1-course-intro.html#paragraph-5",
    "title": "Welcome",
    "section": "paragraph 5",
    "text": "paragraph 5\nAnd for members of the general public, this course demystifies AI and data analysis. You won’t just learn what these models do—you’ll learn how to question them: How does automated analysis shape what we see in sustainability reports? Where do ethical boundaries lie? And how can content analysis be a tool for both transparency and critique?",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "0-1-course-intro.html#paragraph-6",
    "href": "0-1-course-intro.html#paragraph-6",
    "title": "Welcome",
    "section": "paragraph 6",
    "text": "paragraph 6\nBy the end of this course, you’ll be able to extract insights from digital content, summarize and visualize patterns in sustainability communication, and engage with critical discussions about the ethics of AI-assisted research. Whether you’re here to meet a curriculum requirement, advance your own research, or explore a personal passion—you’re in the right place.\nWelcome aboard. Let’s begin.",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#python-for-social-science-data-analysis",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#python-for-social-science-data-analysis",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python for Social Science Data Analysis",
    "text": "Python for Social Science Data Analysis\n\n\n\nOverview of data analysis types: descriptive, inferential, exploratory\nEmphasis on quantitative methods used in social sciences\nIntroduction to computational content analysis (text/image)\nBenefits of using Python for flexible, scalable analysis\nCommon libraries: pandas, statsmodels, matplotlib, scikit-learn"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#ai-aided-quantitative-analysis-with-python",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#ai-aided-quantitative-analysis-with-python",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "AI-Aided Quantitative Analysis with Python",
    "text": "AI-Aided Quantitative Analysis with Python\n\nUse large language models (LLMs) to generate statistical code\nQuery data directly using natural language prompts\nAutomate hypothesis testing and model fitting\nReduce entry barrier for non-programmers\nBalance between manual control and AI guidance"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#ai-aided-computational-content-analysis",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#ai-aided-computational-content-analysis",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "AI-Aided Computational Content Analysis",
    "text": "AI-Aided Computational Content Analysis\n\nApply LLMs for text classification, summarization, and topic modeling\nUse image models (e.g., CLIP, ViT) for analyzing visual content\nCombine text and visual features for rich content analysis\nAutomate entity recognition and theme detection\nUse AI to scale analysis of large corpora"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#low-code-data-analysis-with-tables",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#low-code-data-analysis-with-tables",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Low-Code Data Analysis with Tables",
    "text": "Low-Code Data Analysis with Tables\n\nLoad and view datasets using pandas DataFrames\nPerform filtering, grouping, and summary stats with minimal code\nContrast high-code scripts vs. low-code AI-assisted workflows\nUse AI tools to generate and interpret table outputs\nExample tools: pandasgui, datatable, ChatGPT + CSV"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#data-transformations-with-ai-assistance",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#data-transformations-with-ai-assistance",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Data Transformations with AI Assistance",
    "text": "Data Transformations with AI Assistance\n\nSelect relevant variables using prompts or code\nFilter observations based on conditions (e.g., time, value ranges)\nAggregate data for summaries (e.g., mean by group)\nCompare traditional pandas syntax vs. AI-assisted queries\nLearn transformation logic through interactive AI feedback"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#low-code-data-analysis-with-graphs",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#low-code-data-analysis-with-graphs",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Low-Code Data Analysis with Graphs",
    "text": "Low-Code Data Analysis with Graphs\n\nCreate basic visualizations (e.g., bar, line, histogram)\nUnderstand univariate (one variable) vs. bivariate (two variables) plots\nUse AI to suggest and generate appropriate plot types\nExample libraries: matplotlib, seaborn, plotnine\nVisualize data trends and relationships with minimal setup"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#multivariate-and-interactive-graphs",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#multivariate-and-interactive-graphs",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Multivariate and Interactive Graphs",
    "text": "Multivariate and Interactive Graphs\n\nVisualize relationships between three or more variables\nUse scatter matrices, heatmaps, or bubble charts\nAdd interactivity with plotly, altair, or holoviews\nUse AI to refine visual layouts and variable selections\nEnable dynamic exploration of patterns in large datasets"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture.html",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture.html",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nIn the social sciences, data analysis takes many forms—including descriptive statistics, inferential testing, and exploratory modeling. Python provides a flexible and scalable environment for performing all of these, with a growing ecosystem tailored to academic and applied research. One emerging area is computational content analysis, where researchers apply Python tools to analyze text and visual content systematically. Libraries such as pandas, statsmodels, matplotlib, and scikit-learn enable everything from data wrangling to advanced modeling, making Python an essential tool for quantitative social scientists.\n\n\n\n\nArtificial intelligence—especially large language models—now supports researchers in performing quantitative analysis more efficiently. These models can generate Python code on demand for statistical tasks, allowing users to interact with their data through natural language. This reduces the technical barrier for non-programmers while enabling rapid hypothesis testing, data cleaning, and model generation. The balance between manual coding and AI-assisted scripting gives researchers more control and efficiency, particularly when iterating through complex analytical workflows.\n\n\n\n\nAI is also transforming how we approach content analysis. Large language models can assist in extracting meaning from text through classification, summarization, or topic modeling. Meanwhile, vision models like CLIP or ViT enable analysis of image and video content. Together, these tools support multimodal analysis, where textual and visual data are examined in tandem. Researchers can automate tasks such as entity recognition and theme detection, significantly speeding up the analysis of large, complex corpora while still allowing for nuanced interpretation.\n\n\n\n\nPython makes it easy to read and manipulate tabular data, especially using the pandas library. With AI-assisted tools, users can perform complex filtering, grouping, and aggregation with simple prompts rather than verbose code. This low-code approach is ideal for exploratory data analysis, especially in non-technical fields. Tools like pandasgui, datatable, or conversational agents like ChatGPT can generate, execute, and explain table operations in real-time. This democratizes access to data insights and facilitates reproducibility.\n\n\n\n\nTransforming data—through selection, filtering, and aggregation—is foundational to any analysis workflow. In Python, these operations are traditionally handled with pandas, but AI tools can now suggest or execute them via natural language instructions. This makes it easier for users to identify the variables they need, set conditions for filtering, and compute summaries like group means or totals. By comparing AI-generated queries to traditional code, learners gain a deeper understanding of the logic behind data manipulation.\n\n\n\n\nVisualization is a key part of understanding and communicating data. Python supports a wide range of basic plotting options, including bar charts, line graphs, and histograms. With AI assistance, users can describe the kind of graph they want and receive ready-to-run code. This enables quick generation of univariate and bivariate plots that explore single variables or pairwise relationships. Libraries such as matplotlib, seaborn, and plotnine provide powerful tools for crafting clear, insightful visualizations with minimal setup.\n\n\n\n\nWhen analysis involves more than two variables, visualization becomes more complex—and more powerful. Python offers tools to create multivariate plots like scatter matrices, heatmaps, and bubble charts. For dynamic exploration, libraries like plotly, altair, and holoviews enable interactive features such as zooming, filtering, and tooltip displays. AI support can refine these visualizations by adjusting parameters or recommending plot types based on data structure. This helps users engage deeply with high-dimensional data and uncover hidden patterns.",
    "crumbs": [
      "module 2",
      "lesson 2.2",
      "video lecture 2.2.1"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture.html#lesson-2.2-python-and-ai-aided-data-analysis",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture.html#lesson-2.2-python-and-ai-aided-data-analysis",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nIn the social sciences, data analysis takes many forms—including descriptive statistics, inferential testing, and exploratory modeling. Python provides a flexible and scalable environment for performing all of these, with a growing ecosystem tailored to academic and applied research. One emerging area is computational content analysis, where researchers apply Python tools to analyze text and visual content systematically. Libraries such as pandas, statsmodels, matplotlib, and scikit-learn enable everything from data wrangling to advanced modeling, making Python an essential tool for quantitative social scientists.\n\n\n\n\nArtificial intelligence—especially large language models—now supports researchers in performing quantitative analysis more efficiently. These models can generate Python code on demand for statistical tasks, allowing users to interact with their data through natural language. This reduces the technical barrier for non-programmers while enabling rapid hypothesis testing, data cleaning, and model generation. The balance between manual coding and AI-assisted scripting gives researchers more control and efficiency, particularly when iterating through complex analytical workflows.\n\n\n\n\nAI is also transforming how we approach content analysis. Large language models can assist in extracting meaning from text through classification, summarization, or topic modeling. Meanwhile, vision models like CLIP or ViT enable analysis of image and video content. Together, these tools support multimodal analysis, where textual and visual data are examined in tandem. Researchers can automate tasks such as entity recognition and theme detection, significantly speeding up the analysis of large, complex corpora while still allowing for nuanced interpretation.\n\n\n\n\nPython makes it easy to read and manipulate tabular data, especially using the pandas library. With AI-assisted tools, users can perform complex filtering, grouping, and aggregation with simple prompts rather than verbose code. This low-code approach is ideal for exploratory data analysis, especially in non-technical fields. Tools like pandasgui, datatable, or conversational agents like ChatGPT can generate, execute, and explain table operations in real-time. This democratizes access to data insights and facilitates reproducibility.\n\n\n\n\nTransforming data—through selection, filtering, and aggregation—is foundational to any analysis workflow. In Python, these operations are traditionally handled with pandas, but AI tools can now suggest or execute them via natural language instructions. This makes it easier for users to identify the variables they need, set conditions for filtering, and compute summaries like group means or totals. By comparing AI-generated queries to traditional code, learners gain a deeper understanding of the logic behind data manipulation.\n\n\n\n\nVisualization is a key part of understanding and communicating data. Python supports a wide range of basic plotting options, including bar charts, line graphs, and histograms. With AI assistance, users can describe the kind of graph they want and receive ready-to-run code. This enables quick generation of univariate and bivariate plots that explore single variables or pairwise relationships. Libraries such as matplotlib, seaborn, and plotnine provide powerful tools for crafting clear, insightful visualizations with minimal setup.\n\n\n\n\nWhen analysis involves more than two variables, visualization becomes more complex—and more powerful. Python offers tools to create multivariate plots like scatter matrices, heatmaps, and bubble charts. For dynamic exploration, libraries like plotly, altair, and holoviews enable interactive features such as zooming, filtering, and tooltip displays. AI support can refine these visualizations by adjusting parameters or recommending plot types based on data structure. This helps users engage deeply with high-dimensional data and uncover hidden patterns.",
    "crumbs": [
      "module 2",
      "lesson 2.2",
      "video lecture 2.2.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#functions-of-texts-in-sustainability-communication",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#functions-of-texts-in-sustainability-communication",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Functions of Texts in Sustainability Communication",
    "text": "Functions of Texts in Sustainability Communication\n\n\n\nInformational texts provide data-driven insights and factual details.\n\nPersuasive texts motivate audiences toward action or change.\n\nNarrative texts create emotional connections to sustainability themes.\n\nVisual-supported texts enhance accessibility and engagement.\n\nEach text type targets specific audiences and communication goals."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#nlp-and-challenges-of-unstructured-text",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#nlp-and-challenges-of-unstructured-text",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "NLP and Challenges of Unstructured Text",
    "text": "NLP and Challenges of Unstructured Text\n\n\n\nNLP applications include sentiment analysis, translation, and summarization.\n\nUnstructured text often contains ambiguous language and incomplete sentences.\n\nDomain-specific jargon and colloquialisms complicate processing.\n\nNoise in data sources like social media adds layers of preprocessing needs.\n\nRobust algorithms and preprocessing pipelines mitigate these challenges."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#basic-concepts-units-tokens-and-n-grams",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#basic-concepts-units-tokens-and-n-grams",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Basic Concepts: Units, Tokens, and N-grams",
    "text": "Basic Concepts: Units, Tokens, and N-grams\n\n\n\nText units include sentences, words, and characters as analytical building blocks.\n\nTokens are the smallest logical units, often derived from words.\n\nN-grams capture sequences of n tokens, revealing contextual patterns.\n\nCommon n-grams include bigrams (two words) and trigrams (three words).\n\nThese concepts underpin more advanced NLP tasks."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#formats-and-conversion-to-plain-text",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#formats-and-conversion-to-plain-text",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Formats and Conversion to Plain Text",
    "text": "Formats and Conversion to Plain Text\n\n\n\nText is often stored in formats like PDF, HTML, or Markdown.\n\nPDFs may contain layout artifacts, complicating text extraction.\n\nHTML requires parsing to remove tags and extract meaningful content.\n\nTools like Beautiful Soup and PDFMiner streamline these conversions.\n\nConverting to plain text ensures compatibility with NLP workflows.\n\n\n\n\n\n📰"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#text-features-readability-pos-and-ner",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#text-features-readability-pos-and-ner",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Features: Readability, POS, and NER",
    "text": "Text Features: Readability, POS, and NER\n\n\n\nReadability indices assess the complexity of written content.\n\nPOS tagging categorizes words by their grammatical function.\n\nNER identifies and classifies specific entities, such as names and dates.\n\nThese features provide insights into the style and structure of text.\n\nThey are essential for contextual and thematic understanding in NLP."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#reading-text-into-dataframes-and-preprocessing",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#reading-text-into-dataframes-and-preprocessing",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Reading Text into Dataframes and Preprocessing",
    "text": "Reading Text into Dataframes and Preprocessing\n\n\n\nDataframes structure text data for analysis and visualization.\n\nNormalization includes lowercasing and punctuation removal.\n\nTokenization splits text into analyzable units like words or phrases.\n\nPandas and NLTK are widely used for preprocessing workflows.\n\nClean, tokenized data is a prerequisite for most NLP tasks.\n\n\n\n\n\n🧮"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#manifest-text-content-and-frequency-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#manifest-text-content-and-frequency-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Manifest Text Content and Frequency Analysis",
    "text": "Manifest Text Content and Frequency Analysis\n\n\n\nManifest content refers to explicitly observable text elements.\n\nSentence and word counts provide quantitative content metrics.\n\nWord frequency analysis highlights key terms and dominant themes.\n\nVisualizations like word clouds offer intuitive insights into text data.\n\nThese metrics are foundational for exploratory text analysis.\n\n\n\n\n\n📊"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-2",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-2",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "lesson 3-2",
    "text": "lesson 3-2\n\nvideo-lecture.qmd:Part-of-speech and named entity recognition\n\n\ncomputer-lab.qmd:Inferential text analysis, tokenization\n\n\ncomputer-lab.qmd:Inferential text analysis, POS and NER",
    "crumbs": [
      "module 3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-3",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-3",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "lesson 3-3",
    "text": "lesson 3-3\n\nvideo-lecture.qmd:Interpreting the results of NLP analysis\n\n\ncomputer-lab.qmd:Summarizing results of text analysis\n\n\ncomputer-lab.qmd:Visualizing results of text analysis",
    "crumbs": [
      "module 3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#quantitative-content-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#quantitative-content-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Quantitative content analysis",
    "text": "Quantitative content analysis\n\n\n\nSystematically evaluates text features like theme frequency and sentiment.\n\nText-level (e.g., tone) differs from word-level (e.g., keyword counts).\n\nHuman coders offer flexibility but lack scalability for large datasets.\n\nAI-aided coding sacrifices nuance for rapid, scalable analysis.\n\nComplements qualitative analysis, validating insights with numerical rigor.\n\n\n\n\n\n\n\n📰"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#operationalizing-sustainability",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#operationalizing-sustainability",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Operationalizing sustainability",
    "text": "Operationalizing sustainability\n\n\n\nDistinguishes authentic sustainability from greenwashing via metrics.\n\nAuthentic communication uses specific, measurable environmental metrics.\n\nGreenwashing features vague terms and unverifiable claims.\n\nNamed entity recognition maps entities and their relationships.\n\nPart-of-speech analysis reveals intent through nouns, verbs, adjectives.\n\n\n\n\n\n\n\n♻️"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#comparison-across-organizations",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#comparison-across-organizations",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Comparison across organizations",
    "text": "Comparison across organizations\n\n\n\nCompares Preem (fossil fuel) and Vattenfall (renewable energy).\n\nFossil fuel firms emphasize mitigation; renewables highlight innovation.\n\nNLP detects differences in word choice and narrative tone.\n\nPublic scrutiny shapes fossil fuel firms’ defensive messaging.\n\nQuantifies alignment with sustainability goals across sectors.\n\n\n\n\n\n\n\n⚖️"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#summarizing-results-of-text-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#summarizing-results-of-text-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Summarizing results of text analysis",
    "text": "Summarizing results of text analysis\n\n\n\nSimplifies complex token-level dataframes for better readability.\n\nAggregates metrics like content category counts or sentiment scores.\n\nAnalyzes dependent variables (e.g., category frequency) against independents.\n\nHighlights trends, e.g., adjective use by organization type.\n\nEnsures findings are actionable for diverse stakeholders.\n\n\n\n\n\n\n\n📈"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#select-filter-aggregate",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#select-filter-aggregate",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Select, filter, aggregate",
    "text": "Select, filter, aggregate\n\n\n\nSelects key columns like token entity or part-of-speech tags.\n\nFilters out noise, e.g., null values or low-frequency tokens.\n\nAggregates data to compute category counts or sentiment means.\n\nEnables precise comparisons, e.g., sustainability terms by organization.\n\nTransforms raw data into structured, research-ready insights.\n\n\n\n\n\n\n\n🎯"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#visualizing-results-of-text-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#visualizing-results-of-text-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Visualizing results of text analysis",
    "text": "Visualizing results of text analysis\n\n\n\nVisualizations make NLP results intuitive compared to tables.\n\nOptions include bar plots, word clouds, and heatmaps.\n\nSimple visuals (e.g., bar plots) are clearer than complex ones.\n\nAI tools like Matplotlib streamline visualization processes.\n\nHighlights trends, e.g., term frequency differences across firms.\n\n\n\n\n\n\n\n🎨"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#stacked-bar-plots",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#stacked-bar-plots",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Stacked bar plots",
    "text": "Stacked bar plots\n\n\n\nSimple bar plots show single-organization metrics like category frequency.\n\nStacked bar plots compare multiple variables across organizations.\n\nSegments represent variables (e.g., word types) within bars.\n\nReveals differences, e.g., adjective use in Preem vs. Vattenfall.\n\nSupports multivariate analysis for clear, comparative insights.\n\n\n\n\n\n📊"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nQuantitative content analysis systematically evaluates content features in texts, such as frequency of themes, sentiment, or specific terms, to derive measurable insights. Text-level features, like overall tone or narrative structure, contrast with word-level features, such as keyword counts or lexical diversity. Traditionally, human coders offer flexibility in interpreting nuanced meanings but face limitations in scalability due to time and resource constraints. Conversely, AI-aided coding, leveraging natural language processing (NLP), sacrifices some interpretive flexibility for enhanced scalability, enabling rapid analysis of large datasets. This approach excels in identifying patterns, such as sustainability-related terms across corporate reports. Quantitative content analysis complements qualitative content analysis by providing numerical rigor to validate or challenge qualitative interpretations. For instance, while qualitative analysis might explore the context of sustainability claims, quantitative methods count their occurrences or measure their prominence, offering a robust foundation for comparing communication strategies across organizations. Integrating both approaches ensures a comprehensive understanding of sustainability communication, balancing depth with breadth in analyzing textual data.\n\n\n\nOperationalizing sustainability in NLP analysis involves defining metrics to distinguish authentic sustainability communication from greenwashing, addressing the research question of communication integrity. Authentic sustainability communication is characterized by specific, measurable indicators, such as detailed environmental impact metrics or commitments to renewable energy. In contrast, greenwashing may feature vague terms, exaggerated claims, or lack of verifiable data. NLP techniques like named entity recognition (NER) identify key entities (e.g., organizations, policies) and their relationships, revealing networks of accountability or obfuscation. Part-of-speech (POS) analysis further dissects texts by examining nouns (e.g., “emissions”), verbs (e.g., “reduce”), and adjectives (e.g., “sustainable”), which signal intent or emphasis. For example, authentic communication might use precise verbs like “implemented” rather than ambiguous ones like “aimed.” By quantifying these linguistic elements, researchers can systematically evaluate the credibility of sustainability claims. This approach enables a structured comparison of corporate narratives, ensuring that sustainability communication is not only rhetorically compelling but also substantively grounded in actionable and transparent practices.\n\n\n\nComparing sustainability communication across organizations, such as Preem (fossil fuel) and Vattenfall (renewable energy), reveals distinct rhetorical strategies shaped by their operational contexts. Fossil fuel companies like Preem may emphasize mitigation efforts, such as carbon capture, to counter environmental criticism, while renewable energy firms like Vattenfall might highlight innovation and clean energy achievements. These differences manifest in word choice, thematic focus, and narrative tone, detectable through NLP analysis. For instance, Preem’s texts might feature terms like “efficiency” or “transition,” whereas Vattenfall’s could prioritize “renewable” or “zero-emission.” Quantitative analysis of these features, such as term frequency or sentiment scores, allows researchers to map organizational priorities and assess alignment with sustainability goals. Expected differences also stem from public perception pressures: fossil fuel companies face greater scrutiny, potentially leading to defensive or compensatory messaging. By systematically comparing these patterns, NLP analysis provides evidence-based insights into how organizational type influences sustainability communication, enabling stakeholders to evaluate authenticity and strategic intent across diverse energy sectors.\n\n\n\nSummarizing NLP results transforms complex token-level analysis dataframes, which detail individual words or entities, into accessible insights. These raw dataframes, often dense with columns like token frequency or entity type, are challenging to interpret directly. Generating a new dataframe with summary statistics simplifies this by aggregating key metrics, such as content category counts (e.g., “sustainability” vs. “innovation” mentions) or sentiment measurements. The dependent variable, such as the frequency of a content category, is analyzed against independent variables like word type (e.g., nouns vs. adjectives) or organizational type (e.g., fossil fuel vs. renewable). For example, a summary dataframe might reveal that renewable energy firms use more positive adjectives than fossil fuel companies. This aggregated view highlights trends and differences without overwhelming stakeholders with granular data. By focusing on high-level patterns, summarizing ensures that findings are actionable, facilitating communication of results to diverse audiences, from researchers to corporate decision-makers, while retaining the analytical rigor of the original NLP process.\n\n\n\nSelecting, filtering, and aggregating data are critical steps in refining NLP datasets for meaningful interpretation. Selecting relevant columns, such as token entity (e.g., “Preem” or “carbon”) or part-of-speech tags (e.g., “adjective”), focuses analysis on variables pertinent to sustainability communication. Filtering removes irrelevant or incomplete data, such as rows with null values or tokens below a minimum frequency threshold, ensuring data quality. For instance, excluding low-count tokens reduces noise from rare or insignificant terms. Aggregation then computes summary metrics, such as category counts (e.g., frequency of “renewable” mentions) or measurement means (e.g., average sentiment score) per organization. This process might reveal, for example, that Vattenfall’s texts contain higher counts of “sustainability” than Preem’s. By systematically narrowing and synthesizing the dataset, these steps transform raw NLP outputs into structured insights. This enables researchers to address specific research questions, such as comparing authentic sustainability claims, with clarity and precision, supporting robust conclusions about organizational communication strategies.\n\n\n\nData visualization enhances the interpretability of NLP results, offering a more intuitive alternative to summary tables. By representing complex data—such as term frequencies or sentiment scores—visually, charts and graphs make patterns immediately apparent. Options include bar plots, word clouds, or heatmaps, each suited to different insights (e.g., word clouds for keyword prominence, heatmaps for correlations). Simple visualizations, like bar plots comparing sustainability term counts across organizations, are often more effective than complex ones, as they avoid overwhelming viewers. AI-aided data analysis streamlines visualization by automating data processing and integrating tools like Python’s Matplotlib or Seaborn, reducing manual effort. For instance, a bar plot might vividly contrast Preem’s focus on “efficiency” with Vattenfall’s on “renewable energy,” making differences accessible to non-technical stakeholders. Visualizations thus bridge the gap between raw NLP outputs and actionable insights, enabling researchers to communicate findings effectively while highlighting key trends in sustainability communication with clarity and impact.\n\n\n\nStacked bar plots are an effective tool for visualizing NLP results, particularly for comparing sustainability communication across organizations. For a single organization, simple bar plots can display metrics like the frequency of content categories (e.g., “sustainability” vs. “innovation”). Stacked bar plots extend this to bivariate or multivariate analysis by showing multiple variables within each bar, such as the proportion of nouns, verbs, and adjectives in sustainability-related texts across organizations like Preem and Vattenfall. Each segment of the bar represents a variable (e.g., word type), with the total bar height indicating overall frequency. This approach highlights differences, such as Vattenfall’s higher use of positive adjectives compared to Preem’s noun-heavy focus on “emissions.” Stacked bar plots thus provide a clear, comparative view of complex data, making it easier to identify patterns and trends. By visualizing multivariate relationships intuitively, they support researchers in communicating nuanced findings about organizational sustainability narratives to diverse audiences, from academics to industry stakeholders.",
    "crumbs": [
      "module 3",
      "lesson 3.3",
      "video lecture 3.3.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture.html#lesson-3.3-interpreting-the-results-of-nlp-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture.html#lesson-3.3-interpreting-the-results-of-nlp-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nQuantitative content analysis systematically evaluates content features in texts, such as frequency of themes, sentiment, or specific terms, to derive measurable insights. Text-level features, like overall tone or narrative structure, contrast with word-level features, such as keyword counts or lexical diversity. Traditionally, human coders offer flexibility in interpreting nuanced meanings but face limitations in scalability due to time and resource constraints. Conversely, AI-aided coding, leveraging natural language processing (NLP), sacrifices some interpretive flexibility for enhanced scalability, enabling rapid analysis of large datasets. This approach excels in identifying patterns, such as sustainability-related terms across corporate reports. Quantitative content analysis complements qualitative content analysis by providing numerical rigor to validate or challenge qualitative interpretations. For instance, while qualitative analysis might explore the context of sustainability claims, quantitative methods count their occurrences or measure their prominence, offering a robust foundation for comparing communication strategies across organizations. Integrating both approaches ensures a comprehensive understanding of sustainability communication, balancing depth with breadth in analyzing textual data.\n\n\n\nOperationalizing sustainability in NLP analysis involves defining metrics to distinguish authentic sustainability communication from greenwashing, addressing the research question of communication integrity. Authentic sustainability communication is characterized by specific, measurable indicators, such as detailed environmental impact metrics or commitments to renewable energy. In contrast, greenwashing may feature vague terms, exaggerated claims, or lack of verifiable data. NLP techniques like named entity recognition (NER) identify key entities (e.g., organizations, policies) and their relationships, revealing networks of accountability or obfuscation. Part-of-speech (POS) analysis further dissects texts by examining nouns (e.g., “emissions”), verbs (e.g., “reduce”), and adjectives (e.g., “sustainable”), which signal intent or emphasis. For example, authentic communication might use precise verbs like “implemented” rather than ambiguous ones like “aimed.” By quantifying these linguistic elements, researchers can systematically evaluate the credibility of sustainability claims. This approach enables a structured comparison of corporate narratives, ensuring that sustainability communication is not only rhetorically compelling but also substantively grounded in actionable and transparent practices.\n\n\n\nComparing sustainability communication across organizations, such as Preem (fossil fuel) and Vattenfall (renewable energy), reveals distinct rhetorical strategies shaped by their operational contexts. Fossil fuel companies like Preem may emphasize mitigation efforts, such as carbon capture, to counter environmental criticism, while renewable energy firms like Vattenfall might highlight innovation and clean energy achievements. These differences manifest in word choice, thematic focus, and narrative tone, detectable through NLP analysis. For instance, Preem’s texts might feature terms like “efficiency” or “transition,” whereas Vattenfall’s could prioritize “renewable” or “zero-emission.” Quantitative analysis of these features, such as term frequency or sentiment scores, allows researchers to map organizational priorities and assess alignment with sustainability goals. Expected differences also stem from public perception pressures: fossil fuel companies face greater scrutiny, potentially leading to defensive or compensatory messaging. By systematically comparing these patterns, NLP analysis provides evidence-based insights into how organizational type influences sustainability communication, enabling stakeholders to evaluate authenticity and strategic intent across diverse energy sectors.\n\n\n\nSummarizing NLP results transforms complex token-level analysis dataframes, which detail individual words or entities, into accessible insights. These raw dataframes, often dense with columns like token frequency or entity type, are challenging to interpret directly. Generating a new dataframe with summary statistics simplifies this by aggregating key metrics, such as content category counts (e.g., “sustainability” vs. “innovation” mentions) or sentiment measurements. The dependent variable, such as the frequency of a content category, is analyzed against independent variables like word type (e.g., nouns vs. adjectives) or organizational type (e.g., fossil fuel vs. renewable). For example, a summary dataframe might reveal that renewable energy firms use more positive adjectives than fossil fuel companies. This aggregated view highlights trends and differences without overwhelming stakeholders with granular data. By focusing on high-level patterns, summarizing ensures that findings are actionable, facilitating communication of results to diverse audiences, from researchers to corporate decision-makers, while retaining the analytical rigor of the original NLP process.\n\n\n\nSelecting, filtering, and aggregating data are critical steps in refining NLP datasets for meaningful interpretation. Selecting relevant columns, such as token entity (e.g., “Preem” or “carbon”) or part-of-speech tags (e.g., “adjective”), focuses analysis on variables pertinent to sustainability communication. Filtering removes irrelevant or incomplete data, such as rows with null values or tokens below a minimum frequency threshold, ensuring data quality. For instance, excluding low-count tokens reduces noise from rare or insignificant terms. Aggregation then computes summary metrics, such as category counts (e.g., frequency of “renewable” mentions) or measurement means (e.g., average sentiment score) per organization. This process might reveal, for example, that Vattenfall’s texts contain higher counts of “sustainability” than Preem’s. By systematically narrowing and synthesizing the dataset, these steps transform raw NLP outputs into structured insights. This enables researchers to address specific research questions, such as comparing authentic sustainability claims, with clarity and precision, supporting robust conclusions about organizational communication strategies.\n\n\n\nData visualization enhances the interpretability of NLP results, offering a more intuitive alternative to summary tables. By representing complex data—such as term frequencies or sentiment scores—visually, charts and graphs make patterns immediately apparent. Options include bar plots, word clouds, or heatmaps, each suited to different insights (e.g., word clouds for keyword prominence, heatmaps for correlations). Simple visualizations, like bar plots comparing sustainability term counts across organizations, are often more effective than complex ones, as they avoid overwhelming viewers. AI-aided data analysis streamlines visualization by automating data processing and integrating tools like Python’s Matplotlib or Seaborn, reducing manual effort. For instance, a bar plot might vividly contrast Preem’s focus on “efficiency” with Vattenfall’s on “renewable energy,” making differences accessible to non-technical stakeholders. Visualizations thus bridge the gap between raw NLP outputs and actionable insights, enabling researchers to communicate findings effectively while highlighting key trends in sustainability communication with clarity and impact.\n\n\n\nStacked bar plots are an effective tool for visualizing NLP results, particularly for comparing sustainability communication across organizations. For a single organization, simple bar plots can display metrics like the frequency of content categories (e.g., “sustainability” vs. “innovation”). Stacked bar plots extend this to bivariate or multivariate analysis by showing multiple variables within each bar, such as the proportion of nouns, verbs, and adjectives in sustainability-related texts across organizations like Preem and Vattenfall. Each segment of the bar represents a variable (e.g., word type), with the total bar height indicating overall frequency. This approach highlights differences, such as Vattenfall’s higher use of positive adjectives compared to Preem’s noun-heavy focus on “emissions.” Stacked bar plots thus provide a clear, comparative view of complex data, making it easier to identify patterns and trends. By visualizing multivariate relationships intuitively, they support researchers in communicating nuanced findings about organizational sustainability narratives to diverse audiences, from academics to industry stakeholders.",
    "crumbs": [
      "module 3",
      "lesson 3.3",
      "video lecture 3.3.1"
    ]
  },
  {
    "objectID": "0-3-course-outro.html",
    "href": "0-3-course-outro.html",
    "title": "Farewell",
    "section": "",
    "text": "script slides",
    "crumbs": [
      "course start",
      "farewell"
    ]
  },
  {
    "objectID": "0-3-course-outro.html#mooc-farewell-address",
    "href": "0-3-course-outro.html#mooc-farewell-address",
    "title": "Farewell",
    "section": "mooc farewell address",
    "text": "mooc farewell address\nAs we wrap up AI-Aided Content Analysis of Sustainability Communication, let’s reflect on the bridge from decoding digital sustainability messages to exploring their effects on minds and hearts, behaviors and cognitions.",
    "crumbs": [
      "course start",
      "farewell"
    ]
  },
  {
    "objectID": "0-3-course-outro.html#paragraph-1",
    "href": "0-3-course-outro.html#paragraph-1",
    "title": "Farewell",
    "section": "paragraph 1",
    "text": "paragraph 1\nNow, take those content features you’ve extracted—themes in climate reports, visuals in greenwashing—and use them to test real effects on audiences. With your low-code skills, design experiments to see how messages shape thoughts and feelings in groups or individuals.",
    "crumbs": [
      "course start",
      "farewell"
    ]
  },
  {
    "objectID": "0-3-course-outro.html#paragraph-2",
    "href": "0-3-course-outro.html#paragraph-2",
    "title": "Farewell",
    "section": "paragraph 2",
    "text": "paragraph 2\nStart with surveys: Layer sentiment scores into questions to check attitude shifts—does hopeful framing ease eco-anxiety in young adults? Or try psychophysiological tools like eye-tracking to spot where eyes linger on flood images, or EEG for spikes at guilt trips like “cut your carbon.”",
    "crumbs": [
      "course start",
      "farewell"
    ]
  },
  {
    "objectID": "0-3-course-outro.html#paragraph-3",
    "href": "0-3-course-outro.html#paragraph-3",
    "title": "Farewell",
    "section": "paragraph 3",
    "text": "paragraph 3\nFor web-based experiments, it’s easy and inclusive: A/B test urgency in social media posts with web surveys — does it boost arousal in city vs. rural folks? Track clicks or bias scores right in browsers, revealing how messages empower or alienate diverse groups.",
    "crumbs": [
      "course start",
      "farewell"
    ]
  },
  {
    "objectID": "0-3-course-outro.html#paragraph-4",
    "href": "0-3-course-outro.html#paragraph-4",
    "title": "Farewell",
    "section": "paragraph 4",
    "text": "paragraph 4\nLund students, this amps up your theses with mixed methods. Researchers, it’s a toolkit for stronger papers. Public folks, it shows how to critique comms that truly connect.\nYou’ve got the power to test sustainability talk’s real touch. Experiment boldly, question deeply—what if our messages heal more than they inform? Stay curious. Until next time.",
    "crumbs": [
      "course start",
      "farewell"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "!pip install -q pdfminer.six\nimport os\nfrom pdfminer.high_level import extract_text\n\n# Directories containing the PDFs\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.pdf'):\n                pdf_path = os.path.join(root, file)\n                text_path = os.path.splitext(pdf_path)[0] + '.txt'\n\n                try:\n                    text = extract_text(pdf_path)\n                    with open(text_path, 'w', encoding='utf-8') as f:\n                        f.write(text)\n                    print(f\"Converted {pdf_path} to {text_path}\")\n                except Exception as e:\n                    print(f\"Failed to convert {pdf_path}: {e}\")\nImporting Necessary Libraries\nThe code begins by importing essential modules. It imports os for interacting with the operating system’s file system and extract_text from pdfminer.high_level for extracting text content from PDF files.\n\nDefining the Directories Containing PDFs\nTwo lists named directories are defined. The first is a placeholder with ['organization1', 'organization2'], and the second specifies the actual paths to the directories containing the PDF files: - /content/osm-cca-nlp/res/pdf/preem - /content/osm-cca-nlp/res/pdf/vattenfall\n\nIterating Over Each Directory\nThe code uses a for loop to iterate through each directory specified in the directories list. This allows the program to process multiple directories sequentially.\n\nWalking Through Directory Trees\nWithin each directory, the os.walk(directory) function traverses the directory tree. It yields a tuple containing the root path, a list of dirs (subdirectories), and a list of files in each directory.\n\nIdentifying PDF Files\nFor every file in the files list, the code checks if the file name ends with .pdf (case-insensitive) using file.lower().endswith('.pdf'). This ensures that only PDF files are processed.\n\nConstructing File Paths\nThe full path to the PDF file is constructed using os.path.join(root, file). The corresponding text file path is created by replacing the .pdf extension with .txt using os.path.splitext(pdf_path)[0] + '.txt'.\n\nExtracting Text from PDFs\nA try block is initiated to attempt text extraction. The extract_text(pdf_path) function reads the content of the PDF file and stores it in the variable text.\n\nWriting Extracted Text to Files\nIf text extraction is successful, the code opens a new text file at text_path in write mode with UTF-8 encoding. It writes the extracted text into this file and then closes it, ensuring the text is saved next to the original PDF.\n\nLogging Successful Conversions\nAfter successfully writing the text file, the code prints a message indicating the PDF file has been converted, using:\nprint(f\"Converted {pdf_path} to {text_path}\")\n\nHandling Exceptions\nAn except block catches any exceptions that occur during the extraction or writing process. If an error occurs, it prints a failure message with the path of the PDF file and the exception details:\nprint(f\"Failed to convert {pdf_path}: {e}\")\n\n\n\n\nimport os\nimport pandas as pd\nimport re\nimport string\n\n# Directories containing the text files\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\ndata = []\ntext_index = 1\n\n# Allowed characters: alphabetic, punctuation, and whitespace\nallowed_chars = set(string.ascii_letters + string.punctuation + string.whitespace)\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.txt'):\n                file_path = os.path.join(root, file)\n                folder_name = os.path.basename(root)\n\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    raw_text = f.read()\n\n                # Keep only allowed characters\n                clean_text = ''.join(c for c in raw_text if c in allowed_chars)\n\n                # Replace sequences of whitespace with a single space\n                clean_text = re.sub(r'\\s+', ' ', clean_text)\n\n                # Trim leading and trailing whitespace\n                clean_text = clean_text.strip()\n\n                data.append({\n                    'text_index': text_index,\n                    'file_path': file_path,\n                    'folder_name': folder_name,\n                    'raw_text': raw_text,\n                    'clean_text': clean_text\n                })\n\n                text_index += 1\n\n# Create DataFrame\ndf_texts = pd.DataFrame(data, columns=['text_index', 'file_path', 'folder_name', 'raw_text', 'clean_text'])\n\n# Save DataFrame to TSV file\ndf_texts.to_csv('df_texts.tsv', sep='\\t', index=False)\ndf_texts.head()",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#lesson-3.1-reading-text-into-dataframes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#lesson-3.1-reading-text-into-dataframes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "!pip install -q pdfminer.six\nimport os\nfrom pdfminer.high_level import extract_text\n\n# Directories containing the PDFs\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.pdf'):\n                pdf_path = os.path.join(root, file)\n                text_path = os.path.splitext(pdf_path)[0] + '.txt'\n\n                try:\n                    text = extract_text(pdf_path)\n                    with open(text_path, 'w', encoding='utf-8') as f:\n                        f.write(text)\n                    print(f\"Converted {pdf_path} to {text_path}\")\n                except Exception as e:\n                    print(f\"Failed to convert {pdf_path}: {e}\")\nImporting Necessary Libraries\nThe code begins by importing essential modules. It imports os for interacting with the operating system’s file system and extract_text from pdfminer.high_level for extracting text content from PDF files.\n\nDefining the Directories Containing PDFs\nTwo lists named directories are defined. The first is a placeholder with ['organization1', 'organization2'], and the second specifies the actual paths to the directories containing the PDF files: - /content/osm-cca-nlp/res/pdf/preem - /content/osm-cca-nlp/res/pdf/vattenfall\n\nIterating Over Each Directory\nThe code uses a for loop to iterate through each directory specified in the directories list. This allows the program to process multiple directories sequentially.\n\nWalking Through Directory Trees\nWithin each directory, the os.walk(directory) function traverses the directory tree. It yields a tuple containing the root path, a list of dirs (subdirectories), and a list of files in each directory.\n\nIdentifying PDF Files\nFor every file in the files list, the code checks if the file name ends with .pdf (case-insensitive) using file.lower().endswith('.pdf'). This ensures that only PDF files are processed.\n\nConstructing File Paths\nThe full path to the PDF file is constructed using os.path.join(root, file). The corresponding text file path is created by replacing the .pdf extension with .txt using os.path.splitext(pdf_path)[0] + '.txt'.\n\nExtracting Text from PDFs\nA try block is initiated to attempt text extraction. The extract_text(pdf_path) function reads the content of the PDF file and stores it in the variable text.\n\nWriting Extracted Text to Files\nIf text extraction is successful, the code opens a new text file at text_path in write mode with UTF-8 encoding. It writes the extracted text into this file and then closes it, ensuring the text is saved next to the original PDF.\n\nLogging Successful Conversions\nAfter successfully writing the text file, the code prints a message indicating the PDF file has been converted, using:\nprint(f\"Converted {pdf_path} to {text_path}\")\n\nHandling Exceptions\nAn except block catches any exceptions that occur during the extraction or writing process. If an error occurs, it prints a failure message with the path of the PDF file and the exception details:\nprint(f\"Failed to convert {pdf_path}: {e}\")\n\n\n\n\nimport os\nimport pandas as pd\nimport re\nimport string\n\n# Directories containing the text files\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\ndata = []\ntext_index = 1\n\n# Allowed characters: alphabetic, punctuation, and whitespace\nallowed_chars = set(string.ascii_letters + string.punctuation + string.whitespace)\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.txt'):\n                file_path = os.path.join(root, file)\n                folder_name = os.path.basename(root)\n\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    raw_text = f.read()\n\n                # Keep only allowed characters\n                clean_text = ''.join(c for c in raw_text if c in allowed_chars)\n\n                # Replace sequences of whitespace with a single space\n                clean_text = re.sub(r'\\s+', ' ', clean_text)\n\n                # Trim leading and trailing whitespace\n                clean_text = clean_text.strip()\n\n                data.append({\n                    'text_index': text_index,\n                    'file_path': file_path,\n                    'folder_name': folder_name,\n                    'raw_text': raw_text,\n                    'clean_text': clean_text\n                })\n\n                text_index += 1\n\n# Create DataFrame\ndf_texts = pd.DataFrame(data, columns=['text_index', 'file_path', 'folder_name', 'raw_text', 'clean_text'])\n\n# Save DataFrame to TSV file\ndf_texts.to_csv('df_texts.tsv', sep='\\t', index=False)\ndf_texts.head()",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "3-1-1-video-lecture-slides.pdf\n\n\n\n\n\npowerpoint\nscript\n\n\n\n\n\n\n\nTexts in sustainability communication serve varied purposes, including raising awareness, educating audiences, advocating for change, and influencing policies. Informational texts like reports and white papers provide detailed data and analyses, while persuasive content such as blogs and social media posts aim to inspire action. Visual- and data-supported texts often enhance engagement by presenting complex ideas in accessible formats. Understanding the intended function is critical for effective communication and targeted NLP applications.\n\n\n\nNatural Language Processing (NLP) covers diverse areas, from sentiment analysis to machine translation and summarization. Working with unstructured text presents challenges, such as handling ambiguous language, idiomatic expressions, and domain-specific jargon. Additionally, processing noisy data from social media or OCR errors in scanned documents often complicates the analysis. Effective NLP requires robust preprocessing pipelines and domain-specific adjustments to achieve meaningful results.\n\n\n\nIn NLP, text analysis begins with defining the units of analysis—such as sentences, words, or characters. Tokens, the smallest logical units of text, are derived from splitting strings into meaningful components. N-grams, sequences of n tokens, capture contextual relationships, with bigrams and trigrams being particularly useful for understanding phrases. Mastering these foundational concepts is essential for advanced text processing.\n\n\n\nText is often embedded in formats like PDFs, HTML, or Markdown, each presenting unique challenges for extraction. PDFs may include layout artifacts, while HTML contains tags that must be parsed. Converting these formats into clean plain text ensures compatibility with NLP tools. Tools like Tika, Beautiful Soup, and Markdown parsers simplify this conversion process, paving the way for structured analysis.\n\n\n\nKey features of text content include readability indices like the Flesch Reading Ease, which assess complexity, and linguistic features such as Part-of-Speech (POS) tagging, which categorizes words based on their grammatical role. Named Entity Recognition (NER) identifies and classifies proper nouns, dates, or other specific entities. These features offer insights into the style, structure, and meaning of the text, aiding both analysis and decision-making.\n\n\n\nTextual data is often ingested into dataframes for analysis, enabling structured workflows. This process involves normalization tasks, such as lowercasing and removing punctuation, followed by tokenization to split the text into analyzable units. Pandas and NLTK are popular tools for managing these steps, enabling efficient preparation of text for downstream NLP tasks.\n\n\n\nManifest content refers to the explicit, observable elements of a text, such as word counts, sentence lengths, and overall structure. Techniques like word frequency analysis reveal dominant themes and help identify key terms. Sentence and word counts provide quantitative measures of text characteristics, while visualization tools like word clouds offer intuitive insights into content prominence.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "video lecture 3.1.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html#lesson-3.1-natural-language-processing-nlp-in-social-science",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html#lesson-3.1-natural-language-processing-nlp-in-social-science",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "3-1-1-video-lecture-slides.pdf\n\n\n\n\n\npowerpoint\nscript\n\n\n\n\n\n\n\nTexts in sustainability communication serve varied purposes, including raising awareness, educating audiences, advocating for change, and influencing policies. Informational texts like reports and white papers provide detailed data and analyses, while persuasive content such as blogs and social media posts aim to inspire action. Visual- and data-supported texts often enhance engagement by presenting complex ideas in accessible formats. Understanding the intended function is critical for effective communication and targeted NLP applications.\n\n\n\nNatural Language Processing (NLP) covers diverse areas, from sentiment analysis to machine translation and summarization. Working with unstructured text presents challenges, such as handling ambiguous language, idiomatic expressions, and domain-specific jargon. Additionally, processing noisy data from social media or OCR errors in scanned documents often complicates the analysis. Effective NLP requires robust preprocessing pipelines and domain-specific adjustments to achieve meaningful results.\n\n\n\nIn NLP, text analysis begins with defining the units of analysis—such as sentences, words, or characters. Tokens, the smallest logical units of text, are derived from splitting strings into meaningful components. N-grams, sequences of n tokens, capture contextual relationships, with bigrams and trigrams being particularly useful for understanding phrases. Mastering these foundational concepts is essential for advanced text processing.\n\n\n\nText is often embedded in formats like PDFs, HTML, or Markdown, each presenting unique challenges for extraction. PDFs may include layout artifacts, while HTML contains tags that must be parsed. Converting these formats into clean plain text ensures compatibility with NLP tools. Tools like Tika, Beautiful Soup, and Markdown parsers simplify this conversion process, paving the way for structured analysis.\n\n\n\nKey features of text content include readability indices like the Flesch Reading Ease, which assess complexity, and linguistic features such as Part-of-Speech (POS) tagging, which categorizes words based on their grammatical role. Named Entity Recognition (NER) identifies and classifies proper nouns, dates, or other specific entities. These features offer insights into the style, structure, and meaning of the text, aiding both analysis and decision-making.\n\n\n\nTextual data is often ingested into dataframes for analysis, enabling structured workflows. This process involves normalization tasks, such as lowercasing and removing punctuation, followed by tokenization to split the text into analyzable units. Pandas and NLTK are popular tools for managing these steps, enabling efficient preparation of text for downstream NLP tasks.\n\n\n\nManifest content refers to the explicit, observable elements of a text, such as word counts, sentence lengths, and overall structure. Techniques like word frequency analysis reveal dominant themes and help identify key terms. Sentence and word counts provide quantitative measures of text characteristics, while visualization tools like word clouds offer intuitive insights into content prominence.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "video lecture 3.1.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#token-relationships-and-knowledge-graphs",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#token-relationships-and-knowledge-graphs",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Token Relationships and Knowledge Graphs",
    "text": "Token Relationships and Knowledge Graphs\n\n\n\n\nTokens form the building blocks of text relationships and meaning in NLP.\n\nAspect-based sentiment analysis identifies sentiment tied to specific aspects or topics.\n\nKnowledge graphs map relationships between entities to provide contextual insights.\n\nToken relationships enhance machine understanding of complex linguistic structures.\n\nThese concepts drive applications in sentiment analysis, recommendations, and chatbots."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#units-of-nlp-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#units-of-nlp-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Units of NLP Analysis",
    "text": "Units of NLP Analysis\n\n\n\nNLP processes text at various levels: texts, paragraphs, sentences, words, and tokens.\n\nTexts provide overarching narratives; tokens are the smallest meaningful units.\n\nParagraphs and sentences act as natural segmentation points for processing.\n\nTokens are annotated with attributes like part-of-speech and syntactic role.\n\nUnderstanding these units is key to granular and scalable text analysis.\n\n\n\n\n\n\n🧮"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#applying-spacy-nlp-models-to-dataframes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#applying-spacy-nlp-models-to-dataframes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Applying SpaCy NLP Models to Dataframes",
    "text": "Applying SpaCy NLP Models to Dataframes\n\n\n\nSpaCy provides pre-trained NLP models for efficient text processing.\n\nText and sentence dataframes integrate structured data with NLP outputs.\n\nNLP models parse and enrich text with token, dependency, and entity annotations.\n\nBatch processing of text improves efficiency for large datasets.\n\nPractical applications include text classification, summarization, and sentiment analysis."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#iterating-over-spacy-documents",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#iterating-over-spacy-documents",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Iterating Over SpaCy Documents",
    "text": "Iterating Over SpaCy Documents\n\n\n\nSentence-level dataframes organize text into manageable units for analysis.\n\nSpaCy document objects provide linguistic annotations for each token.\n\nIterating enables extraction of sentence-specific attributes like entities or sentiments.\n\nCombines structured data analysis with NLP insights for robust results.\n\nStreamlines processes such as summarization, search indexing, and context identification."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-normalization-and-token-attributes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-normalization-and-token-attributes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Normalization and Token Attributes",
    "text": "Text Normalization and Token Attributes\n\n\n\nNormalization reduces text variability by standardizing tokens.\n\nLemmatization extracts the base form of words for consistent analysis.\n\nToken attributes, such as text and lemma, enable detailed linguistic understanding.\n\nImproved token consistency enhances downstream NLP tasks like matching and clustering.\n\nNormalization is essential for multilingual and noisy text processing.\n\n\n\n\n\n\n🧮"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#inferring-named-entity-recognition-ner",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#inferring-named-entity-recognition-ner",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Inferring Named Entity Recognition (NER)",
    "text": "Inferring Named Entity Recognition (NER)\n\n\n\nNER identifies and categorizes entities like names, organizations, and dates.\n\nExtracted entities provide structured insights from unstructured text.\n\nApplications include content categorization, fraud detection, and customer sentiment analysis.\n\nNER supports personalized recommendations and enhanced search capabilities.\n\nIt is fundamental to building knowledge graphs and question-answering systems.\n\n\n\n\n\n\n🧮"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#inferring-part-of-speech-tagging-pos",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#inferring-part-of-speech-tagging-pos",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Inferring Part-of-Speech Tagging (POS)",
    "text": "Inferring Part-of-Speech Tagging (POS)\n\n\n\nPOS tagging assigns grammatical roles to tokens like nouns, verbs, and adjectives.\n\nCaptures the syntactic structure of text for deeper linguistic understanding.\n\nApplications include syntax parsing, machine translation, and text generation.\n\nPOS tagging enhances accuracy in sentiment analysis and topic modeling.\n\nSupports advanced NLP tasks like dependency parsing and coreference resolution.\n\n\n\n\n\n\n🧮"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nUnderstanding token relationships is fundamental in NLP, where aspect-based sentiment analysis identifies sentiment tied to specific aspects of a text. Knowledge graphs further enrich this understanding by mapping relationships between entities, enabling sophisticated insights into connections and context within the data.\n\n\n\nNatural language processing operates across multiple levels of granularity, from complete texts and paragraphs to sentences, words, and individual tokens. Each unit provides unique insights, with tokens representing the smallest meaningful elements, forming the foundation for complex linguistic analysis.\n\n\n\nSpaCy’s powerful NLP models can be seamlessly integrated with text and sentence dataframes, enabling batch processing of textual data. This application simplifies linguistic analysis and provides structured outputs, such as token attributes and syntactic dependencies, directly usable for further processing.\n\n\n\nIterating over a sentence-level dataframe and corresponding SpaCy document objects allows detailed exploration of linguistic features. This method facilitates tasks like extracting sentence-specific attributes, analyzing syntactic structures, and identifying patterns within textual data.\n\n\n\nText normalization ensures consistency and clarity by standardizing tokens, extracting essential attributes like token text and lemma. Lemmatization, in particular, reduces words to their base forms, enhancing search, indexing, and overall NLP model performance.\n\n\n\nNamed entity recognition (NER) identifies and classifies entities such as names, organizations, and dates within text, enabling applications like automated content categorization, customer sentiment analysis, and improved information retrieval systems.\n\n\n\nPart-of-speech tagging assigns grammatical categories to tokens, such as nouns, verbs, or adjectives, providing critical insights into text structure. Applications include syntactic parsing, language generation, and improving machine translation systems by capturing grammatical nuances.",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "video lecture 3.2.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html#lesson-3.2-part-of-speech-and-named-entity-recognition",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html#lesson-3.2-part-of-speech-and-named-entity-recognition",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nUnderstanding token relationships is fundamental in NLP, where aspect-based sentiment analysis identifies sentiment tied to specific aspects of a text. Knowledge graphs further enrich this understanding by mapping relationships between entities, enabling sophisticated insights into connections and context within the data.\n\n\n\nNatural language processing operates across multiple levels of granularity, from complete texts and paragraphs to sentences, words, and individual tokens. Each unit provides unique insights, with tokens representing the smallest meaningful elements, forming the foundation for complex linguistic analysis.\n\n\n\nSpaCy’s powerful NLP models can be seamlessly integrated with text and sentence dataframes, enabling batch processing of textual data. This application simplifies linguistic analysis and provides structured outputs, such as token attributes and syntactic dependencies, directly usable for further processing.\n\n\n\nIterating over a sentence-level dataframe and corresponding SpaCy document objects allows detailed exploration of linguistic features. This method facilitates tasks like extracting sentence-specific attributes, analyzing syntactic structures, and identifying patterns within textual data.\n\n\n\nText normalization ensures consistency and clarity by standardizing tokens, extracting essential attributes like token text and lemma. Lemmatization, in particular, reduces words to their base forms, enhancing search, indexing, and overall NLP model performance.\n\n\n\nNamed entity recognition (NER) identifies and classifies entities such as names, organizations, and dates within text, enabling applications like automated content categorization, customer sentiment analysis, and improved information retrieval systems.\n\n\n\nPart-of-speech tagging assigns grammatical categories to tokens, such as nouns, verbs, or adjectives, providing critical insights into text structure. Applications include syntactic parsing, language generation, and improving machine translation systems by capturing grammatical nuances.",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "video lecture 3.2.1"
    ]
  },
  {
    "objectID": "aicasc-2-1-1-video-lecture.html#hello-there",
    "href": "aicasc-2-1-1-video-lecture.html#hello-there",
    "title": "Quarto Presentations",
    "section": "Hello, There",
    "text": "Hello, There\nThis presentation will show you examples of what you can do with Quarto and Reveal.js, including:\n\nPresenting code and LaTeX equations\nIncluding computations in slide output\nImage, video, and iframe backgrounds\nFancy transitions and animations\nPrinting to PDF\n\n…and much more"
  },
  {
    "objectID": "aicasc-2-1-1-video-lecture.html#pretty-code",
    "href": "aicasc-2-1-1-video-lecture.html#pretty-code",
    "title": "Quarto Presentations",
    "section": "Pretty Code",
    "text": "Pretty Code\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility\n\nimport seaborn as sns\n\n# Load the iris dataset\ndf = sns.load_dataset(\"iris\")\n\n# Display the first 5 rows\ndf.head()\n\nLearn more: Syntax Highlighting"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#open-science-methods",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\n\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#the-concept-of-jupyter-notebooks",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#the-concept-of-jupyter-notebooks",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "The Concept of Jupyter Notebooks",
    "text": "The Concept of Jupyter Notebooks\n\n\n\nEnables interactive, step-wise execution of code.\n\nIntegrates code, text, and visualizations within one document.\n\nIdeal for iterative development, debugging, and documentation.\n\nSupports educational and research purposes effectively.\n\nContrasts with traditional script-based development environments."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#google-colab-notebook-interface",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#google-colab-notebook-interface",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Google Colab Notebook Interface",
    "text": "Google Colab Notebook Interface\n\n\n\nCombines a cloud-hosted Jupyter notebook with GPU access.\n\nFeatures cells for executing code and writing text.\n\nAuto-saves progress and links to Google Drive for file storage.\n\nSupports collaboration and sharing with other users.\n\nRequires no local setup, ideal for rapid project development."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#what-is-programming-and-python-programming",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#what-is-programming-and-python-programming",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "What Is Programming and Python Programming",
    "text": "What Is Programming and Python Programming\n\n\n\nProgramming involves writing instructions for computers to execute.\n\nPython is a high-level, versatile language known for readability.\n\nWidely used in web development, data science, and automation.\n\nPython’s extensive libraries simplify complex tasks.\n\nPopular for both beginner learning and professional development."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-use-cases-low-vs.-high-code",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-use-cases-low-vs.-high-code",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python Use Cases: Low vs. High-Code",
    "text": "Python Use Cases: Low vs. High-Code\n\n\n\nLow-code leverages Python’s simplicity for fast data analysis.\n\nHigh-code applications involve deeper customization and algorithms.\n\nSocial sciences benefit from low-code data manipulation and visualization.\n\nComputer science projects often require complex, high-code solutions.\n\nPython accommodates both low-code and high-code workflows."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#basic-python-syntax-variables-data-objects-loops",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#basic-python-syntax-variables-data-objects-loops",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Basic Python Syntax: Variables, Data Objects, Loops",
    "text": "Basic Python Syntax: Variables, Data Objects, Loops\n\n\n\nVariables store information and data types define their nature.\n\nLists, dictionaries, and tuples organize data into structures.\n\nLoops iterate over data to automate repetitive tasks.\n\nPython syntax emphasizes simplicity and readability.\n\nMastery of basic syntax is essential for further programming skills.\n\n\n\n\n\n💻"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture.html",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture.html",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nOpen science promotes research practices that are transparent, reproducible, and openly accessible. It encourages international collaboration through the open sharing of data, code, and methodologies, making research more verifiable and impactful. By relying on open-source tools and platforms, it breaks down barriers between disciplines and enhances the credibility and integrity of scientific output. Ultimately, open science fosters innovation by enabling broader participation in the research process.\n\n\n\n\nPython can be run in different environments depending on the user’s needs. Local environments require installing Python and managing dependencies on your own machine, which offers control but can be complex. Cloud environments like Google Colab remove the need for setup, providing instant access to computational resources. Jupyter notebooks, used both locally and in the cloud, allow for interactive coding with integrated documentation. This flexibility makes notebooks especially useful for iterative analysis and educational purposes.\n\n\n\n\nJupyter Notebooks offer a powerful way to write and run code in small, interactive steps. Each notebook combines live code, explanations, and visualizations in a single document, making it ideal for experimentation, learning, and communication of results. They support a workflow where code can be modified and tested incrementally, which is especially helpful in debugging and teaching. Compared to traditional script-based environments, notebooks provide a more transparent and readable approach to programming.\n\n\n\n\nGoogle Colab is a free, cloud-based platform that hosts Jupyter notebooks and provides access to GPUs and other computing resources. It features an interface where users can write and execute code in cells while documenting their process in markdown. The notebook automatically saves to Google Drive, supporting easy sharing and collaboration. This makes it a perfect choice for beginners or teams who want to start coding without the hassle of installing anything locally.\n\n\n\n\nProgramming is the process of writing instructions that computers can follow to perform tasks, from calculations to automation. Python, known for its readable syntax and versatility, is one of the most popular languages for both beginners and professionals. It’s widely used in fields such as web development, data science, machine learning, and more. With its vast ecosystem of libraries, Python simplifies many tasks, making it accessible for quick prototyping and robust application development alike.\n\n\n\n\nPython supports both low-code and high-code workflows depending on user expertise and project complexity. Low-code approaches are useful for quick data analysis and visualization, especially in fields like social science where rapid insight is key. In contrast, high-code solutions enable more advanced customization, often used in computer science and AI development. The strength of Python lies in its ability to serve both ends of the spectrum efficiently.\n\n\n\n\nUnderstanding Python’s basic syntax is the first step in learning to program effectively. Variables are used to store information, and Python’s built-in data types like lists, dictionaries, and tuples help organize it. Loops enable automation by allowing code to repeat across items or conditions. Python’s syntax is intentionally clean and intuitive, which reduces errors and makes code easier to read. These fundamentals are the building blocks for writing useful and scalable programs.",
    "crumbs": [
      "module 2",
      "lesson 2.1",
      "video lecture 2.1.1"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture.html#lesson-2.1-ai-aided-and-low-code-programming",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture.html#lesson-2.1-ai-aided-and-low-code-programming",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nOpen science promotes research practices that are transparent, reproducible, and openly accessible. It encourages international collaboration through the open sharing of data, code, and methodologies, making research more verifiable and impactful. By relying on open-source tools and platforms, it breaks down barriers between disciplines and enhances the credibility and integrity of scientific output. Ultimately, open science fosters innovation by enabling broader participation in the research process.\n\n\n\n\nPython can be run in different environments depending on the user’s needs. Local environments require installing Python and managing dependencies on your own machine, which offers control but can be complex. Cloud environments like Google Colab remove the need for setup, providing instant access to computational resources. Jupyter notebooks, used both locally and in the cloud, allow for interactive coding with integrated documentation. This flexibility makes notebooks especially useful for iterative analysis and educational purposes.\n\n\n\n\nJupyter Notebooks offer a powerful way to write and run code in small, interactive steps. Each notebook combines live code, explanations, and visualizations in a single document, making it ideal for experimentation, learning, and communication of results. They support a workflow where code can be modified and tested incrementally, which is especially helpful in debugging and teaching. Compared to traditional script-based environments, notebooks provide a more transparent and readable approach to programming.\n\n\n\n\nGoogle Colab is a free, cloud-based platform that hosts Jupyter notebooks and provides access to GPUs and other computing resources. It features an interface where users can write and execute code in cells while documenting their process in markdown. The notebook automatically saves to Google Drive, supporting easy sharing and collaboration. This makes it a perfect choice for beginners or teams who want to start coding without the hassle of installing anything locally.\n\n\n\n\nProgramming is the process of writing instructions that computers can follow to perform tasks, from calculations to automation. Python, known for its readable syntax and versatility, is one of the most popular languages for both beginners and professionals. It’s widely used in fields such as web development, data science, machine learning, and more. With its vast ecosystem of libraries, Python simplifies many tasks, making it accessible for quick prototyping and robust application development alike.\n\n\n\n\nPython supports both low-code and high-code workflows depending on user expertise and project complexity. Low-code approaches are useful for quick data analysis and visualization, especially in fields like social science where rapid insight is key. In contrast, high-code solutions enable more advanced customization, often used in computer science and AI development. The strength of Python lies in its ability to serve both ends of the spectrum efficiently.\n\n\n\n\nUnderstanding Python’s basic syntax is the first step in learning to program effectively. Variables are used to store information, and Python’s built-in data types like lists, dictionaries, and tuples help organize it. Loops enable automation by allowing code to repeat across items or conditions. Python’s syntax is intentionally clean and intuitive, which reduces errors and makes code easier to read. These fundamentals are the building blocks for writing useful and scalable programs.",
    "crumbs": [
      "module 2",
      "lesson 2.1",
      "video lecture 2.1.1"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/index.html#lesson-2-2",
    "href": "module02-introduction-to-low-code-python-programming-20pct/index.html#lesson-2-2",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "lesson 2-2",
    "text": "lesson 2-2\n\nvideo-lecture.qmd:Python and AI-aided data analysis\n\n\ncomputer-lab.qmd:Low-code data analysis, tables\n\n\ncomputer-lab.qmd:Low-code data analysis, graphs",
    "crumbs": [
      "module 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "",
    "text": "index page..",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module01-understanding-sustainability-communication-content-10pct",
    "href": "index.html#module01-understanding-sustainability-communication-content-10pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module01-understanding-sustainability-communication-content-10pct",
    "text": "module01-understanding-sustainability-communication-content-10pct\n\naicasc: sustainability communication content\naicasc: find online sustainability communication\naicasc: setting up an initial content database",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module02-introduction-to-low-code-python-programming-20pct",
    "href": "index.html#module02-introduction-to-low-code-python-programming-20pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module02-introduction-to-low-code-python-programming-20pct",
    "text": "module02-introduction-to-low-code-python-programming-20pct\n\naicasc: ai-aided and low-code programming\naicasc: google colab notebook user interface\naicasc: trying out some low-code python\naicasc: python and computational data analysis\naicasc: low-code data analysis, tables\naicasc: low-code data analysis, graphs",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module03-analyzing-text-content-natural-language-processing-30pct",
    "href": "index.html#module03-analyzing-text-content-natural-language-processing-30pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module03-analyzing-text-content-natural-language-processing-30pct",
    "text": "module03-analyzing-text-content-natural-language-processing-30pct\n\naicasc: natural language processing (NLP) in social science\naicasc: reading text into dataframes\naicasc: descriptive text analysis\naicasc: part-of-speech and named entity recognition\naicasc: inferential text analysis, tokenization\naicasc: inferential text analysis, POS and NER\naicasc: interpreting the results of NLP analysis\naicasc: summarizing results of text analysis\naicasc: visualizing results of text analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module04-analyzing-image-content-computer-vision-30pct",
    "href": "index.html#module04-analyzing-image-content-computer-vision-30pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module04-analyzing-image-content-computer-vision-30pct",
    "text": "module04-analyzing-image-content-computer-vision-30pct\n\naicasc: computer vision (CV) in social science\naicasc: reading images into dataframes\naicasc: descriptive image analysis\naicasc: image classification and object detection\naicasc: inferential image analysis, classification\naicasc: inferential image analysis, object detection\naicasc: interpreting the results of CV analysis\naicasc: summarizing results of image analysis\naicasc: summarizing results of image analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "href": "index.html#module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "text": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct\n\naicasc: key takeaways and ethical perspectives\naicasc: analysis documentation and open science\naicasc: communicating ai-aided content analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#overview-of-mooc-course-ca-10-lessons-x-3-hours-work",
    "href": "index.html#overview-of-mooc-course-ca-10-lessons-x-3-hours-work",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "overview of mooc course (ca 10 lessons x 3 hours work)",
    "text": "overview of mooc course (ca 10 lessons x 3 hours work)\n\ncheck list for setting up screen recordings\n\nvideo lecture\n\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google slides browser tab\nstart google slides in presenter view\nread presentation script, stop screen recording\ncontinue screen recording until done, finalize video\n\n\n\ncomputer lab\n\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google colab browser tab\nexecute jupyter notebook cells and explain, stop screen recording\ncontinue screen recording until done, finalize video\n\n\n\nprepare quiz\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "about.html#literature",
    "href": "about.html#literature",
    "title": "About",
    "section": "literature",
    "text": "literature\n\nsustainability communication\n\n\n\n\n\n\nWeder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\ncomputational content analysis, computer vision\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\ncomputational content analysis, natural language processing\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\ncomputational data analysis\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\ncourse literature\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)"
  },
  {
    "objectID": "about.html#websites-and-apps",
    "href": "about.html#websites-and-apps",
    "title": "About",
    "section": "websites and apps",
    "text": "websites and apps\n\nhttps://www.python.org/downloads/\nhttps://wiki.python.org/moin/BeginnersGuide\nhttps://www.anaconda.com/products/individual\nhttps://code.visualstudio.com/download\nhttps://github.com/jupyterlab/jupyterlab_app#download\nhttps://trinket.io/home"
  },
  {
    "objectID": "about.html#online-articles",
    "href": "about.html#online-articles",
    "title": "About",
    "section": "online articles",
    "text": "online articles\n\nsome text here Kedia and Rasu (2020)"
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture.html",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture.html",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nIn this course, you have learned how AI can support content analysis in sustainability research by making it easier to process large amounts of data. You have gained practical skills in analyzing text and image content using low-code tools, and you are now able to critically interpret the results that machine learning models produce. Alongside the technical skills, you have explored the ethical boundaries of automation in communication analysis and reflected on core principles of transparency, bias, and accountability in AI-supported research.\n\n\n\n\nThroughout the course, you practiced collecting and organizing sustainability communication content from real-world sources. You applied both text and image analysis in Python notebooks, integrating NLP and computer vision techniques within a social science context. These methods allowed you to summarize and visualize data, uncovering communication patterns that connect computational results to audience effects and strategic messaging.\n\n\n\n\nA key theme in this final module is the ethics of automating interpretation. We asked how algorithmic bias might shape findings and where the limits of interpretability lie when results are produced automatically. You reflected on the role of the researcher in guiding and validating AI outputs, while also considering privacy and consent when collecting digital content. The main takeaway is that ethical reflection needs to be integrated into every stage of research design and dissemination.\n\n\n\n\nAn important practice in AI-aided analysis is documenting your work transparently. Reproducibility is emphasized through clear and structured workflows that store data, code, and results in accessible formats. Using cloud platforms and version control supports collaboration, while framing analysis in ways that enable peer review and replication connects computational practice with open science values. In the computer lab, you downloaded an example Jupyter notebook from Google Colab to practice this workflow.\n\n\n\n\nJupyter and Quarto provide powerful tools to integrate code, output, and documentation in one place. These platforms support reproducible research through automated execution and allow for easy conversion of analyses into HTML, PDF, or Word. By embedding metadata, citations, and environment settings, your notebooks become well-structured analytical reports that are transparent and shareable across research communities.\n\n\n\n\nEqually important is how results are communicated. Quarto helps translate technical findings into accessible narratives that use visualizations to highlight patterns in sustainability messaging. This allows you to connect observed communication effects with organizational strategies and present methods and results clearly to both technical and non-technical audiences. In practice, you also experimented with converting your notebooks into Quarto documents as part of the computer lab.\n\n\n\n\nFinally, we connected your practical exercises to the process of producing publishable research. A well-documented Google Colab or Quarto notebook is an excellent starting point for a paper. By defining clear research questions, using reproducible methods, and including structured visualizations, you can align your findings with existing literature and frameworks. Journals focusing on digital methods, media studies, or sustainability communication provide natural outlets for publishing this type of AI-aided analysis.",
    "crumbs": [
      "module 5",
      "lesson 5.1",
      "video lecture 5.1.1"
    ]
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture.html#lesson-5.1-key-takeaways-and-ethical-perspectives",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture.html#lesson-5.1-key-takeaways-and-ethical-perspectives",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nIn this course, you have learned how AI can support content analysis in sustainability research by making it easier to process large amounts of data. You have gained practical skills in analyzing text and image content using low-code tools, and you are now able to critically interpret the results that machine learning models produce. Alongside the technical skills, you have explored the ethical boundaries of automation in communication analysis and reflected on core principles of transparency, bias, and accountability in AI-supported research.\n\n\n\n\nThroughout the course, you practiced collecting and organizing sustainability communication content from real-world sources. You applied both text and image analysis in Python notebooks, integrating NLP and computer vision techniques within a social science context. These methods allowed you to summarize and visualize data, uncovering communication patterns that connect computational results to audience effects and strategic messaging.\n\n\n\n\nA key theme in this final module is the ethics of automating interpretation. We asked how algorithmic bias might shape findings and where the limits of interpretability lie when results are produced automatically. You reflected on the role of the researcher in guiding and validating AI outputs, while also considering privacy and consent when collecting digital content. The main takeaway is that ethical reflection needs to be integrated into every stage of research design and dissemination.\n\n\n\n\nAn important practice in AI-aided analysis is documenting your work transparently. Reproducibility is emphasized through clear and structured workflows that store data, code, and results in accessible formats. Using cloud platforms and version control supports collaboration, while framing analysis in ways that enable peer review and replication connects computational practice with open science values. In the computer lab, you downloaded an example Jupyter notebook from Google Colab to practice this workflow.\n\n\n\n\nJupyter and Quarto provide powerful tools to integrate code, output, and documentation in one place. These platforms support reproducible research through automated execution and allow for easy conversion of analyses into HTML, PDF, or Word. By embedding metadata, citations, and environment settings, your notebooks become well-structured analytical reports that are transparent and shareable across research communities.\n\n\n\n\nEqually important is how results are communicated. Quarto helps translate technical findings into accessible narratives that use visualizations to highlight patterns in sustainability messaging. This allows you to connect observed communication effects with organizational strategies and present methods and results clearly to both technical and non-technical audiences. In practice, you also experimented with converting your notebooks into Quarto documents as part of the computer lab.\n\n\n\n\nFinally, we connected your practical exercises to the process of producing publishable research. A well-documented Google Colab or Quarto notebook is an excellent starting point for a paper. By defining clear research questions, using reproducible methods, and including structured visualizations, you can align your findings with existing literature and frameworks. Journals focusing on digital methods, media studies, or sustainability communication provide natural outlets for publishing this type of AI-aided analysis.",
    "crumbs": [
      "module 5",
      "lesson 5.1",
      "video lecture 5.1.1"
    ]
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#key-takeaways-and-ethical-perspectives",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#key-takeaways-and-ethical-perspectives",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Key takeaways and ethical perspectives",
    "text": "Key takeaways and ethical perspectives\n\nUnderstand how AI can support content analysis in sustainability research\nGain practical skills in text and image data analysis using low-code tools\nLearn to critically interpret results produced by machine learning models\nExplore ethical boundaries of automation in communication analysis\nReflect on transparency, bias, and accountability in AI-supported research"
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#what-you-learned-in-this-course",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#what-you-learned-in-this-course",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "What You Learned in This Course",
    "text": "What You Learned in This Course\n\nCollect and organize sustainability content from real-world sources\nAnalyze both textual and visual communication using Python notebooks\nApply NLP and computer vision techniques in a social science context\nSummarize and visualize data to uncover communication patterns\nConnect computational methods to audience effects and strategic messaging"
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#ethics-of-automating-content-interpretation",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#ethics-of-automating-content-interpretation",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Ethics of Automating Content Interpretation",
    "text": "Ethics of Automating Content Interpretation\n\nQuestion how algorithmic bias might shape findings\nEvaluate the limits of interpretability in automated results\nDiscuss the role of the researcher in guiding and validating AI output\nConsider privacy and consent when collecting public digital content\nIntegrate ethical reflection into research design and dissemination"
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#analysis-documentation-and-open-science-jupyter",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#analysis-documentation-and-open-science-jupyter",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Analysis documentation and open science (Jupyter)",
    "text": "Analysis documentation and open science (Jupyter)\n\nEmphasize reproducibility through transparent workflows\nStore data, code, and results in accessible formats\nUse cloud platforms and version control for collaborative research\nFrame analysis in ways that support peer review and replication\nConnect computational practice with open science values\ncomputer lab: download ipython notebook from google colab"
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#using-jupyter-notebooks-to-ensure-reproducibility",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#using-jupyter-notebooks-to-ensure-reproducibility",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Using Jupyter Notebooks to Ensure Reproducibility",
    "text": "Using Jupyter Notebooks to Ensure Reproducibility\n\nQuarto integrates code, output, and documentation in one file\nSupports reproducible research with automated execution\nAllows easy conversion to HTML, PDF, or Word for publication\nEmbeds metadata, citations, and environment settings\nIdeal for sharing transparent, well-structured analytical reports"
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#communicating-ai-aided-content-analysis-quarto",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#communicating-ai-aided-content-analysis-quarto",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Communicating AI-aided content analysis (Quarto)",
    "text": "Communicating AI-aided content analysis (Quarto)\n\nTranslate technical findings into accessible narratives\nUse visualizations to highlight patterns in sustainability messaging\nConnect communication effects with organizational strategies\nPresent methods and results clearly for both technical and non-technical audiences\nPosition findings within broader ethical and scientific discussions\ncomputer lab: convert ipython notebook to quarto document"
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#turning-your-analysis-into-a-research-publication",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#turning-your-analysis-into-a-research-publication",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Turning Your Analysis into a Research Publication",
    "text": "Turning Your Analysis into a Research Publication\n\nStart with a well-documented Google Colab or Quarto notebook\nDefine a clear research question with reproducible methods\nUse structured visualizations to support interpretations\nAlign findings with existing literature and frameworks\nSubmit to journals focusing on digital methods, media studies, or sustainability communication"
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#functions-of-visuals-in-sustainability-communication",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#functions-of-visuals-in-sustainability-communication",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Functions of Visuals in Sustainability Communication",
    "text": "Functions of Visuals in Sustainability Communication\n\n\n\nVisuals enhance message clarity and audience engagement in sustainability topics.\nPhotos evoke emotional responses, encouraging empathy and action.\nInfographics simplify complex data for easier public understanding.\nVideos convey real-time impact, offering immersive storytelling.\nVisual variety can reinforce key sustainability messages across contexts."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#computer-vision-areas-and-content-challenges",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#computer-vision-areas-and-content-challenges",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Computer Vision Areas and Content Challenges",
    "text": "Computer Vision Areas and Content Challenges\n\nComputer vision applications include object detection, facial recognition, and scene analysis.\nVariations in lighting and angle present challenges in accurate image interpretation.\nEnvironmental monitoring relies on detecting specific features in natural scenes.\nHandling diverse image sources requires robust algorithms and large datasets.\nComputer vision aids sustainability by tracking changes in environmental conditions."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-basics-pixels-rgb-and-grayscale",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-basics-pixels-rgb-and-grayscale",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Image Basics: Pixels, RGB, and Grayscale",
    "text": "Image Basics: Pixels, RGB, and Grayscale\n\nImages are composed of pixels, each storing color or intensity values.\nRGB channels (Red, Green, Blue) combine to create various colors in visuals.\nGrayscale simplifies images by converting them into shades of gray.\nEach color channel holds intensity values ranging from 0 to 255.\nUnderstanding these basics is essential for manipulating and analyzing images."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#constructing-and-manipulating-images-with-numpy",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#constructing-and-manipulating-images-with-numpy",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Constructing and Manipulating Images with NumPy",
    "text": "Constructing and Manipulating Images with NumPy\n\nImages can be represented as NumPy matrices, enabling pixel-based control.\nEach matrix element corresponds to a pixel’s intensity or color value.\nAdjusting matrix values allows for brightness or color changes in specific regions.\nImage manipulations aid in highlighting areas of interest or removing noise.\nUsing NumPy enhances flexibility in preprocessing images for computer vision tasks."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-features-colors-histograms-and-edges",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-features-colors-histograms-and-edges",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Image Features: Colors, Histograms, and Edges",
    "text": "Image Features: Colors, Histograms, and Edges\n\nImage content features help models identify patterns within visuals.\nColor histograms reveal dominant tones by analyzing pixel distributions.\nEdge detection outlines shapes, essential for object and scene recognition.\nTexture features assist in distinguishing smooth versus rough surfaces.\nFeatures enable AI to interpret images more effectively by capturing key visual aspects."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#reading-images-into-dataframes-and-matrix-structures",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#reading-images-into-dataframes-and-matrix-structures",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Reading Images into Dataframes and Matrix Structures",
    "text": "Reading Images into Dataframes and Matrix Structures\n\nTools like OpenCV allow images to be imported as matrices or dataframes.\nDataframes enable organization of pixel data for efficient analysis.\nMatrix structures store spatial information, which is crucial for vision models.\nImage data in structured formats simplifies comparisons across visuals.\nAccessing pixel-level data provides precision in analyzing image content."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#normalizing-image-content-resize-grayscale-and-consistency",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#normalizing-image-content-resize-grayscale-and-consistency",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Normalizing Image Content: Resize, Grayscale, and Consistency",
    "text": "Normalizing Image Content: Resize, Grayscale, and Consistency\n\nImage normalization enhances model performance by standardizing inputs.\nResizing aligns images to a uniform size, aiding consistency in analysis.\nGrayscale conversion reduces complexity and focuses on essential shapes.\nBrightness adjustments help maintain consistency across varied image sources.\nNormalizing inputs is vital for achieving accurate, reliable computer vision results."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#interpreting-results-of-cv-analysis",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#interpreting-results-of-cv-analysis",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Interpreting results of CV analysis",
    "text": "Interpreting results of CV analysis\n\nInterpretation is the final step that must explicitly answer the research questions you posed at the outset.\nIn CV the evidence is visual—labels, counts, boxes, and spatial patterns—rather than textual tokens and n-grams.\nNLP can serve as a template for analogies, but CV also demands reasoning about composition, color, scale, and co-presence.\nConnect model outputs such as class probabilities and detection counts to theoretical constructs like risk or solution framing.\nReport uncertainty and alternative explanations so claims remain proportional to confidence and grounded in the study design."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#operationalizations-using-image-features",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#operationalizations-using-image-features",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Operationalizations using image features",
    "text": "Operationalizations using image features\n\nTranslate communication concepts into measurable variables derived from images.\nDefine a clear dependent variable and its measurement as a label, count, or segmented area share.\nSpecify explanatory variables, often categorical, and code them consistently across organization, sector, campaign, and time.\nState a priori expectations and link each to a specific statistical test or model.\nUse this design to limit post-hoc bias and to clarify which features indicate the constructs of interest."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#comparisons-across-organizations",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#comparisons-across-organizations",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Comparisons across organizations",
    "text": "Comparisons across organizations\n\nBegin with theory-driven expectations about how visuals should differ between organizations.\nHigh-impact firms are expected to feature mitigation and infrastructure cues.\nLow-impact firms are expected to emphasize ecosystems, communities, and everyday practices.\nDistinguish common imagery from distinctive features by comparing normalized rates rather than raw counts.\nContextualize differences across channels and time to avoid attributing one-off campaigns to enduring strategies."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#summarizing-results-of-image-analysis",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#summarizing-results-of-image-analysis",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Summarizing results of image analysis",
    "text": "Summarizing results of image analysis\n\nTidy the classification dataframe and make labels interpretable, including splitting compound class names.\nProduce core summaries such as class frequencies, detections per image, and mean confidence.\nFit models that test associations, for example logistic or Poisson regression with campaign random effects.\nReport effect sizes with uncertainty and control for multiple comparisons when many classes are tested.\nDeclare whether the analysis is exploratory or confirmatory so readers weigh results appropriately."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#select-filter-aggregate",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#select-filter-aggregate",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Select, filter, aggregate",
    "text": "Select, filter, aggregate\n\nSelect dependent and independent variables that reflect the conceptual framework.\nFilter out nulls, corrupt items, and predictions below class-specific confidence thresholds.\nAggregate with simple functions—counts, proportions, and means—at image, campaign, organization, or time levels.\nBuild compact summary tables such as class-by-organization with normalized proportions.\nUse this disciplined routine to stabilize estimates and make interpretation transparent and reproducible."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#visualizing-results-of-image-analysis",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#visualizing-results-of-image-analysis",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Visualizing results of image analysis",
    "text": "Visualizing results of image analysis\n\nUse numeric graphics such as bar charts, ridgeline densities, and co-occurrence heatmaps to show prevalence and differences.\nComplement them with CV-specific visuals that reveal what the model saw, including bounding boxes and segmentation overlays.\nInclude diagnostic views such as confusion matrices, precision–recall curves, and curated misclassification examples.\nSeparate data visualizations that describe the corpus from method visualizations that describe model behavior.\nDisplay trends with clear normalization and uncertainty so visual comparisons map cleanly to the claims."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#grouped-bar-plots",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#grouped-bar-plots",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Grouped bar plots",
    "text": "Grouped bar plots\n\nChoose grouped bars to visualize multi-dimensional comparisons while retaining simple bivariate structure.\nEncode organizations as groups and image classes or themes as bars within each group.\nNormalize to proportions to control for unequal sample sizes across organizations.\nOrder bars by prevalence or effect size and add error bars or confidence intervals where appropriate.\nHighlight where labels overlap across organizations and where distinctive imagery is over-represented."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture.html",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture.html",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nInterpreting computer-vision (CV) outputs is the final step of the image-analysis workflow and should explicitly answer the research questions you posed at the outset. Although conceptually similar to interpreting NLP results, here the evidence is visual rather than textual: labels, counts, bounding boxes, and spatial patterns replace tokens and n-grams. Because NLP is often more mature in the social sciences, it can serve as an interpretive template (e.g., map “object frequency” to “term frequency,” or “salience in the frame” to “prominence in a paragraph”), but CV frequently demands more “out-of-the-box” reasoning about composition, color, scale, and co-presence in the scene. The key is to connect model outputs (e.g., class probabilities, detection counts) to constructs in your theory (e.g., risk framing, solution framing, corporate presence) and to articulate how the visual evidence supports or disconfirms your hypotheses. Throughout, report uncertainty (confidence scores, error patterns) and discuss alternative explanations to ensure your claims remain tied to the questions you set out to answer.\n\n\n\nOperationalizing communication concepts in CV means translating them into measurable variables derived from images. Define your dependent variable clearly (e.g., the probability or count of “nature imagery,” “industrial infrastructure,” or “logo presence”) and specify how it is measured (classification label, detection count, or segmented area share). Then define explanatory variables—often categorical—such as organization, sector, campaign, channel, or time period, and code them consistently (e.g., one-hot or hierarchical groupings). State expectations ex ante (e.g., organizations with lower environmental impact will display higher “nature imagery” prevalence), and link each expectation to a specific statistical test or model. This design anchors interpretation, reduces post-hoc bias, and makes it clear which visual features are intended to serve as indicators of the underlying communication constructs.\n\n\n\nWhen comparing organizations with high versus low environmental impact, specify what you expect to see visually and why. High-impact firms may feature mitigation and infrastructure cues (plants, pipelines, protective gear, dashboards), whereas lower-impact firms may emphasize ecosystems, communities, and everyday practices (forests, biodiversity, people in natural settings). Note that overlap is possible—both groups might depict wind turbines or lab settings—so your analysis should distinguish distinctive features (used disproportionately) from common ones (shared baseline imagery). Use stratified summaries and normalization (e.g., per 100 images) to ensure fair comparisons across unequal sample sizes. Finally, contextualize differences: are they consistent across channels and time, or limited to specific campaigns and events?\n\n\n\nStart by reading the image-classification dataframe and ensuring labels and metadata are tidy and interpretable (splitting long, compound class labels into meaningful tokens can aid grouping and display). Summarize key metrics—class frequencies, detection counts per image, mean confidence—before fitting models that test associations (e.g., logistic or Poisson regression, mixed effects for campaigns). Report effect sizes with uncertainty, and use appropriate significance controls when testing many classes. Clarify whether your stance is exploratory (pattern finding, hypothesis generation) or confirmatory (pre-specified hypotheses), since this affects how results should be weighed. Well-structured summaries turn raw model outputs into evidence that answers substantive questions.\n\n\n\nPrior to modeling, select the variables that correspond to your conceptual framework (dependent and independent), and filter rows to remove noise: nulls, corrupt images, or predictions below class-specific confidence thresholds. Aggregate with simple, interpretable functions—counts, proportions, means—at analysis-relevant levels (image, campaign, organization, time window). Build compact summary tables (e.g., class-by-organization with normalized proportions) that feed directly into statistical tests and figures. This disciplined select-filter-aggregate routine reduces variance from noisy predictions and makes downstream interpretation more robust and transparent.\n\n\n\nUse standard numeric visualizations to communicate prevalence and differences (bar charts, ridgeline densities, heatmaps of feature co-occurrence), and complement them with CV-specific visuals that show what the model saw (detections drawn as bounding boxes, segmentation overlays). Include diagnostic views—confusion matrices, precision–recall curves, examples of edge cases and misclassifications—to help audiences gauge reliability. Distinguish between data visualizations (what appears in the corpus) and method visualizations (how the model behaves), and use each for its purpose. When showing trends, be explicit about normalization and uncertainty so visual comparisons map cleanly to your claims.\n\n\n\nGrouped (clustered) bar plots are a compact way to display multi-dimensional comparisons while retaining the simplicity of bivariate bars. Use groups (color/hue and legend) to encode organizations and bars to encode image classes or themes, normalizing to proportions to control for different sample sizes. This format highlights both overlapping labels (common imagery across organizations) and distinctive labels (classes over-represented in one group). Order bars by effect size or prevalence, add error bars or CIs where appropriate, and keep legends concise. Well-designed grouped bars make it easy to see where visual narratives converge and where they diverge.",
    "crumbs": [
      "module 4",
      "lesson 4.3",
      "video lecture 4.3.1"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture.html#lesson-4.3-interpreting-the-results-of-cv-analysis",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture.html#lesson-4.3-interpreting-the-results-of-cv-analysis",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nInterpreting computer-vision (CV) outputs is the final step of the image-analysis workflow and should explicitly answer the research questions you posed at the outset. Although conceptually similar to interpreting NLP results, here the evidence is visual rather than textual: labels, counts, bounding boxes, and spatial patterns replace tokens and n-grams. Because NLP is often more mature in the social sciences, it can serve as an interpretive template (e.g., map “object frequency” to “term frequency,” or “salience in the frame” to “prominence in a paragraph”), but CV frequently demands more “out-of-the-box” reasoning about composition, color, scale, and co-presence in the scene. The key is to connect model outputs (e.g., class probabilities, detection counts) to constructs in your theory (e.g., risk framing, solution framing, corporate presence) and to articulate how the visual evidence supports or disconfirms your hypotheses. Throughout, report uncertainty (confidence scores, error patterns) and discuss alternative explanations to ensure your claims remain tied to the questions you set out to answer.\n\n\n\nOperationalizing communication concepts in CV means translating them into measurable variables derived from images. Define your dependent variable clearly (e.g., the probability or count of “nature imagery,” “industrial infrastructure,” or “logo presence”) and specify how it is measured (classification label, detection count, or segmented area share). Then define explanatory variables—often categorical—such as organization, sector, campaign, channel, or time period, and code them consistently (e.g., one-hot or hierarchical groupings). State expectations ex ante (e.g., organizations with lower environmental impact will display higher “nature imagery” prevalence), and link each expectation to a specific statistical test or model. This design anchors interpretation, reduces post-hoc bias, and makes it clear which visual features are intended to serve as indicators of the underlying communication constructs.\n\n\n\nWhen comparing organizations with high versus low environmental impact, specify what you expect to see visually and why. High-impact firms may feature mitigation and infrastructure cues (plants, pipelines, protective gear, dashboards), whereas lower-impact firms may emphasize ecosystems, communities, and everyday practices (forests, biodiversity, people in natural settings). Note that overlap is possible—both groups might depict wind turbines or lab settings—so your analysis should distinguish distinctive features (used disproportionately) from common ones (shared baseline imagery). Use stratified summaries and normalization (e.g., per 100 images) to ensure fair comparisons across unequal sample sizes. Finally, contextualize differences: are they consistent across channels and time, or limited to specific campaigns and events?\n\n\n\nStart by reading the image-classification dataframe and ensuring labels and metadata are tidy and interpretable (splitting long, compound class labels into meaningful tokens can aid grouping and display). Summarize key metrics—class frequencies, detection counts per image, mean confidence—before fitting models that test associations (e.g., logistic or Poisson regression, mixed effects for campaigns). Report effect sizes with uncertainty, and use appropriate significance controls when testing many classes. Clarify whether your stance is exploratory (pattern finding, hypothesis generation) or confirmatory (pre-specified hypotheses), since this affects how results should be weighed. Well-structured summaries turn raw model outputs into evidence that answers substantive questions.\n\n\n\nPrior to modeling, select the variables that correspond to your conceptual framework (dependent and independent), and filter rows to remove noise: nulls, corrupt images, or predictions below class-specific confidence thresholds. Aggregate with simple, interpretable functions—counts, proportions, means—at analysis-relevant levels (image, campaign, organization, time window). Build compact summary tables (e.g., class-by-organization with normalized proportions) that feed directly into statistical tests and figures. This disciplined select-filter-aggregate routine reduces variance from noisy predictions and makes downstream interpretation more robust and transparent.\n\n\n\nUse standard numeric visualizations to communicate prevalence and differences (bar charts, ridgeline densities, heatmaps of feature co-occurrence), and complement them with CV-specific visuals that show what the model saw (detections drawn as bounding boxes, segmentation overlays). Include diagnostic views—confusion matrices, precision–recall curves, examples of edge cases and misclassifications—to help audiences gauge reliability. Distinguish between data visualizations (what appears in the corpus) and method visualizations (how the model behaves), and use each for its purpose. When showing trends, be explicit about normalization and uncertainty so visual comparisons map cleanly to your claims.\n\n\n\nGrouped (clustered) bar plots are a compact way to display multi-dimensional comparisons while retaining the simplicity of bivariate bars. Use groups (color/hue and legend) to encode organizations and bars to encode image classes or themes, normalizing to proportions to control for different sample sizes. This format highlights both overlapping labels (common imagery across organizations) and distinctive labels (classes over-represented in one group). Order bars by effect size or prevalence, add error bars or CIs where appropriate, and keep legends concise. Well-designed grouped bars make it easy to see where visual narratives converge and where they diverge.",
    "crumbs": [
      "module 4",
      "lesson 4.3",
      "video lecture 4.3.1"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nIn sustainability communication, visuals play diverse and essential roles, helping to convey messages effectively and inspire action. Photographs can evoke empathy by illustrating real-world environmental or social issues, while infographics break down complex data into digestible insights. Videos offer immersive storytelling, capturing time-based changes in climate or pollution. Charts and maps further enhance understanding by representing geographic data or environmental statistics visually. Choosing the right type of visual for the message strengthens the impact of sustainability communication, making data more relatable and motivating audiences toward sustainable practices.\n\n\n\nComputer vision encompasses various applications such as facial recognition, object detection, and environmental monitoring, each posing unique challenges. For example, recognizing objects under varying lighting conditions or from different angles is complex and often demands robust datasets and model fine-tuning. In sustainability contexts, computer vision helps detect patterns in satellite imagery for deforestation monitoring or pollution tracking. Challenges in image quality, such as low resolution or noise, add further complexity. Addressing these issues enhances the accuracy and reliability of computer vision applications in diverse fields.\n\n\n\nImages consist of pixels, the smallest units of a digital image, each carrying information about color or intensity. In color images, RGB (Red, Green, Blue) channels control the intensity of each primary color, creating a wide range of colors when combined. Grayscale images simplify this by displaying only shades of gray, reducing computational requirements and focusing on shapes and textures. Understanding pixels and color channels is foundational for image manipulation and analysis, as these elements define the structure and appearance of digital visuals.\n\n\n\nConstructing and manipulating images using NumPy matrices provides precise control over individual pixel regions. Representing an image as a matrix allows for adjustments to brightness, contrast, or specific colors by altering matrix values. Manipulating specific pixel regions can emphasize or obscure areas within an image, supporting tasks like highlighting points of interest in sustainability visuals. Using NumPy for image processing is powerful in computer vision as it enables direct, customizable changes to image data, preparing images for model analysis.\n\n\n\nUnderstanding image content features, such as color histograms and edges, is essential for analyzing and interpreting images. Color histograms reveal the distribution of colors in an image, helping identify dominant tones, while edge detection algorithms outline shapes and boundaries, crucial for object recognition tasks. Texture features capture surface variations, aiding in the differentiation of smooth and rough surfaces. These features provide a structured understanding of images, enabling computer vision models to detect patterns and make sense of visual data for diverse applications.\n\n\n\nDigital images can be represented as dataframes or matrices, with libraries like OpenCV facilitating this transformation. A matrix structure stores spatial information about pixels, enabling efficient analysis and comparison across images. Dataframes provide additional flexibility, organizing pixel values into a structured format for advanced data processing. Accessing image data in structured forms, like matrices, allows for detailed examination of image content, supporting tasks such as object detection and pattern recognition in computer vision applications.\n\n\n\nNormalizing image content enhances consistency across datasets, making inputs suitable for analysis by computer vision models. Resizing images ensures uniform dimensions, while grayscale conversion reduces complexity, focusing models on shapes rather than colors. Brightness normalization further enhances consistency, allowing models to interpret images reliably despite varying lighting conditions. These preprocessing steps are essential in computer vision as they reduce the variability of image inputs, helping AI models perform consistently across different visual data sources.",
    "crumbs": [
      "module 4",
      "lesson 4.1",
      "video lecture 4.1.1"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html#lesson-4.1-computer-vision-cv-in-social-science",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html#lesson-4.1-computer-vision-cv-in-social-science",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nIn sustainability communication, visuals play diverse and essential roles, helping to convey messages effectively and inspire action. Photographs can evoke empathy by illustrating real-world environmental or social issues, while infographics break down complex data into digestible insights. Videos offer immersive storytelling, capturing time-based changes in climate or pollution. Charts and maps further enhance understanding by representing geographic data or environmental statistics visually. Choosing the right type of visual for the message strengthens the impact of sustainability communication, making data more relatable and motivating audiences toward sustainable practices.\n\n\n\nComputer vision encompasses various applications such as facial recognition, object detection, and environmental monitoring, each posing unique challenges. For example, recognizing objects under varying lighting conditions or from different angles is complex and often demands robust datasets and model fine-tuning. In sustainability contexts, computer vision helps detect patterns in satellite imagery for deforestation monitoring or pollution tracking. Challenges in image quality, such as low resolution or noise, add further complexity. Addressing these issues enhances the accuracy and reliability of computer vision applications in diverse fields.\n\n\n\nImages consist of pixels, the smallest units of a digital image, each carrying information about color or intensity. In color images, RGB (Red, Green, Blue) channels control the intensity of each primary color, creating a wide range of colors when combined. Grayscale images simplify this by displaying only shades of gray, reducing computational requirements and focusing on shapes and textures. Understanding pixels and color channels is foundational for image manipulation and analysis, as these elements define the structure and appearance of digital visuals.\n\n\n\nConstructing and manipulating images using NumPy matrices provides precise control over individual pixel regions. Representing an image as a matrix allows for adjustments to brightness, contrast, or specific colors by altering matrix values. Manipulating specific pixel regions can emphasize or obscure areas within an image, supporting tasks like highlighting points of interest in sustainability visuals. Using NumPy for image processing is powerful in computer vision as it enables direct, customizable changes to image data, preparing images for model analysis.\n\n\n\nUnderstanding image content features, such as color histograms and edges, is essential for analyzing and interpreting images. Color histograms reveal the distribution of colors in an image, helping identify dominant tones, while edge detection algorithms outline shapes and boundaries, crucial for object recognition tasks. Texture features capture surface variations, aiding in the differentiation of smooth and rough surfaces. These features provide a structured understanding of images, enabling computer vision models to detect patterns and make sense of visual data for diverse applications.\n\n\n\nDigital images can be represented as dataframes or matrices, with libraries like OpenCV facilitating this transformation. A matrix structure stores spatial information about pixels, enabling efficient analysis and comparison across images. Dataframes provide additional flexibility, organizing pixel values into a structured format for advanced data processing. Accessing image data in structured forms, like matrices, allows for detailed examination of image content, supporting tasks such as object detection and pattern recognition in computer vision applications.\n\n\n\nNormalizing image content enhances consistency across datasets, making inputs suitable for analysis by computer vision models. Resizing images ensures uniform dimensions, while grayscale conversion reduces complexity, focusing models on shapes rather than colors. Brightness normalization further enhances consistency, allowing models to interpret images reliably despite varying lighting conditions. These preprocessing steps are essential in computer vision as they reduce the variability of image inputs, helping AI models perform consistently across different visual data sources.",
    "crumbs": [
      "module 4",
      "lesson 4.1",
      "video lecture 4.1.1"
    ]
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture.html",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture.html",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nTo understand sustainability communication, we need to define content both formally and functionally. Formally, we can consider sustainability content as any material that aligns with themes of environmental or social responsibility, such as climate action, resource use, or equity. Functionally, sustainability communication serves organizational purposes—such as shaping brand identity, complying with regulations, or engaging stakeholders. These functions affect how audiences perceive credibility and authenticity. Analyzing content through both lenses helps clarify its structure, intent, and potential impact.\n\n\n\n\nWhen collecting online sustainability communication for research, sampling methods shape the validity and reliability of findings. Random sampling is considered the gold standard in social science, but it may be difficult to apply in digital contexts. Instead, purposive or strategic sampling can ensure the inclusion of relevant or contrasting cases. Whichever method is used, researchers must assess whether their sample supports generalizable claims. A strong sampling design ensures that observed patterns are not merely artifacts of selection bias.\n\n\n\n\nSustainability communication can be studied using qualitative, quantitative, or mixed methods. Qualitative approaches explore the deeper meanings behind content, while quantitative methods enable comparisons and statistical testing. Descriptive studies reveal patterns and practices, while explanatory studies focus on why these patterns occur. For example, a linguistic comparison between high-impact and low-impact organizations might reveal different rhetorical strategies or terminology use. The process begins by formulating clear research questions and operationalizing them into testable hypotheses or coding schemes.\n\n\n\n\nThe first step in collecting sustainability communication is to select which organizations to study, guided by your research questions. You may focus on a single organization for depth or compare contrasting organizations for breadth. Official websites are a primary source of curated, strategic communication. Within these sites, researchers can collect the URLs of sustainability-related pages, often found under sections like “Sustainability,” “CSR,” or “Environment.” These links will form the basis of your content collection and further analysis.\n\n\n\n\nTo capture complex sustainability content online, one effective method is to print web pages to PDF using a browser. This approach preserves multimodal elements such as layout, images, and embedded links, which are essential for analyzing how messages are visually and textually constructed. It’s a fast and accessible technique that requires no special tools. However, a limitation is that videos or dynamic elements are not captured, making this best suited for static page content.\n\n\n\n\nAfter collecting the content, it’s important to store it in a way that supports reproducibility and easy access. Saving files locally is straightforward and works offline, but can be harder to share or back up. Using cloud platforms like Google Drive or GitHub offers better collaboration, version control, and accessibility. Regardless of the method, the goal is to make communication content systematically available for analysis, and to ensure it can be traced back to its source.\n\n\n\n\nOrganizing files into folders by organization or theme helps compare sustainability messaging across cases. Consistent file path naming is important for automating the analysis and maintaining structure. If content is saved as rendered PDFs, it may need to be converted into plain text later for textual analysis or coding. A clear folder structure simplifies navigation, supports reproducibility, and ensures that data management doesn’t become a barrier to insight.",
    "crumbs": [
      "module 1",
      "lesson 1.1",
      "video lecture 1.1.1"
    ]
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture.html#lesson-1.1-sustainability-communication-content",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture.html#lesson-1.1-sustainability-communication-content",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "",
    "text": "powerpoint\nscript\n\n\n\n\n\n\nTo understand sustainability communication, we need to define content both formally and functionally. Formally, we can consider sustainability content as any material that aligns with themes of environmental or social responsibility, such as climate action, resource use, or equity. Functionally, sustainability communication serves organizational purposes—such as shaping brand identity, complying with regulations, or engaging stakeholders. These functions affect how audiences perceive credibility and authenticity. Analyzing content through both lenses helps clarify its structure, intent, and potential impact.\n\n\n\n\nWhen collecting online sustainability communication for research, sampling methods shape the validity and reliability of findings. Random sampling is considered the gold standard in social science, but it may be difficult to apply in digital contexts. Instead, purposive or strategic sampling can ensure the inclusion of relevant or contrasting cases. Whichever method is used, researchers must assess whether their sample supports generalizable claims. A strong sampling design ensures that observed patterns are not merely artifacts of selection bias.\n\n\n\n\nSustainability communication can be studied using qualitative, quantitative, or mixed methods. Qualitative approaches explore the deeper meanings behind content, while quantitative methods enable comparisons and statistical testing. Descriptive studies reveal patterns and practices, while explanatory studies focus on why these patterns occur. For example, a linguistic comparison between high-impact and low-impact organizations might reveal different rhetorical strategies or terminology use. The process begins by formulating clear research questions and operationalizing them into testable hypotheses or coding schemes.\n\n\n\n\nThe first step in collecting sustainability communication is to select which organizations to study, guided by your research questions. You may focus on a single organization for depth or compare contrasting organizations for breadth. Official websites are a primary source of curated, strategic communication. Within these sites, researchers can collect the URLs of sustainability-related pages, often found under sections like “Sustainability,” “CSR,” or “Environment.” These links will form the basis of your content collection and further analysis.\n\n\n\n\nTo capture complex sustainability content online, one effective method is to print web pages to PDF using a browser. This approach preserves multimodal elements such as layout, images, and embedded links, which are essential for analyzing how messages are visually and textually constructed. It’s a fast and accessible technique that requires no special tools. However, a limitation is that videos or dynamic elements are not captured, making this best suited for static page content.\n\n\n\n\nAfter collecting the content, it’s important to store it in a way that supports reproducibility and easy access. Saving files locally is straightforward and works offline, but can be harder to share or back up. Using cloud platforms like Google Drive or GitHub offers better collaboration, version control, and accessibility. Regardless of the method, the goal is to make communication content systematically available for analysis, and to ensure it can be traced back to its source.\n\n\n\n\nOrganizing files into folders by organization or theme helps compare sustainability messaging across cases. Consistent file path naming is important for automating the analysis and maintaining structure. If content is saved as rendered PDFs, it may need to be converted into plain text later for textual analysis or coding. A clear folder structure simplifies navigation, supports reproducibility, and ensures that data management doesn’t become a barrier to insight.",
    "crumbs": [
      "module 1",
      "lesson 1.1",
      "video lecture 1.1.1"
    ]
  }
]