[
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#open-science-methods",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/index.html#lesson-2-2",
    "href": "module02-introduction-to-low-code-python-programming-20pct/index.html#lesson-2-2",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "lesson 2-2",
    "text": "lesson 2-2\n\nvideo-lecture.qmd:Python and AI-aided data analysis\n\n\ncomputer-lab.qmd:Low-code data analysis, tables\n\n\ncomputer-lab.qmd:Low-code data analysis, graphs",
    "crumbs": [
      "module 2"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#open-science-methods",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Weder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "resources.html#literature",
    "href": "resources.html#literature",
    "title": "Resources",
    "section": "",
    "text": "Weder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "resources.html#websites-and-apps",
    "href": "resources.html#websites-and-apps",
    "title": "Resources",
    "section": "websites and apps",
    "text": "websites and apps\n\nhttps://www.python.org/downloads/\nhttps://wiki.python.org/moin/BeginnersGuide\nhttps://www.anaconda.com/products/individual\nhttps://code.visualstudio.com/download\nhttps://github.com/jupyterlab/jupyterlab_app#download\nhttps://trinket.io/home",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "resources.html#online-articles",
    "href": "resources.html#online-articles",
    "title": "Resources",
    "section": "online articles",
    "text": "online articles\n\nsome text here Kedia and Rasu (2020)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#open-science-methods",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#functions-of-visuals-in-sustainability-communication",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#functions-of-visuals-in-sustainability-communication",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Functions of Visuals in Sustainability Communication",
    "text": "Functions of Visuals in Sustainability Communication\n\n\n\nVisuals enhance message clarity and audience engagement in sustainability topics.\nPhotos evoke emotional responses, encouraging empathy and action.\nInfographics simplify complex data for easier public understanding.\nVideos convey real-time impact, offering immersive storytelling.\nVisual variety can reinforce key sustainability messages across contexts."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#computer-vision-areas-and-content-challenges",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#computer-vision-areas-and-content-challenges",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Computer Vision Areas and Content Challenges",
    "text": "Computer Vision Areas and Content Challenges\n\nComputer vision applications include object detection, facial recognition, and scene analysis.\nVariations in lighting and angle present challenges in accurate image interpretation.\nEnvironmental monitoring relies on detecting specific features in natural scenes.\nHandling diverse image sources requires robust algorithms and large datasets.\nComputer vision aids sustainability by tracking changes in environmental conditions."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-basics-pixels-rgb-and-grayscale",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-basics-pixels-rgb-and-grayscale",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Image Basics: Pixels, RGB, and Grayscale",
    "text": "Image Basics: Pixels, RGB, and Grayscale\n\nImages are composed of pixels, each storing color or intensity values.\nRGB channels (Red, Green, Blue) combine to create various colors in visuals.\nGrayscale simplifies images by converting them into shades of gray.\nEach color channel holds intensity values ranging from 0 to 255.\nUnderstanding these basics is essential for manipulating and analyzing images."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#constructing-and-manipulating-images-with-numpy",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#constructing-and-manipulating-images-with-numpy",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Constructing and Manipulating Images with NumPy",
    "text": "Constructing and Manipulating Images with NumPy\n\nImages can be represented as NumPy matrices, enabling pixel-based control.\nEach matrix element corresponds to a pixel’s intensity or color value.\nAdjusting matrix values allows for brightness or color changes in specific regions.\nImage manipulations aid in highlighting areas of interest or removing noise.\nUsing NumPy enhances flexibility in preprocessing images for computer vision tasks."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-features-colors-histograms-and-edges",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-features-colors-histograms-and-edges",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Image Features: Colors, Histograms, and Edges",
    "text": "Image Features: Colors, Histograms, and Edges\n\nImage content features help models identify patterns within visuals.\nColor histograms reveal dominant tones by analyzing pixel distributions.\nEdge detection outlines shapes, essential for object and scene recognition.\nTexture features assist in distinguishing smooth versus rough surfaces.\nFeatures enable AI to interpret images more effectively by capturing key visual aspects."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#reading-images-into-dataframes-and-matrix-structures",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#reading-images-into-dataframes-and-matrix-structures",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Reading Images into Dataframes and Matrix Structures",
    "text": "Reading Images into Dataframes and Matrix Structures\n\nTools like OpenCV allow images to be imported as matrices or dataframes.\nDataframes enable organization of pixel data for efficient analysis.\nMatrix structures store spatial information, which is crucial for vision models.\nImage data in structured formats simplifies comparisons across visuals.\nAccessing pixel-level data provides precision in analyzing image content."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#normalizing-image-content-resize-grayscale-and-consistency",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#normalizing-image-content-resize-grayscale-and-consistency",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Normalizing Image Content: Resize, Grayscale, and Consistency",
    "text": "Normalizing Image Content: Resize, Grayscale, and Consistency\n\nImage normalization enhances model performance by standardizing inputs.\nResizing aligns images to a uniform size, aiding consistency in analysis.\nGrayscale conversion reduces complexity and focuses on essential shapes.\nBrightness adjustments help maintain consistency across varied image sources.\nNormalizing inputs is vital for achieving accurate, reliable computer vision results.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "about.html#literature",
    "href": "about.html#literature",
    "title": "About",
    "section": "literature",
    "text": "literature\n\nsustainability communication\n\n\n\n\n\n\n\n\n\nWeder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\ncomputational content analysis, computer vision\n\n\n\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\ncomputational content analysis, natural language processing\n\n\n\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\ncomputational data analysis\n\n\n\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\ncourse literature\n\n\n\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)"
  },
  {
    "objectID": "about.html#websites-and-apps",
    "href": "about.html#websites-and-apps",
    "title": "About",
    "section": "websites and apps",
    "text": "websites and apps\n\nhttps://www.python.org/downloads/\nhttps://wiki.python.org/moin/BeginnersGuide\nhttps://www.anaconda.com/products/individual\nhttps://code.visualstudio.com/download\nhttps://github.com/jupyterlab/jupyterlab_app#download\nhttps://trinket.io/home"
  },
  {
    "objectID": "about.html#online-articles",
    "href": "about.html#online-articles",
    "title": "About",
    "section": "online articles",
    "text": "online articles\n\nsome text here Kedia and Rasu (2020)"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "Understanding token relationships is fundamental in NLP, where aspect-based sentiment analysis identifies sentiment tied to specific aspects of a text. Knowledge graphs further enrich this understanding by mapping relationships between entities, enabling sophisticated insights into connections and context within the data.\n\n\n\nNatural language processing operates across multiple levels of granularity, from complete texts and paragraphs to sentences, words, and individual tokens. Each unit provides unique insights, with tokens representing the smallest meaningful elements, forming the foundation for complex linguistic analysis.\n\n\n\nSpaCy’s powerful NLP models can be seamlessly integrated with text and sentence dataframes, enabling batch processing of textual data. This application simplifies linguistic analysis and provides structured outputs, such as token attributes and syntactic dependencies, directly usable for further processing.\n\n\n\nIterating over a sentence-level dataframe and corresponding SpaCy document objects allows detailed exploration of linguistic features. This method facilitates tasks like extracting sentence-specific attributes, analyzing syntactic structures, and identifying patterns within textual data.\n\n\n\nText normalization ensures consistency and clarity by standardizing tokens, extracting essential attributes like token text and lemma. Lemmatization, in particular, reduces words to their base forms, enhancing search, indexing, and overall NLP model performance.\n\n\n\nNamed entity recognition (NER) identifies and classifies entities such as names, organizations, and dates within text, enabling applications like automated content categorization, customer sentiment analysis, and improved information retrieval systems.\n\n\n\nPart-of-speech tagging assigns grammatical categories to tokens, such as nouns, verbs, or adjectives, providing critical insights into text structure. Applications include syntactic parsing, language generation, and improving machine translation systems by capturing grammatical nuances.",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "video lecture 3.2.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html#lesson-3.2-part-of-speech-and-named-entity-recognition",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html#lesson-3.2-part-of-speech-and-named-entity-recognition",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "Understanding token relationships is fundamental in NLP, where aspect-based sentiment analysis identifies sentiment tied to specific aspects of a text. Knowledge graphs further enrich this understanding by mapping relationships between entities, enabling sophisticated insights into connections and context within the data.\n\n\n\nNatural language processing operates across multiple levels of granularity, from complete texts and paragraphs to sentences, words, and individual tokens. Each unit provides unique insights, with tokens representing the smallest meaningful elements, forming the foundation for complex linguistic analysis.\n\n\n\nSpaCy’s powerful NLP models can be seamlessly integrated with text and sentence dataframes, enabling batch processing of textual data. This application simplifies linguistic analysis and provides structured outputs, such as token attributes and syntactic dependencies, directly usable for further processing.\n\n\n\nIterating over a sentence-level dataframe and corresponding SpaCy document objects allows detailed exploration of linguistic features. This method facilitates tasks like extracting sentence-specific attributes, analyzing syntactic structures, and identifying patterns within textual data.\n\n\n\nText normalization ensures consistency and clarity by standardizing tokens, extracting essential attributes like token text and lemma. Lemmatization, in particular, reduces words to their base forms, enhancing search, indexing, and overall NLP model performance.\n\n\n\nNamed entity recognition (NER) identifies and classifies entities such as names, organizations, and dates within text, enabling applications like automated content categorization, customer sentiment analysis, and improved information retrieval systems.\n\n\n\nPart-of-speech tagging assigns grammatical categories to tokens, such as nouns, verbs, or adjectives, providing critical insights into text structure. Applications include syntactic parsing, language generation, and improving machine translation systems by capturing grammatical nuances.",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "video lecture 3.2.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#functions-of-texts-in-sustainability-communication",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#functions-of-texts-in-sustainability-communication",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Functions of Texts in Sustainability Communication",
    "text": "Functions of Texts in Sustainability Communication\n\n\n\nInformational texts provide data-driven insights and factual details.\n\nPersuasive texts motivate audiences toward action or change.\n\nNarrative texts create emotional connections to sustainability themes.\n\nVisual-supported texts enhance accessibility and engagement.\n\nEach text type targets specific audiences and communication goals."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#nlp-areas-and-challenges-of-unstructured-text",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#nlp-areas-and-challenges-of-unstructured-text",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "NLP Areas and Challenges of Unstructured Text",
    "text": "NLP Areas and Challenges of Unstructured Text\n\n\n\nNLP applications include sentiment analysis, translation, and summarization.\n\nUnstructured text often contains ambiguous language and incomplete sentences.\n\nDomain-specific jargon and colloquialisms complicate processing.\n\nNoise in data sources like social media adds layers of preprocessing needs.\n\nRobust algorithms and preprocessing pipelines mitigate these challenges."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#basic-concepts-units-tokens-and-n-grams",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#basic-concepts-units-tokens-and-n-grams",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Basic Concepts: Units, Tokens, and N-grams",
    "text": "Basic Concepts: Units, Tokens, and N-grams\n\nText units include sentences, words, and characters as analytical building blocks.\n\nTokens are the smallest logical units, often derived from words.\n\nN-grams capture sequences of n tokens, revealing contextual patterns.\n\nCommon n-grams include bigrams (two words) and trigrams (three words).\n\nThese concepts underpin more advanced NLP tasks."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#formats-and-conversion-to-plain-text",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#formats-and-conversion-to-plain-text",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Formats and Conversion to Plain Text",
    "text": "Formats and Conversion to Plain Text\n\nText is often stored in formats like PDF, HTML, or Markdown.\n\nPDFs may contain layout artifacts, complicating text extraction.\n\nHTML requires parsing to remove tags and extract meaningful content.\n\nTools like Beautiful Soup and Tika streamline these conversions.\n\nConverting to plain text ensures compatibility with NLP workflows."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#text-features-readability-pos-and-ner",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#text-features-readability-pos-and-ner",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Features: Readability, POS, and NER",
    "text": "Text Features: Readability, POS, and NER\n\nReadability indices assess the complexity of written content.\n\nPOS tagging categorizes words by their grammatical function.\n\nNER identifies and classifies specific entities, such as names and dates.\n\nThese features provide insights into the style and structure of text.\n\nThey are essential for contextual and thematic understanding in NLP."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#reading-text-into-dataframes-and-preprocessing",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#reading-text-into-dataframes-and-preprocessing",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Reading Text into Dataframes and Preprocessing",
    "text": "Reading Text into Dataframes and Preprocessing\n\nDataframes structure text data for analysis and visualization.\n\nNormalization includes lowercasing and punctuation removal.\n\nTokenization splits text into analyzable units like words or phrases.\n\nPandas and NLTK are widely used for preprocessing workflows.\n\nClean, tokenized data is a prerequisite for most NLP tasks."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#manifest-text-content-and-frequency-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#manifest-text-content-and-frequency-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Manifest Text Content and Frequency Analysis",
    "text": "Manifest Text Content and Frequency Analysis\n\nManifest content refers to explicitly observable text elements.\n\nSentence and word counts provide quantitative content metrics.\n\nWord frequency analysis highlights key terms and dominant themes.\n\nVisualizations like word clouds offer intuitive insights into text data.\n\nThese metrics are foundational for exploratory text analysis.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "3-1-1-video-lecture-slides.pdf\n\n\n\n\n\n\n\n\n\n\nTexts in sustainability communication serve varied purposes, including raising awareness, educating audiences, advocating for change, and influencing policies. Informational texts like reports and white papers provide detailed data and analyses, while persuasive content such as blogs and social media posts aim to inspire action. Visual- and data-supported texts often enhance engagement by presenting complex ideas in accessible formats. Understanding the intended function is critical for effective communication and targeted NLP applications.\n\n\n\nNatural Language Processing (NLP) covers diverse areas, from sentiment analysis to machine translation and summarization. Working with unstructured text presents challenges, such as handling ambiguous language, idiomatic expressions, and domain-specific jargon. Additionally, processing noisy data from social media or OCR errors in scanned documents often complicates the analysis. Effective NLP requires robust preprocessing pipelines and domain-specific adjustments to achieve meaningful results.\n\n\n\nIn NLP, text analysis begins with defining the units of analysis—such as sentences, words, or characters. Tokens, the smallest logical units of text, are derived from splitting strings into meaningful components. N-grams, sequences of n tokens, capture contextual relationships, with bigrams and trigrams being particularly useful for understanding phrases. Mastering these foundational concepts is essential for advanced text processing.\n\n\n\nText is often embedded in formats like PDFs, HTML, or Markdown, each presenting unique challenges for extraction. PDFs may include layout artifacts, while HTML contains tags that must be parsed. Converting these formats into clean plain text ensures compatibility with NLP tools. Tools like Tika, Beautiful Soup, and Markdown parsers simplify this conversion process, paving the way for structured analysis.\n\n\n\nKey features of text content include readability indices like the Flesch Reading Ease, which assess complexity, and linguistic features such as Part-of-Speech (POS) tagging, which categorizes words based on their grammatical role. Named Entity Recognition (NER) identifies and classifies proper nouns, dates, or other specific entities. These features offer insights into the style, structure, and meaning of the text, aiding both analysis and decision-making.\n\n\n\nTextual data is often ingested into dataframes for analysis, enabling structured workflows. This process involves normalization tasks, such as lowercasing and removing punctuation, followed by tokenization to split the text into analyzable units. Pandas and NLTK are popular tools for managing these steps, enabling efficient preparation of text for downstream NLP tasks.\n\n\n\nManifest content refers to the explicit, observable elements of a text, such as word counts, sentence lengths, and overall structure. Techniques like word frequency analysis reveal dominant themes and help identify key terms. Sentence and word counts provide quantitative measures of text characteristics, while visualization tools like word clouds offer intuitive insights into content prominence.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "video lecture 3.1.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html#lesson-3.1-natural-language-processing-nlp-in-social-science",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html#lesson-3.1-natural-language-processing-nlp-in-social-science",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "3-1-1-video-lecture-slides.pdf\n\n\n\n\n\n\n\n\n\n\nTexts in sustainability communication serve varied purposes, including raising awareness, educating audiences, advocating for change, and influencing policies. Informational texts like reports and white papers provide detailed data and analyses, while persuasive content such as blogs and social media posts aim to inspire action. Visual- and data-supported texts often enhance engagement by presenting complex ideas in accessible formats. Understanding the intended function is critical for effective communication and targeted NLP applications.\n\n\n\nNatural Language Processing (NLP) covers diverse areas, from sentiment analysis to machine translation and summarization. Working with unstructured text presents challenges, such as handling ambiguous language, idiomatic expressions, and domain-specific jargon. Additionally, processing noisy data from social media or OCR errors in scanned documents often complicates the analysis. Effective NLP requires robust preprocessing pipelines and domain-specific adjustments to achieve meaningful results.\n\n\n\nIn NLP, text analysis begins with defining the units of analysis—such as sentences, words, or characters. Tokens, the smallest logical units of text, are derived from splitting strings into meaningful components. N-grams, sequences of n tokens, capture contextual relationships, with bigrams and trigrams being particularly useful for understanding phrases. Mastering these foundational concepts is essential for advanced text processing.\n\n\n\nText is often embedded in formats like PDFs, HTML, or Markdown, each presenting unique challenges for extraction. PDFs may include layout artifacts, while HTML contains tags that must be parsed. Converting these formats into clean plain text ensures compatibility with NLP tools. Tools like Tika, Beautiful Soup, and Markdown parsers simplify this conversion process, paving the way for structured analysis.\n\n\n\nKey features of text content include readability indices like the Flesch Reading Ease, which assess complexity, and linguistic features such as Part-of-Speech (POS) tagging, which categorizes words based on their grammatical role. Named Entity Recognition (NER) identifies and classifies proper nouns, dates, or other specific entities. These features offer insights into the style, structure, and meaning of the text, aiding both analysis and decision-making.\n\n\n\nTextual data is often ingested into dataframes for analysis, enabling structured workflows. This process involves normalization tasks, such as lowercasing and removing punctuation, followed by tokenization to split the text into analyzable units. Pandas and NLTK are popular tools for managing these steps, enabling efficient preparation of text for downstream NLP tasks.\n\n\n\nManifest content refers to the explicit, observable elements of a text, such as word counts, sentence lengths, and overall structure. Techniques like word frequency analysis reveal dominant themes and help identify key terms. Sentence and word counts provide quantitative measures of text characteristics, while visualization tools like word clouds offer intuitive insights into content prominence.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "video lecture 3.1.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-2",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-2",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "lesson 3-2",
    "text": "lesson 3-2\n\nvideo-lecture.qmd:Part-of-speech and named entity recognition\n\n\ncomputer-lab.qmd:Inferential text analysis, tokenization\n\n\ncomputer-lab.qmd:Inferential text analysis, POS and NER",
    "crumbs": [
      "module 3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-3",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-3",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "lesson 3-3",
    "text": "lesson 3-3\n\nvideo-lecture.qmd:Interpreting the results of NLP analysis\n\n\ncomputer-lab.qmd:Summarizing results of text analysis\n\n\ncomputer-lab.qmd:Visualizing results of text analysis",
    "crumbs": [
      "module 3"
    ]
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#open-science-methods",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#open-science-methods",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#token-relationships-and-knowledge-graphs",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#token-relationships-and-knowledge-graphs",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Token Relationships and Knowledge Graphs",
    "text": "Token Relationships and Knowledge Graphs\n\n\n\n\nTokens form the building blocks of text relationships and meaning in NLP.\n\nAspect-based sentiment analysis identifies sentiment tied to specific aspects or topics.\n\nKnowledge graphs map relationships between entities to provide contextual insights.\n\nToken relationships enhance machine understanding of complex linguistic structures.\n\nThese concepts drive applications in sentiment analysis, recommendations, and chatbots."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#units-of-nlp-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#units-of-nlp-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Units of NLP Analysis",
    "text": "Units of NLP Analysis\n\n\nNLP processes text at various levels: texts, paragraphs, sentences, words, and tokens.\n\nTexts provide overarching narratives; tokens are the smallest meaningful units.\n\nParagraphs and sentences act as natural segmentation points for processing.\n\nTokens are annotated with attributes like part-of-speech and syntactic role.\n\nUnderstanding these units is key to granular and scalable text analysis."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#applying-spacy-nlp-models-to-dataframes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#applying-spacy-nlp-models-to-dataframes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Applying SpaCy NLP Models to Dataframes",
    "text": "Applying SpaCy NLP Models to Dataframes\n\nSpaCy provides pre-trained NLP models for efficient text processing.\n\nText and sentence dataframes integrate structured data with NLP outputs.\n\nNLP models parse and enrich text with token, dependency, and entity annotations.\n\nBatch processing of text improves efficiency for large datasets.\n\nPractical applications include text classification, summarization, and sentiment analysis."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#iterating-over-sentence-dataframes-and-spacy-documents",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#iterating-over-sentence-dataframes-and-spacy-documents",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Iterating Over Sentence Dataframes and SpaCy Documents",
    "text": "Iterating Over Sentence Dataframes and SpaCy Documents\n\nSentence-level dataframes organize text into manageable units for analysis.\n\nSpaCy document objects provide linguistic annotations for each token.\n\nIterating enables extraction of sentence-specific attributes like entities or sentiments.\n\nCombines structured data analysis with NLP insights for robust results.\n\nStreamlines processes such as summarization, search indexing, and context identification."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-normalization-and-token-attributes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-normalization-and-token-attributes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Normalization and Token Attributes",
    "text": "Text Normalization and Token Attributes\n\nNormalization reduces text variability by standardizing tokens.\n\nLemmatization extracts the base form of words for consistent analysis.\n\nToken attributes, such as text and lemma, enable detailed linguistic understanding.\n\nImproved token consistency enhances downstream NLP tasks like matching and clustering.\n\nNormalization is essential for multilingual and noisy text processing."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-inference-named-entity-recognition-ner",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-inference-named-entity-recognition-ner",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Inference: Named Entity Recognition (NER)",
    "text": "Text Inference: Named Entity Recognition (NER)\n\nNER identifies and categorizes entities like names, organizations, and dates.\n\nExtracted entities provide structured insights from unstructured text.\n\nApplications include content categorization, fraud detection, and customer sentiment analysis.\n\nNER supports personalized recommendations and enhanced search capabilities.\n\nIt is fundamental to building knowledge graphs and question-answering systems."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-inference-part-of-speech-tagging-pos",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-inference-part-of-speech-tagging-pos",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Inference: Part-of-Speech Tagging (POS)",
    "text": "Text Inference: Part-of-Speech Tagging (POS)\n\nPOS tagging assigns grammatical roles to tokens like nouns, verbs, and adjectives.\n\nCaptures the syntactic structure of text for deeper linguistic understanding.\n\nApplications include syntax parsing, machine translation, and text generation.\n\nPOS tagging enhances accuracy in sentiment analysis and topic modeling.\n\nSupports advanced NLP tasks like dependency parsing and coreference resolution.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "!pip install -q pdfminer.six\nimport os\nfrom pdfminer.high_level import extract_text\n\n# Directories containing the PDFs\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.pdf'):\n                pdf_path = os.path.join(root, file)\n                text_path = os.path.splitext(pdf_path)[0] + '.txt'\n\n                try:\n                    text = extract_text(pdf_path)\n                    with open(text_path, 'w', encoding='utf-8') as f:\n                        f.write(text)\n                    print(f\"Converted {pdf_path} to {text_path}\")\n                except Exception as e:\n                    print(f\"Failed to convert {pdf_path}: {e}\")\nImporting Necessary Libraries\nThe code begins by importing essential modules. It imports os for interacting with the operating system’s file system and extract_text from pdfminer.high_level for extracting text content from PDF files.\n\nDefining the Directories Containing PDFs\nTwo lists named directories are defined. The first is a placeholder with ['organization1', 'organization2'], and the second specifies the actual paths to the directories containing the PDF files: - /content/osm-cca-nlp/res/pdf/preem - /content/osm-cca-nlp/res/pdf/vattenfall\n\nIterating Over Each Directory\nThe code uses a for loop to iterate through each directory specified in the directories list. This allows the program to process multiple directories sequentially.\n\nWalking Through Directory Trees\nWithin each directory, the os.walk(directory) function traverses the directory tree. It yields a tuple containing the root path, a list of dirs (subdirectories), and a list of files in each directory.\n\nIdentifying PDF Files\nFor every file in the files list, the code checks if the file name ends with .pdf (case-insensitive) using file.lower().endswith('.pdf'). This ensures that only PDF files are processed.\n\nConstructing File Paths\nThe full path to the PDF file is constructed using os.path.join(root, file). The corresponding text file path is created by replacing the .pdf extension with .txt using os.path.splitext(pdf_path)[0] + '.txt'.\n\nExtracting Text from PDFs\nA try block is initiated to attempt text extraction. The extract_text(pdf_path) function reads the content of the PDF file and stores it in the variable text.\n\nWriting Extracted Text to Files\nIf text extraction is successful, the code opens a new text file at text_path in write mode with UTF-8 encoding. It writes the extracted text into this file and then closes it, ensuring the text is saved next to the original PDF.\n\nLogging Successful Conversions\nAfter successfully writing the text file, the code prints a message indicating the PDF file has been converted, using:\nprint(f\"Converted {pdf_path} to {text_path}\")\n\nHandling Exceptions\nAn except block catches any exceptions that occur during the extraction or writing process. If an error occurs, it prints a failure message with the path of the PDF file and the exception details:\nprint(f\"Failed to convert {pdf_path}: {e}\")\n\n\n\n\nimport os\nimport pandas as pd\nimport re\nimport string\n\n# Directories containing the text files\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\ndata = []\ntext_index = 1\n\n# Allowed characters: alphabetic, punctuation, and whitespace\nallowed_chars = set(string.ascii_letters + string.punctuation + string.whitespace)\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.txt'):\n                file_path = os.path.join(root, file)\n                folder_name = os.path.basename(root)\n\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    raw_text = f.read()\n\n                # Keep only allowed characters\n                clean_text = ''.join(c for c in raw_text if c in allowed_chars)\n\n                # Replace sequences of whitespace with a single space\n                clean_text = re.sub(r'\\s+', ' ', clean_text)\n\n                # Trim leading and trailing whitespace\n                clean_text = clean_text.strip()\n\n                data.append({\n                    'text_index': text_index,\n                    'file_path': file_path,\n                    'folder_name': folder_name,\n                    'raw_text': raw_text,\n                    'clean_text': clean_text\n                })\n\n                text_index += 1\n\n# Create DataFrame\ndf_texts = pd.DataFrame(data, columns=['text_index', 'file_path', 'folder_name', 'raw_text', 'clean_text'])\n\n# Save DataFrame to TSV file\ndf_texts.to_csv('df_texts.tsv', sep='\\t', index=False)\ndf_texts.head()",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#lesson-3.1-reading-text-into-dataframes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#lesson-3.1-reading-text-into-dataframes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "!pip install -q pdfminer.six\nimport os\nfrom pdfminer.high_level import extract_text\n\n# Directories containing the PDFs\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.pdf'):\n                pdf_path = os.path.join(root, file)\n                text_path = os.path.splitext(pdf_path)[0] + '.txt'\n\n                try:\n                    text = extract_text(pdf_path)\n                    with open(text_path, 'w', encoding='utf-8') as f:\n                        f.write(text)\n                    print(f\"Converted {pdf_path} to {text_path}\")\n                except Exception as e:\n                    print(f\"Failed to convert {pdf_path}: {e}\")\nImporting Necessary Libraries\nThe code begins by importing essential modules. It imports os for interacting with the operating system’s file system and extract_text from pdfminer.high_level for extracting text content from PDF files.\n\nDefining the Directories Containing PDFs\nTwo lists named directories are defined. The first is a placeholder with ['organization1', 'organization2'], and the second specifies the actual paths to the directories containing the PDF files: - /content/osm-cca-nlp/res/pdf/preem - /content/osm-cca-nlp/res/pdf/vattenfall\n\nIterating Over Each Directory\nThe code uses a for loop to iterate through each directory specified in the directories list. This allows the program to process multiple directories sequentially.\n\nWalking Through Directory Trees\nWithin each directory, the os.walk(directory) function traverses the directory tree. It yields a tuple containing the root path, a list of dirs (subdirectories), and a list of files in each directory.\n\nIdentifying PDF Files\nFor every file in the files list, the code checks if the file name ends with .pdf (case-insensitive) using file.lower().endswith('.pdf'). This ensures that only PDF files are processed.\n\nConstructing File Paths\nThe full path to the PDF file is constructed using os.path.join(root, file). The corresponding text file path is created by replacing the .pdf extension with .txt using os.path.splitext(pdf_path)[0] + '.txt'.\n\nExtracting Text from PDFs\nA try block is initiated to attempt text extraction. The extract_text(pdf_path) function reads the content of the PDF file and stores it in the variable text.\n\nWriting Extracted Text to Files\nIf text extraction is successful, the code opens a new text file at text_path in write mode with UTF-8 encoding. It writes the extracted text into this file and then closes it, ensuring the text is saved next to the original PDF.\n\nLogging Successful Conversions\nAfter successfully writing the text file, the code prints a message indicating the PDF file has been converted, using:\nprint(f\"Converted {pdf_path} to {text_path}\")\n\nHandling Exceptions\nAn except block catches any exceptions that occur during the extraction or writing process. If an error occurs, it prints a failure message with the path of the PDF file and the exception details:\nprint(f\"Failed to convert {pdf_path}: {e}\")\n\n\n\n\nimport os\nimport pandas as pd\nimport re\nimport string\n\n# Directories containing the text files\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\ndata = []\ntext_index = 1\n\n# Allowed characters: alphabetic, punctuation, and whitespace\nallowed_chars = set(string.ascii_letters + string.punctuation + string.whitespace)\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.txt'):\n                file_path = os.path.join(root, file)\n                folder_name = os.path.basename(root)\n\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    raw_text = f.read()\n\n                # Keep only allowed characters\n                clean_text = ''.join(c for c in raw_text if c in allowed_chars)\n\n                # Replace sequences of whitespace with a single space\n                clean_text = re.sub(r'\\s+', ' ', clean_text)\n\n                # Trim leading and trailing whitespace\n                clean_text = clean_text.strip()\n\n                data.append({\n                    'text_index': text_index,\n                    'file_path': file_path,\n                    'folder_name': folder_name,\n                    'raw_text': raw_text,\n                    'clean_text': clean_text\n                })\n\n                text_index += 1\n\n# Create DataFrame\ndf_texts = pd.DataFrame(data, columns=['text_index', 'file_path', 'folder_name', 'raw_text', 'clean_text'])\n\n# Save DataFrame to TSV file\ndf_texts.to_csv('df_texts.tsv', sep='\\t', index=False)\ndf_texts.head()",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "In sustainability communication, visuals play diverse and essential roles, helping to convey messages effectively and inspire action. Photographs can evoke empathy by illustrating real-world environmental or social issues, while infographics break down complex data into digestible insights. Videos offer immersive storytelling, capturing time-based changes in climate or pollution. Charts and maps further enhance understanding by representing geographic data or environmental statistics visually. Choosing the right type of visual for the message strengthens the impact of sustainability communication, making data more relatable and motivating audiences toward sustainable practices.\n\n\n\nComputer vision encompasses various applications such as facial recognition, object detection, and environmental monitoring, each posing unique challenges. For example, recognizing objects under varying lighting conditions or from different angles is complex and often demands robust datasets and model fine-tuning. In sustainability contexts, computer vision helps detect patterns in satellite imagery for deforestation monitoring or pollution tracking. Challenges in image quality, such as low resolution or noise, add further complexity. Addressing these issues enhances the accuracy and reliability of computer vision applications in diverse fields.\n\n\n\nImages consist of pixels, the smallest units of a digital image, each carrying information about color or intensity. In color images, RGB (Red, Green, Blue) channels control the intensity of each primary color, creating a wide range of colors when combined. Grayscale images simplify this by displaying only shades of gray, reducing computational requirements and focusing on shapes and textures. Understanding pixels and color channels is foundational for image manipulation and analysis, as these elements define the structure and appearance of digital visuals.\n\n\n\nConstructing and manipulating images using NumPy matrices provides precise control over individual pixel regions. Representing an image as a matrix allows for adjustments to brightness, contrast, or specific colors by altering matrix values. Manipulating specific pixel regions can emphasize or obscure areas within an image, supporting tasks like highlighting points of interest in sustainability visuals. Using NumPy for image processing is powerful in computer vision as it enables direct, customizable changes to image data, preparing images for model analysis.\n\n\n\nUnderstanding image content features, such as color histograms and edges, is essential for analyzing and interpreting images. Color histograms reveal the distribution of colors in an image, helping identify dominant tones, while edge detection algorithms outline shapes and boundaries, crucial for object recognition tasks. Texture features capture surface variations, aiding in the differentiation of smooth and rough surfaces. These features provide a structured understanding of images, enabling computer vision models to detect patterns and make sense of visual data for diverse applications.\n\n\n\nDigital images can be represented as dataframes or matrices, with libraries like OpenCV facilitating this transformation. A matrix structure stores spatial information about pixels, enabling efficient analysis and comparison across images. Dataframes provide additional flexibility, organizing pixel values into a structured format for advanced data processing. Accessing image data in structured forms, like matrices, allows for detailed examination of image content, supporting tasks such as object detection and pattern recognition in computer vision applications.\n\n\n\nNormalizing image content enhances consistency across datasets, making inputs suitable for analysis by computer vision models. Resizing images ensures uniform dimensions, while grayscale conversion reduces complexity, focusing models on shapes rather than colors. Brightness normalization further enhances consistency, allowing models to interpret images reliably despite varying lighting conditions. These preprocessing steps are essential in computer vision as they reduce the variability of image inputs, helping AI models perform consistently across different visual data sources.",
    "crumbs": [
      "module 4",
      "lesson 4.1",
      "video lecture 4.1.1"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html#lesson-4.1-computer-vision-cv-in-social-science",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html#lesson-4.1-computer-vision-cv-in-social-science",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "In sustainability communication, visuals play diverse and essential roles, helping to convey messages effectively and inspire action. Photographs can evoke empathy by illustrating real-world environmental or social issues, while infographics break down complex data into digestible insights. Videos offer immersive storytelling, capturing time-based changes in climate or pollution. Charts and maps further enhance understanding by representing geographic data or environmental statistics visually. Choosing the right type of visual for the message strengthens the impact of sustainability communication, making data more relatable and motivating audiences toward sustainable practices.\n\n\n\nComputer vision encompasses various applications such as facial recognition, object detection, and environmental monitoring, each posing unique challenges. For example, recognizing objects under varying lighting conditions or from different angles is complex and often demands robust datasets and model fine-tuning. In sustainability contexts, computer vision helps detect patterns in satellite imagery for deforestation monitoring or pollution tracking. Challenges in image quality, such as low resolution or noise, add further complexity. Addressing these issues enhances the accuracy and reliability of computer vision applications in diverse fields.\n\n\n\nImages consist of pixels, the smallest units of a digital image, each carrying information about color or intensity. In color images, RGB (Red, Green, Blue) channels control the intensity of each primary color, creating a wide range of colors when combined. Grayscale images simplify this by displaying only shades of gray, reducing computational requirements and focusing on shapes and textures. Understanding pixels and color channels is foundational for image manipulation and analysis, as these elements define the structure and appearance of digital visuals.\n\n\n\nConstructing and manipulating images using NumPy matrices provides precise control over individual pixel regions. Representing an image as a matrix allows for adjustments to brightness, contrast, or specific colors by altering matrix values. Manipulating specific pixel regions can emphasize or obscure areas within an image, supporting tasks like highlighting points of interest in sustainability visuals. Using NumPy for image processing is powerful in computer vision as it enables direct, customizable changes to image data, preparing images for model analysis.\n\n\n\nUnderstanding image content features, such as color histograms and edges, is essential for analyzing and interpreting images. Color histograms reveal the distribution of colors in an image, helping identify dominant tones, while edge detection algorithms outline shapes and boundaries, crucial for object recognition tasks. Texture features capture surface variations, aiding in the differentiation of smooth and rough surfaces. These features provide a structured understanding of images, enabling computer vision models to detect patterns and make sense of visual data for diverse applications.\n\n\n\nDigital images can be represented as dataframes or matrices, with libraries like OpenCV facilitating this transformation. A matrix structure stores spatial information about pixels, enabling efficient analysis and comparison across images. Dataframes provide additional flexibility, organizing pixel values into a structured format for advanced data processing. Accessing image data in structured forms, like matrices, allows for detailed examination of image content, supporting tasks such as object detection and pattern recognition in computer vision applications.\n\n\n\nNormalizing image content enhances consistency across datasets, making inputs suitable for analysis by computer vision models. Resizing images ensures uniform dimensions, while grayscale conversion reduces complexity, focusing models on shapes rather than colors. Brightness normalization further enhances consistency, allowing models to interpret images reliably despite varying lighting conditions. These preprocessing steps are essential in computer vision as they reduce the variability of image inputs, helping AI models perform consistently across different visual data sources.",
    "crumbs": [
      "module 4",
      "lesson 4.1",
      "video lecture 4.1.1"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#open-science-methods",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown.\n\n\n\n\n\nhttps://www.iko.lu.se"
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-2",
    "href": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-2",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "lesson 4-2",
    "text": "lesson 4-2\n\nvideo-lecture.qmd:Image classification and object detection\n\n\ncomputer-lab.qmd:Inferential image analysis, classification\n\n\ncomputer-lab.qmd:Inferential image analysis, object detection",
    "crumbs": [
      "module 4"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-3",
    "href": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-3",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "lesson 4-3",
    "text": "lesson 4-3\n\nvideo-lecture.qmd:Interpreting the results of CV analysis\n\n\ncomputer-lab.qmd:Summarizing results of image analysis\n\n\ncomputer-lab.qmd:Visualizing results of image analysis",
    "crumbs": [
      "module 4"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "",
    "text": "aicasc: sustainability communication content\naicasc: find online sustainability communication\naicasc: setting up an initial content database",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module01-understanding-sustainability-communication-content-10pct",
    "href": "index.html#module01-understanding-sustainability-communication-content-10pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "",
    "text": "aicasc: sustainability communication content\naicasc: find online sustainability communication\naicasc: setting up an initial content database",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module02-introduction-to-low-code-python-programming-20pct",
    "href": "index.html#module02-introduction-to-low-code-python-programming-20pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module02-introduction-to-low-code-python-programming-20pct",
    "text": "module02-introduction-to-low-code-python-programming-20pct\n\naicasc: ai-aided and low-code programming\naicasc: google colab notebook user interface\naicasc: trying out some low-code python\naicasc: python and computational data analysis\naicasc: low-code data analysis, tables\naicasc: low-code data analysis, graphs",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module03-analyzing-text-content-natural-language-processing-30pct",
    "href": "index.html#module03-analyzing-text-content-natural-language-processing-30pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module03-analyzing-text-content-natural-language-processing-30pct",
    "text": "module03-analyzing-text-content-natural-language-processing-30pct\n\naicasc: natural language processing (NLP) in social science\naicasc: reading text into dataframes\naicasc: descriptive text analysis\naicasc: part-of-speech and named entity recognition\naicasc: inferential text analysis, tokenization\naicasc: inferential text analysis, POS and NER\naicasc: interpreting the results of NLP analysis\naicasc: summarizing results of text analysis\naicasc: visualizing results of text analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module04-analyzing-image-content-computer-vision-30pct",
    "href": "index.html#module04-analyzing-image-content-computer-vision-30pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module04-analyzing-image-content-computer-vision-30pct",
    "text": "module04-analyzing-image-content-computer-vision-30pct\n\naicasc: computer vision (CV) in social science\naicasc: reading images into dataframes\naicasc: descriptive image analysis\naicasc: image classification and object detection\naicasc: inferential image analysis, classification\naicasc: inferential image analysis, object detection\naicasc: interpreting the results of CV analysis\naicasc: summarizing results of image analysis\naicasc: summarizing results of image analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "href": "index.html#module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "text": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct\n\naicasc: key takeaways and ethical perspectives\naicasc: analysis documentation and open science\naicasc: communicating ai-aided content analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#overview-of-mooc-course-ca-10-lessons-x-3-hours-work",
    "href": "index.html#overview-of-mooc-course-ca-10-lessons-x-3-hours-work",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "overview of mooc course (ca 10 lessons x 3 hours work)",
    "text": "overview of mooc course (ca 10 lessons x 3 hours work)\n\ncheck list for setting up screen recordings\n\nvideo lecture\n\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google slides browser tab\nstart google slides in presenter view\nread presentation script, stop screen recording\ncontinue screen recording until done, finalize video\n\n\n\ncomputer lab\n\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google colab browser tab\nexecute jupyter notebook cells and explain, stop screen recording\ncontinue screen recording until done, finalize video\n\n\n\nprepare quiz\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "aicasc-2-1-1-video-lecture.html#hello-there",
    "href": "aicasc-2-1-1-video-lecture.html#hello-there",
    "title": "Quarto Presentations",
    "section": "Hello, There",
    "text": "Hello, There\nThis presentation will show you examples of what you can do with Quarto and Reveal.js, including:\n\nPresenting code and LaTeX equations\nIncluding computations in slide output\nImage, video, and iframe backgrounds\nFancy transitions and animations\nPrinting to PDF\n\n…and much more"
  },
  {
    "objectID": "aicasc-2-1-1-video-lecture.html#pretty-code",
    "href": "aicasc-2-1-1-video-lecture.html#pretty-code",
    "title": "Quarto Presentations",
    "section": "Pretty Code",
    "text": "Pretty Code\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility\n\nimport seaborn as sns\n\n# Load the iris dataset\ndf = sns.load_dataset(\"iris\")\n\n# Display the first 5 rows\ndf.head()\n\nLearn more: Syntax Highlighting\n\n\n\n\n\nhttps://quarto.org"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#open-science-methods",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#the-concept-of-jupyter-notebooks",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#the-concept-of-jupyter-notebooks",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "The Concept of Jupyter Notebooks",
    "text": "The Concept of Jupyter Notebooks\n\nEnables interactive, step-wise execution of code.\n\nIntegrates code, text, and visualizations within one document.\n\nIdeal for iterative development, debugging, and documentation.\n\nSupports educational and research purposes effectively.\n\nContrasts with traditional script-based development environments."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#google-colab-notebook-interface",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#google-colab-notebook-interface",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Google Colab Notebook Interface",
    "text": "Google Colab Notebook Interface\n\nCombines a cloud-hosted Jupyter notebook with GPU access.\n\nFeatures cells for executing code and writing text.\n\nAuto-saves progress and links to Google Drive for file storage.\n\nSupports collaboration and sharing with other users.\n\nRequires no local setup, ideal for rapid project development."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#what-is-programming-and-python-programming",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#what-is-programming-and-python-programming",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "What Is Programming and Python Programming",
    "text": "What Is Programming and Python Programming\n\nProgramming involves writing instructions for computers to execute.\n\nPython is a high-level, versatile language known for readability.\n\nWidely used in web development, data science, and automation.\n\nPython’s extensive libraries simplify complex tasks.\n\nPopular for both beginner learning and professional development."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-use-cases-low-code-social-science-vs.-high-code-computer-science",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-use-cases-low-code-social-science-vs.-high-code-computer-science",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python Use Cases: Low-Code Social Science vs. High-Code Computer Science",
    "text": "Python Use Cases: Low-Code Social Science vs. High-Code Computer Science\n\nLow-code leverages Python’s simplicity for fast data analysis.\n\nHigh-code applications involve deeper customization and algorithms.\n\nSocial sciences benefit from low-code data manipulation and visualization.\n\nComputer science projects often require complex, high-code solutions.\n\nPython accommodates both low-code and high-code workflows."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#basic-python-syntax-variables-data-objects-loops",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#basic-python-syntax-variables-data-objects-loops",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Basic Python Syntax: Variables, Data Objects, Loops",
    "text": "Basic Python Syntax: Variables, Data Objects, Loops\n\nVariables store information and data types define their nature.\n\nLists, dictionaries, and tuples organize data into structures.\n\nLoops iterate over data to automate repetitive tasks.\n\nPython syntax emphasizes simplicity and readability.\n\nMastery of basic syntax is essential for further programming skills.\n\n\n\n\n\nhttps://www.iko.lu.se"
  }
]