[
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#sustainability-communication-content",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#sustainability-communication-content",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Sustainability communication content",
    "text": "Sustainability communication content\n\nDefine sustainability content both formally and functionally\nFormal content reflects environmental or social responsibility themes\nFunctional content supports branding, compliance, and stakeholder engagement\nCommunication influences perceptions of credibility and authenticity\nDual analysis reveals intent, structure, and audience impact"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#online-content-sampling",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#online-content-sampling",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Online content sampling",
    "text": "Online content sampling\n\nSampling method influences research reliability and validity\nRandom sampling is ideal but hard to apply in web contexts\nPurposive sampling targets relevant or contrasting examples\nStrategic sampling balances focus and generalizability\nDesign choices must minimize bias in content selection"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#expectations-about-sustainability-communication",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#expectations-about-sustainability-communication",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Expectations about sustainability communication",
    "text": "Expectations about sustainability communication\n\nChoose between qualitative, quantitative, or mixed approaches\nDescriptive analysis explores patterns; explanatory seeks causes\nLinguistic analysis compares tone, terms, and rhetorical strategies\nResearch begins with well-formed questions and hypotheses\nMethodological clarity improves interpretability and rigor"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#find-online-sustainability-communication",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#find-online-sustainability-communication",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Find online sustainability communication",
    "text": "Find online sustainability communication\n\nSelect organizations aligned with your research objectives\nStudy one or compare multiple for contrast and depth\nTarget official websites as curated communication sources\nIdentify and extract sustainability-related URLs\nBuild a dataset from key pages like “CSR” or “Sustainability”"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#convert-multimodal-web-pages-to-pdf",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#convert-multimodal-web-pages-to-pdf",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Convert multimodal web pages to PDF",
    "text": "Convert multimodal web pages to PDF\n\nUse browser print-to-PDF to capture entire web pages\nPreserves images, layout, and static design elements\nEnables analysis of multimodal communication forms\nQuick, user-friendly method requiring no programming\nLimitation: videos and interactive elements not captured"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#setting-up-an-initial-content-database",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#setting-up-an-initial-content-database",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Setting up an initial content database",
    "text": "Setting up an initial content database\n\nEnsure all collected content is stored in an accessible format\nLocal storage is easy but lacks shareability and backup\nCloud solutions like Google Drive support collaboration\nGitHub adds version control and traceability benefits\nA well-structured archive supports long-term analysis"
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#organize-communication-content-in-folders",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture-slides.html#organize-communication-content-in-folders",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "Organize communication content in folders",
    "text": "Organize communication content in folders\n\nSeparate folders allow comparisons between organizations\nNormalize filenames and paths for consistent processing\nFolder structure simplifies automation and batch analysis\nPDFs often need OCR or text extraction for further use\nGood organization supports transparency and reproducibility"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/index.html#lesson-2-2",
    "href": "module02-introduction-to-low-code-python-programming-20pct/index.html#lesson-2-2",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "lesson 2-2",
    "text": "lesson 2-2\n\nvideo-lecture.qmd:Python and AI-aided data analysis\n\n\ncomputer-lab.qmd:Low-code data analysis, tables\n\n\ncomputer-lab.qmd:Low-code data analysis, graphs",
    "crumbs": [
      "module 2"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#python-for-social-science-data-analysis",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#python-for-social-science-data-analysis",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python for Social Science Data Analysis",
    "text": "Python for Social Science Data Analysis\n\n\n\nOverview of data analysis types: descriptive, inferential, exploratory\nEmphasis on quantitative methods used in social sciences\nIntroduction to computational content analysis (text/image)\nBenefits of using Python for flexible, scalable analysis\nCommon libraries: pandas, statsmodels, matplotlib, scikit-learn"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#ai-aided-quantitative-analysis-with-python",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#ai-aided-quantitative-analysis-with-python",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "AI-Aided Quantitative Analysis with Python",
    "text": "AI-Aided Quantitative Analysis with Python\n\nUse large language models (LLMs) to generate statistical code\nQuery data directly using natural language prompts\nAutomate hypothesis testing and model fitting\nReduce entry barrier for non-programmers\nBalance between manual control and AI guidance"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#ai-aided-computational-content-analysis",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#ai-aided-computational-content-analysis",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "AI-Aided Computational Content Analysis",
    "text": "AI-Aided Computational Content Analysis\n\nApply LLMs for text classification, summarization, and topic modeling\nUse image models (e.g., CLIP, ViT) for analyzing visual content\nCombine text and visual features for rich content analysis\nAutomate entity recognition and theme detection\nUse AI to scale analysis of large corpora"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#low-code-data-analysis-with-tables",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#low-code-data-analysis-with-tables",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Low-Code Data Analysis with Tables",
    "text": "Low-Code Data Analysis with Tables\n\nLoad and view datasets using pandas DataFrames\nPerform filtering, grouping, and summary stats with minimal code\nContrast high-code scripts vs. low-code AI-assisted workflows\nUse AI tools to generate and interpret table outputs\nExample tools: pandasgui, datatable, ChatGPT + CSV"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#data-transformations-with-ai-assistance",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#data-transformations-with-ai-assistance",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Data Transformations with AI Assistance",
    "text": "Data Transformations with AI Assistance\n\nSelect relevant variables using prompts or code\nFilter observations based on conditions (e.g., time, value ranges)\nAggregate data for summaries (e.g., mean by group)\nCompare traditional pandas syntax vs. AI-assisted queries\nLearn transformation logic through interactive AI feedback"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#low-code-data-analysis-with-graphs",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#low-code-data-analysis-with-graphs",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Low-Code Data Analysis with Graphs",
    "text": "Low-Code Data Analysis with Graphs\n\nCreate basic visualizations (e.g., bar, line, histogram)\nUnderstand univariate (one variable) vs. bivariate (two variables) plots\nUse AI to suggest and generate appropriate plot types\nExample libraries: matplotlib, seaborn, plotnine\nVisualize data trends and relationships with minimal setup"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#multivariate-and-interactive-graphs",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture-slides.html#multivariate-and-interactive-graphs",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Multivariate and Interactive Graphs",
    "text": "Multivariate and Interactive Graphs\n\nVisualize relationships between three or more variables\nUse scatter matrices, heatmaps, or bubble charts\nAdd interactivity with plotly, altair, or holoviews\nUse AI to refine visual layouts and variable selections\nEnable dynamic exploration of patterns in large datasets"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture.html",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture.html",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "",
    "text": "Open science promotes research practices that are transparent, reproducible, and openly accessible. It encourages international collaboration through the open sharing of data, code, and methodologies, making research more verifiable and impactful. By relying on open-source tools and platforms, it breaks down barriers between disciplines and enhances the credibility and integrity of scientific output. Ultimately, open science fosters innovation by enabling broader participation in the research process.\n\n\n\n\nPython can be run in different environments depending on the user’s needs. Local environments require installing Python and managing dependencies on your own machine, which offers control but can be complex. Cloud environments like Google Colab remove the need for setup, providing instant access to computational resources. Jupyter notebooks, used both locally and in the cloud, allow for interactive coding with integrated documentation. This flexibility makes notebooks especially useful for iterative analysis and educational purposes.\n\n\n\n\nJupyter Notebooks offer a powerful way to write and run code in small, interactive steps. Each notebook combines live code, explanations, and visualizations in a single document, making it ideal for experimentation, learning, and communication of results. They support a workflow where code can be modified and tested incrementally, which is especially helpful in debugging and teaching. Compared to traditional script-based environments, notebooks provide a more transparent and readable approach to programming.\n\n\n\n\nGoogle Colab is a free, cloud-based platform that hosts Jupyter notebooks and provides access to GPUs and other computing resources. It features an interface where users can write and execute code in cells while documenting their process in markdown. The notebook automatically saves to Google Drive, supporting easy sharing and collaboration. This makes it a perfect choice for beginners or teams who want to start coding without the hassle of installing anything locally.\n\n\n\n\nProgramming is the process of writing instructions that computers can follow to perform tasks, from calculations to automation. Python, known for its readable syntax and versatility, is one of the most popular languages for both beginners and professionals. It’s widely used in fields such as web development, data science, machine learning, and more. With its vast ecosystem of libraries, Python simplifies many tasks, making it accessible for quick prototyping and robust application development alike.\n\n\n\n\nPython supports both low-code and high-code workflows depending on user expertise and project complexity. Low-code approaches are useful for quick data analysis and visualization, especially in fields like social science where rapid insight is key. In contrast, high-code solutions enable more advanced customization, often used in computer science and AI development. The strength of Python lies in its ability to serve both ends of the spectrum efficiently.\n\n\n\n\nUnderstanding Python’s basic syntax is the first step in learning to program effectively. Variables are used to store information, and Python’s built-in data types like lists, dictionaries, and tuples help organize it. Loops enable automation by allowing code to repeat across items or conditions. Python’s syntax is intentionally clean and intuitive, which reduces errors and makes code easier to read. These fundamentals are the building blocks for writing useful and scalable programs.",
    "crumbs": [
      "module 2",
      "lesson 2.1",
      "video lecture 2.1.1"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture.html#lesson-2.1-ai-aided-and-low-code-programming",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture.html#lesson-2.1-ai-aided-and-low-code-programming",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "",
    "text": "Open science promotes research practices that are transparent, reproducible, and openly accessible. It encourages international collaboration through the open sharing of data, code, and methodologies, making research more verifiable and impactful. By relying on open-source tools and platforms, it breaks down barriers between disciplines and enhances the credibility and integrity of scientific output. Ultimately, open science fosters innovation by enabling broader participation in the research process.\n\n\n\n\nPython can be run in different environments depending on the user’s needs. Local environments require installing Python and managing dependencies on your own machine, which offers control but can be complex. Cloud environments like Google Colab remove the need for setup, providing instant access to computational resources. Jupyter notebooks, used both locally and in the cloud, allow for interactive coding with integrated documentation. This flexibility makes notebooks especially useful for iterative analysis and educational purposes.\n\n\n\n\nJupyter Notebooks offer a powerful way to write and run code in small, interactive steps. Each notebook combines live code, explanations, and visualizations in a single document, making it ideal for experimentation, learning, and communication of results. They support a workflow where code can be modified and tested incrementally, which is especially helpful in debugging and teaching. Compared to traditional script-based environments, notebooks provide a more transparent and readable approach to programming.\n\n\n\n\nGoogle Colab is a free, cloud-based platform that hosts Jupyter notebooks and provides access to GPUs and other computing resources. It features an interface where users can write and execute code in cells while documenting their process in markdown. The notebook automatically saves to Google Drive, supporting easy sharing and collaboration. This makes it a perfect choice for beginners or teams who want to start coding without the hassle of installing anything locally.\n\n\n\n\nProgramming is the process of writing instructions that computers can follow to perform tasks, from calculations to automation. Python, known for its readable syntax and versatility, is one of the most popular languages for both beginners and professionals. It’s widely used in fields such as web development, data science, machine learning, and more. With its vast ecosystem of libraries, Python simplifies many tasks, making it accessible for quick prototyping and robust application development alike.\n\n\n\n\nPython supports both low-code and high-code workflows depending on user expertise and project complexity. Low-code approaches are useful for quick data analysis and visualization, especially in fields like social science where rapid insight is key. In contrast, high-code solutions enable more advanced customization, often used in computer science and AI development. The strength of Python lies in its ability to serve both ends of the spectrum efficiently.\n\n\n\n\nUnderstanding Python’s basic syntax is the first step in learning to program effectively. Variables are used to store information, and Python’s built-in data types like lists, dictionaries, and tuples help organize it. Loops enable automation by allowing code to repeat across items or conditions. Python’s syntax is intentionally clean and intuitive, which reduces errors and makes code easier to read. These fundamentals are the building blocks for writing useful and scalable programs.",
    "crumbs": [
      "module 2",
      "lesson 2.1",
      "video lecture 2.1.1"
    ]
  },
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to AI-Aided Content Analysis of Sustainability Communication—an online course designed to equip you with cutting-edge tools to explore one of the most pressing challenges of our time: how sustainability is communicated, interpreted, and shaped through digital content.\nWhether you’re a Lund University student advancing your coursework in media studies, environmental science, or digital methods, or a researcher or learner joining us from across the globe, this course opens new doors into how we can see, read, and interpret sustainability communication at scale. And if you’re a curious member of the public—perhaps with an interest in climate issues, artificial intelligence, or media literacy—you’ll find this course a window into how machines can help us make sense of the messages that shape our collective future.\nFrom the very start, you’ll get hands-on experience building your own content database—learning how to find, collect, and structure sustainability communication from real-world websites. Through accessible, low-code Python notebooks running in Google Colab, you’ll be empowered to run your first analyses with minimal technical barriers. Want to see how organizations talk about climate goals? Curious how different companies portray sustainability through visuals? You’ll learn to read and analyze both text and image content, using powerful tools from natural language processing and computer vision.\nFor Lund University students, this course offers specific methodological training that can be applied directly to thesis work or project assignments. You’ll gain not only technical fluency but also a critical understanding of how to document your methods and communicate your findings transparently—skills that align with open science values and interdisciplinary research goals.\nInternational participants and researchers will find the course modular and self-paced, with content that bridges theory and application. You’ll explore how AI models like named entity recognition, tokenization, and object detection can be used to analyze sustainability messages—helping you ask sharper research questions and generate new forms of evidence from digital sources.\nAnd for members of the general public, this course demystifies AI and data analysis. You won’t just learn what these models do—you’ll learn how to question them: How does automated analysis shape what we see in sustainability reports? Where do ethical boundaries lie? And how can content analysis be a tool for both transparency and critique?\nBy the end of this course, you’ll be able to extract insights from digital content, summarize and visualize patterns in sustainability communication, and engage with critical discussions about the ethics of AI-assisted research. Whether you’re here to meet a curriculum requirement, advance your own research, or explore a personal passion—you’re in the right place.\nWelcome aboard. Let’s begin.",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "welcome.html#mooc-welcome-address",
    "href": "welcome.html#mooc-welcome-address",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to AI-Aided Content Analysis of Sustainability Communication—an online course designed to equip you with cutting-edge tools to explore one of the most pressing challenges of our time: how sustainability is communicated, interpreted, and shaped through digital content.\nWhether you’re a Lund University student advancing your coursework in media studies, environmental science, or digital methods, or a researcher or learner joining us from across the globe, this course opens new doors into how we can see, read, and interpret sustainability communication at scale. And if you’re a curious member of the public—perhaps with an interest in climate issues, artificial intelligence, or media literacy—you’ll find this course a window into how machines can help us make sense of the messages that shape our collective future.\nFrom the very start, you’ll get hands-on experience building your own content database—learning how to find, collect, and structure sustainability communication from real-world websites. Through accessible, low-code Python notebooks running in Google Colab, you’ll be empowered to run your first analyses with minimal technical barriers. Want to see how organizations talk about climate goals? Curious how different companies portray sustainability through visuals? You’ll learn to read and analyze both text and image content, using powerful tools from natural language processing and computer vision.\nFor Lund University students, this course offers specific methodological training that can be applied directly to thesis work or project assignments. You’ll gain not only technical fluency but also a critical understanding of how to document your methods and communicate your findings transparently—skills that align with open science values and interdisciplinary research goals.\nInternational participants and researchers will find the course modular and self-paced, with content that bridges theory and application. You’ll explore how AI models like named entity recognition, tokenization, and object detection can be used to analyze sustainability messages—helping you ask sharper research questions and generate new forms of evidence from digital sources.\nAnd for members of the general public, this course demystifies AI and data analysis. You won’t just learn what these models do—you’ll learn how to question them: How does automated analysis shape what we see in sustainability reports? Where do ethical boundaries lie? And how can content analysis be a tool for both transparency and critique?\nBy the end of this course, you’ll be able to extract insights from digital content, summarize and visualize patterns in sustainability communication, and engage with critical discussions about the ethics of AI-assisted research. Whether you’re here to meet a curriculum requirement, advance your own research, or explore a personal passion—you’re in the right place.\nWelcome aboard. Let’s begin.",
    "crumbs": [
      "course start",
      "welcome"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "",
    "text": "index page..",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module01-understanding-sustainability-communication-content-10pct",
    "href": "index.html#module01-understanding-sustainability-communication-content-10pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module01-understanding-sustainability-communication-content-10pct",
    "text": "module01-understanding-sustainability-communication-content-10pct\n\naicasc: sustainability communication content\naicasc: find online sustainability communication\naicasc: setting up an initial content database",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module02-introduction-to-low-code-python-programming-20pct",
    "href": "index.html#module02-introduction-to-low-code-python-programming-20pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module02-introduction-to-low-code-python-programming-20pct",
    "text": "module02-introduction-to-low-code-python-programming-20pct\n\naicasc: ai-aided and low-code programming\naicasc: google colab notebook user interface\naicasc: trying out some low-code python\naicasc: python and computational data analysis\naicasc: low-code data analysis, tables\naicasc: low-code data analysis, graphs",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module03-analyzing-text-content-natural-language-processing-30pct",
    "href": "index.html#module03-analyzing-text-content-natural-language-processing-30pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module03-analyzing-text-content-natural-language-processing-30pct",
    "text": "module03-analyzing-text-content-natural-language-processing-30pct\n\naicasc: natural language processing (NLP) in social science\naicasc: reading text into dataframes\naicasc: descriptive text analysis\naicasc: part-of-speech and named entity recognition\naicasc: inferential text analysis, tokenization\naicasc: inferential text analysis, POS and NER\naicasc: interpreting the results of NLP analysis\naicasc: summarizing results of text analysis\naicasc: visualizing results of text analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module04-analyzing-image-content-computer-vision-30pct",
    "href": "index.html#module04-analyzing-image-content-computer-vision-30pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module04-analyzing-image-content-computer-vision-30pct",
    "text": "module04-analyzing-image-content-computer-vision-30pct\n\naicasc: computer vision (CV) in social science\naicasc: reading images into dataframes\naicasc: descriptive image analysis\naicasc: image classification and object detection\naicasc: inferential image analysis, classification\naicasc: inferential image analysis, object detection\naicasc: interpreting the results of CV analysis\naicasc: summarizing results of image analysis\naicasc: summarizing results of image analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "href": "index.html#module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct",
    "text": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct\n\naicasc: key takeaways and ethical perspectives\naicasc: analysis documentation and open science\naicasc: communicating ai-aided content analysis",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#overview-of-mooc-course-ca-10-lessons-x-3-hours-work",
    "href": "index.html#overview-of-mooc-course-ca-10-lessons-x-3-hours-work",
    "title": "AI-aided content analysis of sustainability communication",
    "section": "overview of mooc course (ca 10 lessons x 3 hours work)",
    "text": "overview of mooc course (ca 10 lessons x 3 hours work)\n\ncheck list for setting up screen recordings\n\nvideo lecture\n\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google slides browser tab\nstart google slides in presenter view\nread presentation script, stop screen recording\ncontinue screen recording until done, finalize video\n\n\n\ncomputer lab\n\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google colab browser tab\nexecute jupyter notebook cells and explain, stop screen recording\ncontinue screen recording until done, finalize video\n\n\n\nprepare quiz\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-2",
    "href": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-2",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "lesson 4-2",
    "text": "lesson 4-2\n\nvideo-lecture.qmd:Image classification and object detection\n\n\ncomputer-lab.qmd:Inferential image analysis, classification\n\n\ncomputer-lab.qmd:Inferential image analysis, object detection",
    "crumbs": [
      "module 4"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-3",
    "href": "module04-analyzing-image-content-computer-vision-30pct/index.html#lesson-4-3",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "lesson 4-3",
    "text": "lesson 4-3\n\nvideo-lecture.qmd:Interpreting the results of CV analysis\n\n\ncomputer-lab.qmd:Summarizing results of image analysis\n\n\ncomputer-lab.qmd:Visualizing results of image analysis",
    "crumbs": [
      "module 4"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#open-science-methods",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-2-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "In sustainability communication, visuals play diverse and essential roles, helping to convey messages effectively and inspire action. Photographs can evoke empathy by illustrating real-world environmental or social issues, while infographics break down complex data into digestible insights. Videos offer immersive storytelling, capturing time-based changes in climate or pollution. Charts and maps further enhance understanding by representing geographic data or environmental statistics visually. Choosing the right type of visual for the message strengthens the impact of sustainability communication, making data more relatable and motivating audiences toward sustainable practices.\n\n\n\nComputer vision encompasses various applications such as facial recognition, object detection, and environmental monitoring, each posing unique challenges. For example, recognizing objects under varying lighting conditions or from different angles is complex and often demands robust datasets and model fine-tuning. In sustainability contexts, computer vision helps detect patterns in satellite imagery for deforestation monitoring or pollution tracking. Challenges in image quality, such as low resolution or noise, add further complexity. Addressing these issues enhances the accuracy and reliability of computer vision applications in diverse fields.\n\n\n\nImages consist of pixels, the smallest units of a digital image, each carrying information about color or intensity. In color images, RGB (Red, Green, Blue) channels control the intensity of each primary color, creating a wide range of colors when combined. Grayscale images simplify this by displaying only shades of gray, reducing computational requirements and focusing on shapes and textures. Understanding pixels and color channels is foundational for image manipulation and analysis, as these elements define the structure and appearance of digital visuals.\n\n\n\nConstructing and manipulating images using NumPy matrices provides precise control over individual pixel regions. Representing an image as a matrix allows for adjustments to brightness, contrast, or specific colors by altering matrix values. Manipulating specific pixel regions can emphasize or obscure areas within an image, supporting tasks like highlighting points of interest in sustainability visuals. Using NumPy for image processing is powerful in computer vision as it enables direct, customizable changes to image data, preparing images for model analysis.\n\n\n\nUnderstanding image content features, such as color histograms and edges, is essential for analyzing and interpreting images. Color histograms reveal the distribution of colors in an image, helping identify dominant tones, while edge detection algorithms outline shapes and boundaries, crucial for object recognition tasks. Texture features capture surface variations, aiding in the differentiation of smooth and rough surfaces. These features provide a structured understanding of images, enabling computer vision models to detect patterns and make sense of visual data for diverse applications.\n\n\n\nDigital images can be represented as dataframes or matrices, with libraries like OpenCV facilitating this transformation. A matrix structure stores spatial information about pixels, enabling efficient analysis and comparison across images. Dataframes provide additional flexibility, organizing pixel values into a structured format for advanced data processing. Accessing image data in structured forms, like matrices, allows for detailed examination of image content, supporting tasks such as object detection and pattern recognition in computer vision applications.\n\n\n\nNormalizing image content enhances consistency across datasets, making inputs suitable for analysis by computer vision models. Resizing images ensures uniform dimensions, while grayscale conversion reduces complexity, focusing models on shapes rather than colors. Brightness normalization further enhances consistency, allowing models to interpret images reliably despite varying lighting conditions. These preprocessing steps are essential in computer vision as they reduce the variability of image inputs, helping AI models perform consistently across different visual data sources.",
    "crumbs": [
      "module 4",
      "lesson 4.1",
      "video lecture 4.1.1"
    ]
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html#lesson-4.1-computer-vision-cv-in-social-science",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture.html#lesson-4.1-computer-vision-cv-in-social-science",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "",
    "text": "In sustainability communication, visuals play diverse and essential roles, helping to convey messages effectively and inspire action. Photographs can evoke empathy by illustrating real-world environmental or social issues, while infographics break down complex data into digestible insights. Videos offer immersive storytelling, capturing time-based changes in climate or pollution. Charts and maps further enhance understanding by representing geographic data or environmental statistics visually. Choosing the right type of visual for the message strengthens the impact of sustainability communication, making data more relatable and motivating audiences toward sustainable practices.\n\n\n\nComputer vision encompasses various applications such as facial recognition, object detection, and environmental monitoring, each posing unique challenges. For example, recognizing objects under varying lighting conditions or from different angles is complex and often demands robust datasets and model fine-tuning. In sustainability contexts, computer vision helps detect patterns in satellite imagery for deforestation monitoring or pollution tracking. Challenges in image quality, such as low resolution or noise, add further complexity. Addressing these issues enhances the accuracy and reliability of computer vision applications in diverse fields.\n\n\n\nImages consist of pixels, the smallest units of a digital image, each carrying information about color or intensity. In color images, RGB (Red, Green, Blue) channels control the intensity of each primary color, creating a wide range of colors when combined. Grayscale images simplify this by displaying only shades of gray, reducing computational requirements and focusing on shapes and textures. Understanding pixels and color channels is foundational for image manipulation and analysis, as these elements define the structure and appearance of digital visuals.\n\n\n\nConstructing and manipulating images using NumPy matrices provides precise control over individual pixel regions. Representing an image as a matrix allows for adjustments to brightness, contrast, or specific colors by altering matrix values. Manipulating specific pixel regions can emphasize or obscure areas within an image, supporting tasks like highlighting points of interest in sustainability visuals. Using NumPy for image processing is powerful in computer vision as it enables direct, customizable changes to image data, preparing images for model analysis.\n\n\n\nUnderstanding image content features, such as color histograms and edges, is essential for analyzing and interpreting images. Color histograms reveal the distribution of colors in an image, helping identify dominant tones, while edge detection algorithms outline shapes and boundaries, crucial for object recognition tasks. Texture features capture surface variations, aiding in the differentiation of smooth and rough surfaces. These features provide a structured understanding of images, enabling computer vision models to detect patterns and make sense of visual data for diverse applications.\n\n\n\nDigital images can be represented as dataframes or matrices, with libraries like OpenCV facilitating this transformation. A matrix structure stores spatial information about pixels, enabling efficient analysis and comparison across images. Dataframes provide additional flexibility, organizing pixel values into a structured format for advanced data processing. Accessing image data in structured forms, like matrices, allows for detailed examination of image content, supporting tasks such as object detection and pattern recognition in computer vision applications.\n\n\n\nNormalizing image content enhances consistency across datasets, making inputs suitable for analysis by computer vision models. Resizing images ensures uniform dimensions, while grayscale conversion reduces complexity, focusing models on shapes rather than colors. Brightness normalization further enhances consistency, allowing models to interpret images reliably despite varying lighting conditions. These preprocessing steps are essential in computer vision as they reduce the variability of image inputs, helping AI models perform consistently across different visual data sources.",
    "crumbs": [
      "module 4",
      "lesson 4.1",
      "video lecture 4.1.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "!pip install -q pdfminer.six\nimport os\nfrom pdfminer.high_level import extract_text\n\n# Directories containing the PDFs\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.pdf'):\n                pdf_path = os.path.join(root, file)\n                text_path = os.path.splitext(pdf_path)[0] + '.txt'\n\n                try:\n                    text = extract_text(pdf_path)\n                    with open(text_path, 'w', encoding='utf-8') as f:\n                        f.write(text)\n                    print(f\"Converted {pdf_path} to {text_path}\")\n                except Exception as e:\n                    print(f\"Failed to convert {pdf_path}: {e}\")\nImporting Necessary Libraries\nThe code begins by importing essential modules. It imports os for interacting with the operating system’s file system and extract_text from pdfminer.high_level for extracting text content from PDF files.\n\nDefining the Directories Containing PDFs\nTwo lists named directories are defined. The first is a placeholder with ['organization1', 'organization2'], and the second specifies the actual paths to the directories containing the PDF files: - /content/osm-cca-nlp/res/pdf/preem - /content/osm-cca-nlp/res/pdf/vattenfall\n\nIterating Over Each Directory\nThe code uses a for loop to iterate through each directory specified in the directories list. This allows the program to process multiple directories sequentially.\n\nWalking Through Directory Trees\nWithin each directory, the os.walk(directory) function traverses the directory tree. It yields a tuple containing the root path, a list of dirs (subdirectories), and a list of files in each directory.\n\nIdentifying PDF Files\nFor every file in the files list, the code checks if the file name ends with .pdf (case-insensitive) using file.lower().endswith('.pdf'). This ensures that only PDF files are processed.\n\nConstructing File Paths\nThe full path to the PDF file is constructed using os.path.join(root, file). The corresponding text file path is created by replacing the .pdf extension with .txt using os.path.splitext(pdf_path)[0] + '.txt'.\n\nExtracting Text from PDFs\nA try block is initiated to attempt text extraction. The extract_text(pdf_path) function reads the content of the PDF file and stores it in the variable text.\n\nWriting Extracted Text to Files\nIf text extraction is successful, the code opens a new text file at text_path in write mode with UTF-8 encoding. It writes the extracted text into this file and then closes it, ensuring the text is saved next to the original PDF.\n\nLogging Successful Conversions\nAfter successfully writing the text file, the code prints a message indicating the PDF file has been converted, using:\nprint(f\"Converted {pdf_path} to {text_path}\")\n\nHandling Exceptions\nAn except block catches any exceptions that occur during the extraction or writing process. If an error occurs, it prints a failure message with the path of the PDF file and the exception details:\nprint(f\"Failed to convert {pdf_path}: {e}\")\n\n\n\n\nimport os\nimport pandas as pd\nimport re\nimport string\n\n# Directories containing the text files\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\ndata = []\ntext_index = 1\n\n# Allowed characters: alphabetic, punctuation, and whitespace\nallowed_chars = set(string.ascii_letters + string.punctuation + string.whitespace)\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.txt'):\n                file_path = os.path.join(root, file)\n                folder_name = os.path.basename(root)\n\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    raw_text = f.read()\n\n                # Keep only allowed characters\n                clean_text = ''.join(c for c in raw_text if c in allowed_chars)\n\n                # Replace sequences of whitespace with a single space\n                clean_text = re.sub(r'\\s+', ' ', clean_text)\n\n                # Trim leading and trailing whitespace\n                clean_text = clean_text.strip()\n\n                data.append({\n                    'text_index': text_index,\n                    'file_path': file_path,\n                    'folder_name': folder_name,\n                    'raw_text': raw_text,\n                    'clean_text': clean_text\n                })\n\n                text_index += 1\n\n# Create DataFrame\ndf_texts = pd.DataFrame(data, columns=['text_index', 'file_path', 'folder_name', 'raw_text', 'clean_text'])\n\n# Save DataFrame to TSV file\ndf_texts.to_csv('df_texts.tsv', sep='\\t', index=False)\ndf_texts.head()",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#lesson-3.1-reading-text-into-dataframes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#lesson-3.1-reading-text-into-dataframes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "!pip install -q pdfminer.six\nimport os\nfrom pdfminer.high_level import extract_text\n\n# Directories containing the PDFs\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.pdf'):\n                pdf_path = os.path.join(root, file)\n                text_path = os.path.splitext(pdf_path)[0] + '.txt'\n\n                try:\n                    text = extract_text(pdf_path)\n                    with open(text_path, 'w', encoding='utf-8') as f:\n                        f.write(text)\n                    print(f\"Converted {pdf_path} to {text_path}\")\n                except Exception as e:\n                    print(f\"Failed to convert {pdf_path}: {e}\")\nImporting Necessary Libraries\nThe code begins by importing essential modules. It imports os for interacting with the operating system’s file system and extract_text from pdfminer.high_level for extracting text content from PDF files.\n\nDefining the Directories Containing PDFs\nTwo lists named directories are defined. The first is a placeholder with ['organization1', 'organization2'], and the second specifies the actual paths to the directories containing the PDF files: - /content/osm-cca-nlp/res/pdf/preem - /content/osm-cca-nlp/res/pdf/vattenfall\n\nIterating Over Each Directory\nThe code uses a for loop to iterate through each directory specified in the directories list. This allows the program to process multiple directories sequentially.\n\nWalking Through Directory Trees\nWithin each directory, the os.walk(directory) function traverses the directory tree. It yields a tuple containing the root path, a list of dirs (subdirectories), and a list of files in each directory.\n\nIdentifying PDF Files\nFor every file in the files list, the code checks if the file name ends with .pdf (case-insensitive) using file.lower().endswith('.pdf'). This ensures that only PDF files are processed.\n\nConstructing File Paths\nThe full path to the PDF file is constructed using os.path.join(root, file). The corresponding text file path is created by replacing the .pdf extension with .txt using os.path.splitext(pdf_path)[0] + '.txt'.\n\nExtracting Text from PDFs\nA try block is initiated to attempt text extraction. The extract_text(pdf_path) function reads the content of the PDF file and stores it in the variable text.\n\nWriting Extracted Text to Files\nIf text extraction is successful, the code opens a new text file at text_path in write mode with UTF-8 encoding. It writes the extracted text into this file and then closes it, ensuring the text is saved next to the original PDF.\n\nLogging Successful Conversions\nAfter successfully writing the text file, the code prints a message indicating the PDF file has been converted, using:\nprint(f\"Converted {pdf_path} to {text_path}\")\n\nHandling Exceptions\nAn except block catches any exceptions that occur during the extraction or writing process. If an error occurs, it prints a failure message with the path of the PDF file and the exception details:\nprint(f\"Failed to convert {pdf_path}: {e}\")\n\n\n\n\nimport os\nimport pandas as pd\nimport re\nimport string\n\n# Directories containing the text files\ndirectories = ['organization1', 'organization2']\ndirectories = ['/content/osm-cca-nlp/res/pdf/preem', '/content/osm-cca-nlp/res/pdf/vattenfall']\n\ndata = []\ntext_index = 1\n\n# Allowed characters: alphabetic, punctuation, and whitespace\nallowed_chars = set(string.ascii_letters + string.punctuation + string.whitespace)\n\nfor directory in directories:\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith('.txt'):\n                file_path = os.path.join(root, file)\n                folder_name = os.path.basename(root)\n\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    raw_text = f.read()\n\n                # Keep only allowed characters\n                clean_text = ''.join(c for c in raw_text if c in allowed_chars)\n\n                # Replace sequences of whitespace with a single space\n                clean_text = re.sub(r'\\s+', ' ', clean_text)\n\n                # Trim leading and trailing whitespace\n                clean_text = clean_text.strip()\n\n                data.append({\n                    'text_index': text_index,\n                    'file_path': file_path,\n                    'folder_name': folder_name,\n                    'raw_text': raw_text,\n                    'clean_text': clean_text\n                })\n\n                text_index += 1\n\n# Create DataFrame\ndf_texts = pd.DataFrame(data, columns=['text_index', 'file_path', 'folder_name', 'raw_text', 'clean_text'])\n\n# Save DataFrame to TSV file\ndf_texts.to_csv('df_texts.tsv', sep='\\t', index=False)\ndf_texts.head()",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#token-relationships-and-knowledge-graphs",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#token-relationships-and-knowledge-graphs",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Token Relationships and Knowledge Graphs",
    "text": "Token Relationships and Knowledge Graphs\n\n\n\n\nTokens form the building blocks of text relationships and meaning in NLP.\n\nAspect-based sentiment analysis identifies sentiment tied to specific aspects or topics.\n\nKnowledge graphs map relationships between entities to provide contextual insights.\n\nToken relationships enhance machine understanding of complex linguistic structures.\n\nThese concepts drive applications in sentiment analysis, recommendations, and chatbots."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#units-of-nlp-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#units-of-nlp-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Units of NLP Analysis",
    "text": "Units of NLP Analysis\n\n\nNLP processes text at various levels: texts, paragraphs, sentences, words, and tokens.\n\nTexts provide overarching narratives; tokens are the smallest meaningful units.\n\nParagraphs and sentences act as natural segmentation points for processing.\n\nTokens are annotated with attributes like part-of-speech and syntactic role.\n\nUnderstanding these units is key to granular and scalable text analysis."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#applying-spacy-nlp-models-to-dataframes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#applying-spacy-nlp-models-to-dataframes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Applying SpaCy NLP Models to Dataframes",
    "text": "Applying SpaCy NLP Models to Dataframes\n\nSpaCy provides pre-trained NLP models for efficient text processing.\n\nText and sentence dataframes integrate structured data with NLP outputs.\n\nNLP models parse and enrich text with token, dependency, and entity annotations.\n\nBatch processing of text improves efficiency for large datasets.\n\nPractical applications include text classification, summarization, and sentiment analysis."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#iterating-over-sentence-dataframes-and-spacy-documents",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#iterating-over-sentence-dataframes-and-spacy-documents",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Iterating Over Sentence Dataframes and SpaCy Documents",
    "text": "Iterating Over Sentence Dataframes and SpaCy Documents\n\nSentence-level dataframes organize text into manageable units for analysis.\n\nSpaCy document objects provide linguistic annotations for each token.\n\nIterating enables extraction of sentence-specific attributes like entities or sentiments.\n\nCombines structured data analysis with NLP insights for robust results.\n\nStreamlines processes such as summarization, search indexing, and context identification."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-normalization-and-token-attributes",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-normalization-and-token-attributes",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Normalization and Token Attributes",
    "text": "Text Normalization and Token Attributes\n\nNormalization reduces text variability by standardizing tokens.\n\nLemmatization extracts the base form of words for consistent analysis.\n\nToken attributes, such as text and lemma, enable detailed linguistic understanding.\n\nImproved token consistency enhances downstream NLP tasks like matching and clustering.\n\nNormalization is essential for multilingual and noisy text processing."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-inference-named-entity-recognition-ner",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-inference-named-entity-recognition-ner",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Inference: Named Entity Recognition (NER)",
    "text": "Text Inference: Named Entity Recognition (NER)\n\nNER identifies and categorizes entities like names, organizations, and dates.\n\nExtracted entities provide structured insights from unstructured text.\n\nApplications include content categorization, fraud detection, and customer sentiment analysis.\n\nNER supports personalized recommendations and enhanced search capabilities.\n\nIt is fundamental to building knowledge graphs and question-answering systems."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-inference-part-of-speech-tagging-pos",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture-slides.html#text-inference-part-of-speech-tagging-pos",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Inference: Part-of-Speech Tagging (POS)",
    "text": "Text Inference: Part-of-Speech Tagging (POS)\n\nPOS tagging assigns grammatical roles to tokens like nouns, verbs, and adjectives.\n\nCaptures the syntactic structure of text for deeper linguistic understanding.\n\nApplications include syntax parsing, machine translation, and text generation.\n\nPOS tagging enhances accuracy in sentiment analysis and topic modeling.\n\nSupports advanced NLP tasks like dependency parsing and coreference resolution."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "Quantitative content analysis systematically evaluates content features in texts, such as frequency of themes, sentiment, or specific terms, to derive measurable insights. Text-level features, like overall tone or narrative structure, contrast with word-level features, such as keyword counts or lexical diversity. Traditionally, human coders offer flexibility in interpreting nuanced meanings but face limitations in scalability due to time and resource constraints. Conversely, AI-aided coding, leveraging natural language processing (NLP), sacrifices some interpretive flexibility for enhanced scalability, enabling rapid analysis of large datasets. This approach excels in identifying patterns, such as sustainability-related terms across corporate reports. Quantitative content analysis complements qualitative content analysis by providing numerical rigor to validate or challenge qualitative interpretations. For instance, while qualitative analysis might explore the context of sustainability claims, quantitative methods count their occurrences or measure their prominence, offering a robust foundation for comparing communication strategies across organizations. Integrating both approaches ensures a comprehensive understanding of sustainability communication, balancing depth with breadth in analyzing textual data.\n\n\n\nOperationalizing sustainability in NLP analysis involves defining metrics to distinguish authentic sustainability communication from greenwashing, addressing the research question of communication integrity. Authentic sustainability communication is characterized by specific, measurable indicators, such as detailed environmental impact metrics or commitments to renewable energy. In contrast, greenwashing may feature vague terms, exaggerated claims, or lack of verifiable data. NLP techniques like named entity recognition (NER) identify key entities (e.g., organizations, policies) and their relationships, revealing networks of accountability or obfuscation. Part-of-speech (POS) analysis further dissects texts by examining nouns (e.g., “emissions”), verbs (e.g., “reduce”), and adjectives (e.g., “sustainable”), which signal intent or emphasis. For example, authentic communication might use precise verbs like “implemented” rather than ambiguous ones like “aimed.” By quantifying these linguistic elements, researchers can systematically evaluate the credibility of sustainability claims. This approach enables a structured comparison of corporate narratives, ensuring that sustainability communication is not only rhetorically compelling but also substantively grounded in actionable and transparent practices.\n\n\n\nComparing sustainability communication across organizations, such as Preem (fossil fuel) and Vattenfall (renewable energy), reveals distinct rhetorical strategies shaped by their operational contexts. Fossil fuel companies like Preem may emphasize mitigation efforts, such as carbon capture, to counter environmental criticism, while renewable energy firms like Vattenfall might highlight innovation and clean energy achievements. These differences manifest in word choice, thematic focus, and narrative tone, detectable through NLP analysis. For instance, Preem’s texts might feature terms like “efficiency” or “transition,” whereas Vattenfall’s could prioritize “renewable” or “zero-emission.” Quantitative analysis of these features, such as term frequency or sentiment scores, allows researchers to map organizational priorities and assess alignment with sustainability goals. Expected differences also stem from public perception pressures: fossil fuel companies face greater scrutiny, potentially leading to defensive or compensatory messaging. By systematically comparing these patterns, NLP analysis provides evidence-based insights into how organizational type influences sustainability communication, enabling stakeholders to evaluate authenticity and strategic intent across diverse energy sectors.\n\n\n\nSummarizing NLP results transforms complex token-level analysis dataframes, which detail individual words or entities, into accessible insights. These raw dataframes, often dense with columns like token frequency or entity type, are challenging to interpret directly. Generating a new dataframe with summary statistics simplifies this by aggregating key metrics, such as content category counts (e.g., “sustainability” vs. “innovation” mentions) or sentiment measurements. The dependent variable, such as the frequency of a content category, is analyzed against independent variables like word type (e.g., nouns vs. adjectives) or organizational type (e.g., fossil fuel vs. renewable). For example, a summary dataframe might reveal that renewable energy firms use more positive adjectives than fossil fuel companies. This aggregated view highlights trends and differences without overwhelming stakeholders with granular data. By focusing on high-level patterns, summarizing ensures that findings are actionable, facilitating communication of results to diverse audiences, from researchers to corporate decision-makers, while retaining the analytical rigor of the original NLP process.\n\n\n\nSelecting, filtering, and aggregating data are critical steps in refining NLP datasets for meaningful interpretation. Selecting relevant columns, such as token entity (e.g., “Preem” or “carbon”) or part-of-speech tags (e.g., “adjective”), focuses analysis on variables pertinent to sustainability communication. Filtering removes irrelevant or incomplete data, such as rows with null values or tokens below a minimum frequency threshold, ensuring data quality. For instance, excluding low-count tokens reduces noise from rare or insignificant terms. Aggregation then computes summary metrics, such as category counts (e.g., frequency of “renewable” mentions) or measurement means (e.g., average sentiment score) per organization. This process might reveal, for example, that Vattenfall’s texts contain higher counts of “sustainability” than Preem’s. By systematically narrowing and synthesizing the dataset, these steps transform raw NLP outputs into structured insights. This enables researchers to address specific research questions, such as comparing authentic sustainability claims, with clarity and precision, supporting robust conclusions about organizational communication strategies.\n\n\n\nData visualization enhances the interpretability of NLP results, offering a more intuitive alternative to summary tables. By representing complex data—such as term frequencies or sentiment scores—visually, charts and graphs make patterns immediately apparent. Options include bar plots, word clouds, or heatmaps, each suited to different insights (e.g., word clouds for keyword prominence, heatmaps for correlations). Simple visualizations, like bar plots comparing sustainability term counts across organizations, are often more effective than complex ones, as they avoid overwhelming viewers. AI-aided data analysis streamlines visualization by automating data processing and integrating tools like Python’s Matplotlib or Seaborn, reducing manual effort. For instance, a bar plot might vividly contrast Preem’s focus on “efficiency” with Vattenfall’s on “renewable energy,” making differences accessible to non-technical stakeholders. Visualizations thus bridge the gap between raw NLP outputs and actionable insights, enabling researchers to communicate findings effectively while highlighting key trends in sustainability communication with clarity and impact.\n\n\n\nStacked bar plots are an effective tool for visualizing NLP results, particularly for comparing sustainability communication across organizations. For a single organization, simple bar plots can display metrics like the frequency of content categories (e.g., “sustainability” vs. “innovation”). Stacked bar plots extend this to bivariate or multivariate analysis by showing multiple variables within each bar, such as the proportion of nouns, verbs, and adjectives in sustainability-related texts across organizations like Preem and Vattenfall. Each segment of the bar represents a variable (e.g., word type), with the total bar height indicating overall frequency. This approach highlights differences, such as Vattenfall’s higher use of positive adjectives compared to Preem’s noun-heavy focus on “emissions.” Stacked bar plots thus provide a clear, comparative view of complex data, making it easier to identify patterns and trends. By visualizing multivariate relationships intuitively, they support researchers in communicating nuanced findings about organizational sustainability narratives to diverse audiences, from academics to industry stakeholders.",
    "crumbs": [
      "module 3",
      "lesson 3.3",
      "video lecture 3.3.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture.html#lesson-3.3-interpreting-the-results-of-nlp-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture.html#lesson-3.3-interpreting-the-results-of-nlp-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "Quantitative content analysis systematically evaluates content features in texts, such as frequency of themes, sentiment, or specific terms, to derive measurable insights. Text-level features, like overall tone or narrative structure, contrast with word-level features, such as keyword counts or lexical diversity. Traditionally, human coders offer flexibility in interpreting nuanced meanings but face limitations in scalability due to time and resource constraints. Conversely, AI-aided coding, leveraging natural language processing (NLP), sacrifices some interpretive flexibility for enhanced scalability, enabling rapid analysis of large datasets. This approach excels in identifying patterns, such as sustainability-related terms across corporate reports. Quantitative content analysis complements qualitative content analysis by providing numerical rigor to validate or challenge qualitative interpretations. For instance, while qualitative analysis might explore the context of sustainability claims, quantitative methods count their occurrences or measure their prominence, offering a robust foundation for comparing communication strategies across organizations. Integrating both approaches ensures a comprehensive understanding of sustainability communication, balancing depth with breadth in analyzing textual data.\n\n\n\nOperationalizing sustainability in NLP analysis involves defining metrics to distinguish authentic sustainability communication from greenwashing, addressing the research question of communication integrity. Authentic sustainability communication is characterized by specific, measurable indicators, such as detailed environmental impact metrics or commitments to renewable energy. In contrast, greenwashing may feature vague terms, exaggerated claims, or lack of verifiable data. NLP techniques like named entity recognition (NER) identify key entities (e.g., organizations, policies) and their relationships, revealing networks of accountability or obfuscation. Part-of-speech (POS) analysis further dissects texts by examining nouns (e.g., “emissions”), verbs (e.g., “reduce”), and adjectives (e.g., “sustainable”), which signal intent or emphasis. For example, authentic communication might use precise verbs like “implemented” rather than ambiguous ones like “aimed.” By quantifying these linguistic elements, researchers can systematically evaluate the credibility of sustainability claims. This approach enables a structured comparison of corporate narratives, ensuring that sustainability communication is not only rhetorically compelling but also substantively grounded in actionable and transparent practices.\n\n\n\nComparing sustainability communication across organizations, such as Preem (fossil fuel) and Vattenfall (renewable energy), reveals distinct rhetorical strategies shaped by their operational contexts. Fossil fuel companies like Preem may emphasize mitigation efforts, such as carbon capture, to counter environmental criticism, while renewable energy firms like Vattenfall might highlight innovation and clean energy achievements. These differences manifest in word choice, thematic focus, and narrative tone, detectable through NLP analysis. For instance, Preem’s texts might feature terms like “efficiency” or “transition,” whereas Vattenfall’s could prioritize “renewable” or “zero-emission.” Quantitative analysis of these features, such as term frequency or sentiment scores, allows researchers to map organizational priorities and assess alignment with sustainability goals. Expected differences also stem from public perception pressures: fossil fuel companies face greater scrutiny, potentially leading to defensive or compensatory messaging. By systematically comparing these patterns, NLP analysis provides evidence-based insights into how organizational type influences sustainability communication, enabling stakeholders to evaluate authenticity and strategic intent across diverse energy sectors.\n\n\n\nSummarizing NLP results transforms complex token-level analysis dataframes, which detail individual words or entities, into accessible insights. These raw dataframes, often dense with columns like token frequency or entity type, are challenging to interpret directly. Generating a new dataframe with summary statistics simplifies this by aggregating key metrics, such as content category counts (e.g., “sustainability” vs. “innovation” mentions) or sentiment measurements. The dependent variable, such as the frequency of a content category, is analyzed against independent variables like word type (e.g., nouns vs. adjectives) or organizational type (e.g., fossil fuel vs. renewable). For example, a summary dataframe might reveal that renewable energy firms use more positive adjectives than fossil fuel companies. This aggregated view highlights trends and differences without overwhelming stakeholders with granular data. By focusing on high-level patterns, summarizing ensures that findings are actionable, facilitating communication of results to diverse audiences, from researchers to corporate decision-makers, while retaining the analytical rigor of the original NLP process.\n\n\n\nSelecting, filtering, and aggregating data are critical steps in refining NLP datasets for meaningful interpretation. Selecting relevant columns, such as token entity (e.g., “Preem” or “carbon”) or part-of-speech tags (e.g., “adjective”), focuses analysis on variables pertinent to sustainability communication. Filtering removes irrelevant or incomplete data, such as rows with null values or tokens below a minimum frequency threshold, ensuring data quality. For instance, excluding low-count tokens reduces noise from rare or insignificant terms. Aggregation then computes summary metrics, such as category counts (e.g., frequency of “renewable” mentions) or measurement means (e.g., average sentiment score) per organization. This process might reveal, for example, that Vattenfall’s texts contain higher counts of “sustainability” than Preem’s. By systematically narrowing and synthesizing the dataset, these steps transform raw NLP outputs into structured insights. This enables researchers to address specific research questions, such as comparing authentic sustainability claims, with clarity and precision, supporting robust conclusions about organizational communication strategies.\n\n\n\nData visualization enhances the interpretability of NLP results, offering a more intuitive alternative to summary tables. By representing complex data—such as term frequencies or sentiment scores—visually, charts and graphs make patterns immediately apparent. Options include bar plots, word clouds, or heatmaps, each suited to different insights (e.g., word clouds for keyword prominence, heatmaps for correlations). Simple visualizations, like bar plots comparing sustainability term counts across organizations, are often more effective than complex ones, as they avoid overwhelming viewers. AI-aided data analysis streamlines visualization by automating data processing and integrating tools like Python’s Matplotlib or Seaborn, reducing manual effort. For instance, a bar plot might vividly contrast Preem’s focus on “efficiency” with Vattenfall’s on “renewable energy,” making differences accessible to non-technical stakeholders. Visualizations thus bridge the gap between raw NLP outputs and actionable insights, enabling researchers to communicate findings effectively while highlighting key trends in sustainability communication with clarity and impact.\n\n\n\nStacked bar plots are an effective tool for visualizing NLP results, particularly for comparing sustainability communication across organizations. For a single organization, simple bar plots can display metrics like the frequency of content categories (e.g., “sustainability” vs. “innovation”). Stacked bar plots extend this to bivariate or multivariate analysis by showing multiple variables within each bar, such as the proportion of nouns, verbs, and adjectives in sustainability-related texts across organizations like Preem and Vattenfall. Each segment of the bar represents a variable (e.g., word type), with the total bar height indicating overall frequency. This approach highlights differences, such as Vattenfall’s higher use of positive adjectives compared to Preem’s noun-heavy focus on “emissions.” Stacked bar plots thus provide a clear, comparative view of complex data, making it easier to identify patterns and trends. By visualizing multivariate relationships intuitively, they support researchers in communicating nuanced findings about organizational sustainability narratives to diverse audiences, from academics to industry stakeholders.",
    "crumbs": [
      "module 3",
      "lesson 3.3",
      "video lecture 3.3.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#quantitative-content-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#quantitative-content-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Quantitative content analysis",
    "text": "Quantitative content analysis\n\n\n\nSystematically evaluates text features like theme frequency and sentiment.\n\nText-level (e.g., tone) differs from word-level (e.g., keyword counts).\n\nHuman coders offer flexibility but lack scalability for large datasets.\n\nAI-aided coding sacrifices nuance for rapid, scalable analysis.\n\nComplements qualitative analysis, validating insights with numerical rigor.\n\n\n\n\n\n\n\n📰"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#operationalizing-sustainability",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#operationalizing-sustainability",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Operationalizing sustainability",
    "text": "Operationalizing sustainability\n\n\n\nDistinguishes authentic sustainability from greenwashing via metrics.\n\nAuthentic communication uses specific, measurable environmental metrics.\n\nGreenwashing features vague terms and unverifiable claims.\n\nNamed entity recognition maps entities and their relationships.\n\nPart-of-speech analysis reveals intent through nouns, verbs, adjectives.\n\n\n\n\n\n\n\n♻️"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#comparison-across-organizations",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#comparison-across-organizations",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Comparison across organizations",
    "text": "Comparison across organizations\n\n\n\nCompares Preem (fossil fuel) and Vattenfall (renewable energy).\n\nFossil fuel firms emphasize mitigation; renewables highlight innovation.\n\nNLP detects differences in word choice and narrative tone.\n\nPublic scrutiny shapes fossil fuel firms’ defensive messaging.\n\nQuantifies alignment with sustainability goals across sectors.\n\n\n\n\n\n\n\n⚖️"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#summarizing-results-of-text-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#summarizing-results-of-text-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Summarizing results of text analysis",
    "text": "Summarizing results of text analysis\n\n\n\nSimplifies complex token-level dataframes for better readability.\n\nAggregates metrics like content category counts or sentiment scores.\n\nAnalyzes dependent variables (e.g., category frequency) against independents.\n\nHighlights trends, e.g., adjective use by organization type.\n\nEnsures findings are actionable for diverse stakeholders.\n\n\n\n\n\n\n\n📈"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#select-filter-aggregate",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#select-filter-aggregate",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Select, filter, aggregate",
    "text": "Select, filter, aggregate\n\n\n\nSelects key columns like token entity or part-of-speech tags.\n\nFilters out noise, e.g., null values or low-frequency tokens.\n\nAggregates data to compute category counts or sentiment means.\n\nEnables precise comparisons, e.g., sustainability terms by organization.\n\nTransforms raw data into structured, research-ready insights.\n\n\n\n\n\n\n\n🎯"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#visualizing-results-of-text-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#visualizing-results-of-text-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Visualizing results of text analysis",
    "text": "Visualizing results of text analysis\n\n\n\nVisualizations make NLP results intuitive compared to tables.\n\nOptions include bar plots, word clouds, and heatmaps.\n\nSimple visuals (e.g., bar plots) are clearer than complex ones.\n\nAI tools like Matplotlib streamline visualization processes.\n\nHighlights trends, e.g., term frequency differences across firms.\n\n\n\n\n\n\n\n🎨"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#stacked-bar-plots",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-1-video-lecture-slides.html#stacked-bar-plots",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Stacked bar plots",
    "text": "Stacked bar plots\n\n\n\nSimple bar plots show single-organization metrics like category frequency.\n\nStacked bar plots compare multiple variables across organizations.\n\nSegments represent variables (e.g., word types) within bars.\n\nReveals differences, e.g., adjective use in Preem vs. Vattenfall.\n\nSupports multivariate analysis for clear, comparative insights.\n\n\n\n\n\n📊"
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#open-science-methods",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module05-ethical-aspects-of-ai-aided-content-analysis-10pct/5-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 5: Ethical aspects of AI-aided content analysis",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-2",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-2",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "lesson 3-2",
    "text": "lesson 3-2\n\nvideo-lecture.qmd:Part-of-speech and named entity recognition\n\n\ncomputer-lab.qmd:Inferential text analysis, tokenization\n\n\ncomputer-lab.qmd:Inferential text analysis, POS and NER",
    "crumbs": [
      "module 3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-3",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#lesson-3-3",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "lesson 3-3",
    "text": "lesson 3-3\n\nvideo-lecture.qmd:Interpreting the results of NLP analysis\n\n\ncomputer-lab.qmd:Summarizing results of text analysis\n\n\ncomputer-lab.qmd:Visualizing results of text analysis",
    "crumbs": [
      "module 3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "3-1-1-video-lecture-slides.pdf\n\n\n\n\n\n\n\n\n\n\nTexts in sustainability communication serve varied purposes, including raising awareness, educating audiences, advocating for change, and influencing policies. Informational texts like reports and white papers provide detailed data and analyses, while persuasive content such as blogs and social media posts aim to inspire action. Visual- and data-supported texts often enhance engagement by presenting complex ideas in accessible formats. Understanding the intended function is critical for effective communication and targeted NLP applications.\n\n\n\nNatural Language Processing (NLP) covers diverse areas, from sentiment analysis to machine translation and summarization. Working with unstructured text presents challenges, such as handling ambiguous language, idiomatic expressions, and domain-specific jargon. Additionally, processing noisy data from social media or OCR errors in scanned documents often complicates the analysis. Effective NLP requires robust preprocessing pipelines and domain-specific adjustments to achieve meaningful results.\n\n\n\nIn NLP, text analysis begins with defining the units of analysis—such as sentences, words, or characters. Tokens, the smallest logical units of text, are derived from splitting strings into meaningful components. N-grams, sequences of n tokens, capture contextual relationships, with bigrams and trigrams being particularly useful for understanding phrases. Mastering these foundational concepts is essential for advanced text processing.\n\n\n\nText is often embedded in formats like PDFs, HTML, or Markdown, each presenting unique challenges for extraction. PDFs may include layout artifacts, while HTML contains tags that must be parsed. Converting these formats into clean plain text ensures compatibility with NLP tools. Tools like Tika, Beautiful Soup, and Markdown parsers simplify this conversion process, paving the way for structured analysis.\n\n\n\nKey features of text content include readability indices like the Flesch Reading Ease, which assess complexity, and linguistic features such as Part-of-Speech (POS) tagging, which categorizes words based on their grammatical role. Named Entity Recognition (NER) identifies and classifies proper nouns, dates, or other specific entities. These features offer insights into the style, structure, and meaning of the text, aiding both analysis and decision-making.\n\n\n\nTextual data is often ingested into dataframes for analysis, enabling structured workflows. This process involves normalization tasks, such as lowercasing and removing punctuation, followed by tokenization to split the text into analyzable units. Pandas and NLTK are popular tools for managing these steps, enabling efficient preparation of text for downstream NLP tasks.\n\n\n\nManifest content refers to the explicit, observable elements of a text, such as word counts, sentence lengths, and overall structure. Techniques like word frequency analysis reveal dominant themes and help identify key terms. Sentence and word counts provide quantitative measures of text characteristics, while visualization tools like word clouds offer intuitive insights into content prominence.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "video lecture 3.1.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html#lesson-3.1-natural-language-processing-nlp-in-social-science",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture.html#lesson-3.1-natural-language-processing-nlp-in-social-science",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "3-1-1-video-lecture-slides.pdf\n\n\n\n\n\n\n\n\n\n\nTexts in sustainability communication serve varied purposes, including raising awareness, educating audiences, advocating for change, and influencing policies. Informational texts like reports and white papers provide detailed data and analyses, while persuasive content such as blogs and social media posts aim to inspire action. Visual- and data-supported texts often enhance engagement by presenting complex ideas in accessible formats. Understanding the intended function is critical for effective communication and targeted NLP applications.\n\n\n\nNatural Language Processing (NLP) covers diverse areas, from sentiment analysis to machine translation and summarization. Working with unstructured text presents challenges, such as handling ambiguous language, idiomatic expressions, and domain-specific jargon. Additionally, processing noisy data from social media or OCR errors in scanned documents often complicates the analysis. Effective NLP requires robust preprocessing pipelines and domain-specific adjustments to achieve meaningful results.\n\n\n\nIn NLP, text analysis begins with defining the units of analysis—such as sentences, words, or characters. Tokens, the smallest logical units of text, are derived from splitting strings into meaningful components. N-grams, sequences of n tokens, capture contextual relationships, with bigrams and trigrams being particularly useful for understanding phrases. Mastering these foundational concepts is essential for advanced text processing.\n\n\n\nText is often embedded in formats like PDFs, HTML, or Markdown, each presenting unique challenges for extraction. PDFs may include layout artifacts, while HTML contains tags that must be parsed. Converting these formats into clean plain text ensures compatibility with NLP tools. Tools like Tika, Beautiful Soup, and Markdown parsers simplify this conversion process, paving the way for structured analysis.\n\n\n\nKey features of text content include readability indices like the Flesch Reading Ease, which assess complexity, and linguistic features such as Part-of-Speech (POS) tagging, which categorizes words based on their grammatical role. Named Entity Recognition (NER) identifies and classifies proper nouns, dates, or other specific entities. These features offer insights into the style, structure, and meaning of the text, aiding both analysis and decision-making.\n\n\n\nTextual data is often ingested into dataframes for analysis, enabling structured workflows. This process involves normalization tasks, such as lowercasing and removing punctuation, followed by tokenization to split the text into analyzable units. Pandas and NLTK are popular tools for managing these steps, enabling efficient preparation of text for downstream NLP tasks.\n\n\n\nManifest content refers to the explicit, observable elements of a text, such as word counts, sentence lengths, and overall structure. Techniques like word frequency analysis reveal dominant themes and help identify key terms. Sentence and word counts provide quantitative measures of text characteristics, while visualization tools like word clouds offer intuitive insights into content prominence.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "video lecture 3.1.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#functions-of-texts-in-sustainability-communication",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#functions-of-texts-in-sustainability-communication",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Functions of Texts in Sustainability Communication",
    "text": "Functions of Texts in Sustainability Communication\n\n\n\nInformational texts provide data-driven insights and factual details.\n\nPersuasive texts motivate audiences toward action or change.\n\nNarrative texts create emotional connections to sustainability themes.\n\nVisual-supported texts enhance accessibility and engagement.\n\nEach text type targets specific audiences and communication goals."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#nlp-areas-and-challenges-of-unstructured-text",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#nlp-areas-and-challenges-of-unstructured-text",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "NLP Areas and Challenges of Unstructured Text",
    "text": "NLP Areas and Challenges of Unstructured Text\n\n\n\nNLP applications include sentiment analysis, translation, and summarization.\n\nUnstructured text often contains ambiguous language and incomplete sentences.\n\nDomain-specific jargon and colloquialisms complicate processing.\n\nNoise in data sources like social media adds layers of preprocessing needs.\n\nRobust algorithms and preprocessing pipelines mitigate these challenges."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#basic-concepts-units-tokens-and-n-grams",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#basic-concepts-units-tokens-and-n-grams",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Basic Concepts: Units, Tokens, and N-grams",
    "text": "Basic Concepts: Units, Tokens, and N-grams\n\nText units include sentences, words, and characters as analytical building blocks.\n\nTokens are the smallest logical units, often derived from words.\n\nN-grams capture sequences of n tokens, revealing contextual patterns.\n\nCommon n-grams include bigrams (two words) and trigrams (three words).\n\nThese concepts underpin more advanced NLP tasks."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#formats-and-conversion-to-plain-text",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#formats-and-conversion-to-plain-text",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Formats and Conversion to Plain Text",
    "text": "Formats and Conversion to Plain Text\n\nText is often stored in formats like PDF, HTML, or Markdown.\n\nPDFs may contain layout artifacts, complicating text extraction.\n\nHTML requires parsing to remove tags and extract meaningful content.\n\nTools like Beautiful Soup and Tika streamline these conversions.\n\nConverting to plain text ensures compatibility with NLP workflows."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#text-features-readability-pos-and-ner",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#text-features-readability-pos-and-ner",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Text Features: Readability, POS, and NER",
    "text": "Text Features: Readability, POS, and NER\n\nReadability indices assess the complexity of written content.\n\nPOS tagging categorizes words by their grammatical function.\n\nNER identifies and classifies specific entities, such as names and dates.\n\nThese features provide insights into the style and structure of text.\n\nThey are essential for contextual and thematic understanding in NLP."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#reading-text-into-dataframes-and-preprocessing",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#reading-text-into-dataframes-and-preprocessing",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Reading Text into Dataframes and Preprocessing",
    "text": "Reading Text into Dataframes and Preprocessing\n\nDataframes structure text data for analysis and visualization.\n\nNormalization includes lowercasing and punctuation removal.\n\nTokenization splits text into analyzable units like words or phrases.\n\nPandas and NLTK are widely used for preprocessing workflows.\n\nClean, tokenized data is a prerequisite for most NLP tasks."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#manifest-text-content-and-frequency-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-1-video-lecture-slides.html#manifest-text-content-and-frequency-analysis",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "Manifest Text Content and Frequency Analysis",
    "text": "Manifest Text Content and Frequency Analysis\n\nManifest content refers to explicitly observable text elements.\n\nSentence and word counts provide quantitative content metrics.\n\nWord frequency analysis highlights key terms and dominant themes.\n\nVisualizations like word clouds offer intuitive insights into text data.\n\nThese metrics are foundational for exploratory text analysis."
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "Understanding token relationships is fundamental in NLP, where aspect-based sentiment analysis identifies sentiment tied to specific aspects of a text. Knowledge graphs further enrich this understanding by mapping relationships between entities, enabling sophisticated insights into connections and context within the data.\n\n\n\nNatural language processing operates across multiple levels of granularity, from complete texts and paragraphs to sentences, words, and individual tokens. Each unit provides unique insights, with tokens representing the smallest meaningful elements, forming the foundation for complex linguistic analysis.\n\n\n\nSpaCy’s powerful NLP models can be seamlessly integrated with text and sentence dataframes, enabling batch processing of textual data. This application simplifies linguistic analysis and provides structured outputs, such as token attributes and syntactic dependencies, directly usable for further processing.\n\n\n\nIterating over a sentence-level dataframe and corresponding SpaCy document objects allows detailed exploration of linguistic features. This method facilitates tasks like extracting sentence-specific attributes, analyzing syntactic structures, and identifying patterns within textual data.\n\n\n\nText normalization ensures consistency and clarity by standardizing tokens, extracting essential attributes like token text and lemma. Lemmatization, in particular, reduces words to their base forms, enhancing search, indexing, and overall NLP model performance.\n\n\n\nNamed entity recognition (NER) identifies and classifies entities such as names, organizations, and dates within text, enabling applications like automated content categorization, customer sentiment analysis, and improved information retrieval systems.\n\n\n\nPart-of-speech tagging assigns grammatical categories to tokens, such as nouns, verbs, or adjectives, providing critical insights into text structure. Applications include syntactic parsing, language generation, and improving machine translation systems by capturing grammatical nuances.",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "video lecture 3.2.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html#lesson-3.2-part-of-speech-and-named-entity-recognition",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-1-video-lecture.html#lesson-3.2-part-of-speech-and-named-entity-recognition",
    "title": "Module 3: Analyzing text content with natural language processing",
    "section": "",
    "text": "Understanding token relationships is fundamental in NLP, where aspect-based sentiment analysis identifies sentiment tied to specific aspects of a text. Knowledge graphs further enrich this understanding by mapping relationships between entities, enabling sophisticated insights into connections and context within the data.\n\n\n\nNatural language processing operates across multiple levels of granularity, from complete texts and paragraphs to sentences, words, and individual tokens. Each unit provides unique insights, with tokens representing the smallest meaningful elements, forming the foundation for complex linguistic analysis.\n\n\n\nSpaCy’s powerful NLP models can be seamlessly integrated with text and sentence dataframes, enabling batch processing of textual data. This application simplifies linguistic analysis and provides structured outputs, such as token attributes and syntactic dependencies, directly usable for further processing.\n\n\n\nIterating over a sentence-level dataframe and corresponding SpaCy document objects allows detailed exploration of linguistic features. This method facilitates tasks like extracting sentence-specific attributes, analyzing syntactic structures, and identifying patterns within textual data.\n\n\n\nText normalization ensures consistency and clarity by standardizing tokens, extracting essential attributes like token text and lemma. Lemmatization, in particular, reduces words to their base forms, enhancing search, indexing, and overall NLP model performance.\n\n\n\nNamed entity recognition (NER) identifies and classifies entities such as names, organizations, and dates within text, enabling applications like automated content categorization, customer sentiment analysis, and improved information retrieval systems.\n\n\n\nPart-of-speech tagging assigns grammatical categories to tokens, such as nouns, verbs, or adjectives, providing critical insights into text structure. Applications include syntactic parsing, language generation, and improving machine translation systems by capturing grammatical nuances.",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "video lecture 3.2.1"
    ]
  },
  {
    "objectID": "about.html#literature",
    "href": "about.html#literature",
    "title": "About",
    "section": "literature",
    "text": "literature\n\nsustainability communication\n\n\n\n\n\n\nWeder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\ncomputational content analysis, computer vision\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\ncomputational content analysis, natural language processing\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\ncomputational data analysis\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\ncourse literature\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)"
  },
  {
    "objectID": "about.html#websites-and-apps",
    "href": "about.html#websites-and-apps",
    "title": "About",
    "section": "websites and apps",
    "text": "websites and apps\n\nhttps://www.python.org/downloads/\nhttps://wiki.python.org/moin/BeginnersGuide\nhttps://www.anaconda.com/products/individual\nhttps://code.visualstudio.com/download\nhttps://github.com/jupyterlab/jupyterlab_app#download\nhttps://trinket.io/home"
  },
  {
    "objectID": "about.html#online-articles",
    "href": "about.html#online-articles",
    "title": "About",
    "section": "online articles",
    "text": "online articles\n\nsome text here Kedia and Rasu (2020)"
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#functions-of-visuals-in-sustainability-communication",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#functions-of-visuals-in-sustainability-communication",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Functions of Visuals in Sustainability Communication",
    "text": "Functions of Visuals in Sustainability Communication\n\n\n\nVisuals enhance message clarity and audience engagement in sustainability topics.\nPhotos evoke emotional responses, encouraging empathy and action.\nInfographics simplify complex data for easier public understanding.\nVideos convey real-time impact, offering immersive storytelling.\nVisual variety can reinforce key sustainability messages across contexts."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#computer-vision-areas-and-content-challenges",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#computer-vision-areas-and-content-challenges",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Computer Vision Areas and Content Challenges",
    "text": "Computer Vision Areas and Content Challenges\n\nComputer vision applications include object detection, facial recognition, and scene analysis.\nVariations in lighting and angle present challenges in accurate image interpretation.\nEnvironmental monitoring relies on detecting specific features in natural scenes.\nHandling diverse image sources requires robust algorithms and large datasets.\nComputer vision aids sustainability by tracking changes in environmental conditions."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-basics-pixels-rgb-and-grayscale",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-basics-pixels-rgb-and-grayscale",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Image Basics: Pixels, RGB, and Grayscale",
    "text": "Image Basics: Pixels, RGB, and Grayscale\n\nImages are composed of pixels, each storing color or intensity values.\nRGB channels (Red, Green, Blue) combine to create various colors in visuals.\nGrayscale simplifies images by converting them into shades of gray.\nEach color channel holds intensity values ranging from 0 to 255.\nUnderstanding these basics is essential for manipulating and analyzing images."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#constructing-and-manipulating-images-with-numpy",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#constructing-and-manipulating-images-with-numpy",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Constructing and Manipulating Images with NumPy",
    "text": "Constructing and Manipulating Images with NumPy\n\nImages can be represented as NumPy matrices, enabling pixel-based control.\nEach matrix element corresponds to a pixel’s intensity or color value.\nAdjusting matrix values allows for brightness or color changes in specific regions.\nImage manipulations aid in highlighting areas of interest or removing noise.\nUsing NumPy enhances flexibility in preprocessing images for computer vision tasks."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-features-colors-histograms-and-edges",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#image-features-colors-histograms-and-edges",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Image Features: Colors, Histograms, and Edges",
    "text": "Image Features: Colors, Histograms, and Edges\n\nImage content features help models identify patterns within visuals.\nColor histograms reveal dominant tones by analyzing pixel distributions.\nEdge detection outlines shapes, essential for object and scene recognition.\nTexture features assist in distinguishing smooth versus rough surfaces.\nFeatures enable AI to interpret images more effectively by capturing key visual aspects."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#reading-images-into-dataframes-and-matrix-structures",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#reading-images-into-dataframes-and-matrix-structures",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Reading Images into Dataframes and Matrix Structures",
    "text": "Reading Images into Dataframes and Matrix Structures\n\nTools like OpenCV allow images to be imported as matrices or dataframes.\nDataframes enable organization of pixel data for efficient analysis.\nMatrix structures store spatial information, which is crucial for vision models.\nImage data in structured formats simplifies comparisons across visuals.\nAccessing pixel-level data provides precision in analyzing image content."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#normalizing-image-content-resize-grayscale-and-consistency",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-1-1-video-lecture-slides.html#normalizing-image-content-resize-grayscale-and-consistency",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Normalizing Image Content: Resize, Grayscale, and Consistency",
    "text": "Normalizing Image Content: Resize, Grayscale, and Consistency\n\nImage normalization enhances model performance by standardizing inputs.\nResizing aligns images to a uniform size, aiding consistency in analysis.\nGrayscale conversion reduces complexity and focuses on essential shapes.\nBrightness adjustments help maintain consistency across varied image sources.\nNormalizing inputs is vital for achieving accurate, reliable computer vision results."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#open-science-methods",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module04-analyzing-image-content-computer-vision-30pct/4-3-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 4: Analyzing image content with computer vision",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Weder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "resources.html#literature",
    "href": "resources.html#literature",
    "title": "Resources",
    "section": "",
    "text": "Weder, Krainer, and Karmasin (2021)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDey (2018)\n\n\n\n\n\n\n\nLakshmanan, Görner, and Gillard (2021)\n\n\n\n\n\n\n\nVasilev (2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVasiliev (2020)\n\n\n\n\n\n\n\nTunstall, Von Werra, and Wolf (2022)\n\n\n\n\n\n\n\nSzeliski (2010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcKinney (2022)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n(some-test?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeuendorf (2017)\n\n\n\n\n\n\n\nKedia and Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "resources.html#websites-and-apps",
    "href": "resources.html#websites-and-apps",
    "title": "Resources",
    "section": "websites and apps",
    "text": "websites and apps\n\nhttps://www.python.org/downloads/\nhttps://wiki.python.org/moin/BeginnersGuide\nhttps://www.anaconda.com/products/individual\nhttps://code.visualstudio.com/download\nhttps://github.com/jupyterlab/jupyterlab_app#download\nhttps://trinket.io/home",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "resources.html#online-articles",
    "href": "resources.html#online-articles",
    "title": "Resources",
    "section": "online articles",
    "text": "online articles\n\nsome text here Kedia and Rasu (2020)",
    "crumbs": [
      "course start",
      "resources"
    ]
  },
  {
    "objectID": "aicasc-2-1-1-video-lecture.html#hello-there",
    "href": "aicasc-2-1-1-video-lecture.html#hello-there",
    "title": "Quarto Presentations",
    "section": "Hello, There",
    "text": "Hello, There\nThis presentation will show you examples of what you can do with Quarto and Reveal.js, including:\n\nPresenting code and LaTeX equations\nIncluding computations in slide output\nImage, video, and iframe backgrounds\nFancy transitions and animations\nPrinting to PDF\n\n…and much more"
  },
  {
    "objectID": "aicasc-2-1-1-video-lecture.html#pretty-code",
    "href": "aicasc-2-1-1-video-lecture.html#pretty-code",
    "title": "Quarto Presentations",
    "section": "Pretty Code",
    "text": "Pretty Code\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility\n\nimport seaborn as sns\n\n# Load the iris dataset\ndf = sns.load_dataset(\"iris\")\n\n# Display the first 5 rows\ndf.head()\n\nLearn more: Syntax Highlighting"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#open-science-methods",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#open-science-methods",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Open Science Methods",
    "text": "Open Science Methods\n\n\n\nEmphasizes reproducibility and transparency in research.\n\nEncourages collaboration among global research communities.\n\nRelies on sharing data, code, and methodologies openly.\n\nSupported by open-source tools and platforms.\n\nEnhances scientific integrity and innovation."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-environments-local-cloud-notebooks",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python Environments: Local, Cloud, Notebooks",
    "text": "Python Environments: Local, Cloud, Notebooks\n\n\n\nLocal environments require Python installation and configuration.\n\nCloud environments like Google Colab eliminate setup hurdles.\n\nNotebooks provide interactive code execution and documentation.\n\nCloud environments offer scalability and resource management.\n\nNotebooks support real-time code execution alongside markdown."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#the-concept-of-jupyter-notebooks",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#the-concept-of-jupyter-notebooks",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "The Concept of Jupyter Notebooks",
    "text": "The Concept of Jupyter Notebooks\n\n\n\nEnables interactive, step-wise execution of code.\n\nIntegrates code, text, and visualizations within one document.\n\nIdeal for iterative development, debugging, and documentation.\n\nSupports educational and research purposes effectively.\n\nContrasts with traditional script-based development environments."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#google-colab-notebook-interface",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#google-colab-notebook-interface",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Google Colab Notebook Interface",
    "text": "Google Colab Notebook Interface\n\n\n\nCombines a cloud-hosted Jupyter notebook with GPU access.\n\nFeatures cells for executing code and writing text.\n\nAuto-saves progress and links to Google Drive for file storage.\n\nSupports collaboration and sharing with other users.\n\nRequires no local setup, ideal for rapid project development."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#what-is-programming-and-python-programming",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#what-is-programming-and-python-programming",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "What Is Programming and Python Programming",
    "text": "What Is Programming and Python Programming\n\n\n\nProgramming involves writing instructions for computers to execute.\n\nPython is a high-level, versatile language known for readability.\n\nWidely used in web development, data science, and automation.\n\nPython’s extensive libraries simplify complex tasks.\n\nPopular for both beginner learning and professional development."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-use-cases-low-vs.-high-code",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#python-use-cases-low-vs.-high-code",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Python Use Cases: Low vs. High-Code",
    "text": "Python Use Cases: Low vs. High-Code\n\n\n\nLow-code leverages Python’s simplicity for fast data analysis.\n\nHigh-code applications involve deeper customization and algorithms.\n\nSocial sciences benefit from low-code data manipulation and visualization.\n\nComputer science projects often require complex, high-code solutions.\n\nPython accommodates both low-code and high-code workflows."
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#basic-python-syntax-variables-data-objects-loops",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-1-1-video-lecture-slides.html#basic-python-syntax-variables-data-objects-loops",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "Basic Python Syntax: Variables, Data Objects, Loops",
    "text": "Basic Python Syntax: Variables, Data Objects, Loops\n\n\n\nVariables store information and data types define their nature.\n\nLists, dictionaries, and tuples organize data into structures.\n\nLoops iterate over data to automate repetitive tasks.\n\nPython syntax emphasizes simplicity and readability.\n\nMastery of basic syntax is essential for further programming skills.\n\n\n\n\n\n💻"
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture.html",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture.html",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "",
    "text": "In the social sciences, data analysis takes many forms—including descriptive statistics, inferential testing, and exploratory modeling. Python provides a flexible and scalable environment for performing all of these, with a growing ecosystem tailored to academic and applied research. One emerging area is computational content analysis, where researchers apply Python tools to analyze text and visual content systematically. Libraries such as pandas, statsmodels, matplotlib, and scikit-learn enable everything from data wrangling to advanced modeling, making Python an essential tool for quantitative social scientists.\n\n\n\n\nArtificial intelligence—especially large language models—now supports researchers in performing quantitative analysis more efficiently. These models can generate Python code on demand for statistical tasks, allowing users to interact with their data through natural language. This reduces the technical barrier for non-programmers while enabling rapid hypothesis testing, data cleaning, and model generation. The balance between manual coding and AI-assisted scripting gives researchers more control and efficiency, particularly when iterating through complex analytical workflows.\n\n\n\n\nAI is also transforming how we approach content analysis. Large language models can assist in extracting meaning from text through classification, summarization, or topic modeling. Meanwhile, vision models like CLIP or ViT enable analysis of image and video content. Together, these tools support multimodal analysis, where textual and visual data are examined in tandem. Researchers can automate tasks such as entity recognition and theme detection, significantly speeding up the analysis of large, complex corpora while still allowing for nuanced interpretation.\n\n\n\n\nPython makes it easy to read and manipulate tabular data, especially using the pandas library. With AI-assisted tools, users can perform complex filtering, grouping, and aggregation with simple prompts rather than verbose code. This low-code approach is ideal for exploratory data analysis, especially in non-technical fields. Tools like pandasgui, datatable, or conversational agents like ChatGPT can generate, execute, and explain table operations in real-time. This democratizes access to data insights and facilitates reproducibility.\n\n\n\n\nTransforming data—through selection, filtering, and aggregation—is foundational to any analysis workflow. In Python, these operations are traditionally handled with pandas, but AI tools can now suggest or execute them via natural language instructions. This makes it easier for users to identify the variables they need, set conditions for filtering, and compute summaries like group means or totals. By comparing AI-generated queries to traditional code, learners gain a deeper understanding of the logic behind data manipulation.\n\n\n\n\nVisualization is a key part of understanding and communicating data. Python supports a wide range of basic plotting options, including bar charts, line graphs, and histograms. With AI assistance, users can describe the kind of graph they want and receive ready-to-run code. This enables quick generation of univariate and bivariate plots that explore single variables or pairwise relationships. Libraries such as matplotlib, seaborn, and plotnine provide powerful tools for crafting clear, insightful visualizations with minimal setup.\n\n\n\n\nWhen analysis involves more than two variables, visualization becomes more complex—and more powerful. Python offers tools to create multivariate plots like scatter matrices, heatmaps, and bubble charts. For dynamic exploration, libraries like plotly, altair, and holoviews enable interactive features such as zooming, filtering, and tooltip displays. AI support can refine these visualizations by adjusting parameters or recommending plot types based on data structure. This helps users engage deeply with high-dimensional data and uncover hidden patterns.",
    "crumbs": [
      "module 2",
      "lesson 2.2",
      "video lecture 2.2.1"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture.html#lesson-2.2-python-and-ai-aided-data-analysis",
    "href": "module02-introduction-to-low-code-python-programming-20pct/2-2-1-video-lecture.html#lesson-2.2-python-and-ai-aided-data-analysis",
    "title": "Module 2: Introduction to low-code Python programming",
    "section": "",
    "text": "In the social sciences, data analysis takes many forms—including descriptive statistics, inferential testing, and exploratory modeling. Python provides a flexible and scalable environment for performing all of these, with a growing ecosystem tailored to academic and applied research. One emerging area is computational content analysis, where researchers apply Python tools to analyze text and visual content systematically. Libraries such as pandas, statsmodels, matplotlib, and scikit-learn enable everything from data wrangling to advanced modeling, making Python an essential tool for quantitative social scientists.\n\n\n\n\nArtificial intelligence—especially large language models—now supports researchers in performing quantitative analysis more efficiently. These models can generate Python code on demand for statistical tasks, allowing users to interact with their data through natural language. This reduces the technical barrier for non-programmers while enabling rapid hypothesis testing, data cleaning, and model generation. The balance between manual coding and AI-assisted scripting gives researchers more control and efficiency, particularly when iterating through complex analytical workflows.\n\n\n\n\nAI is also transforming how we approach content analysis. Large language models can assist in extracting meaning from text through classification, summarization, or topic modeling. Meanwhile, vision models like CLIP or ViT enable analysis of image and video content. Together, these tools support multimodal analysis, where textual and visual data are examined in tandem. Researchers can automate tasks such as entity recognition and theme detection, significantly speeding up the analysis of large, complex corpora while still allowing for nuanced interpretation.\n\n\n\n\nPython makes it easy to read and manipulate tabular data, especially using the pandas library. With AI-assisted tools, users can perform complex filtering, grouping, and aggregation with simple prompts rather than verbose code. This low-code approach is ideal for exploratory data analysis, especially in non-technical fields. Tools like pandasgui, datatable, or conversational agents like ChatGPT can generate, execute, and explain table operations in real-time. This democratizes access to data insights and facilitates reproducibility.\n\n\n\n\nTransforming data—through selection, filtering, and aggregation—is foundational to any analysis workflow. In Python, these operations are traditionally handled with pandas, but AI tools can now suggest or execute them via natural language instructions. This makes it easier for users to identify the variables they need, set conditions for filtering, and compute summaries like group means or totals. By comparing AI-generated queries to traditional code, learners gain a deeper understanding of the logic behind data manipulation.\n\n\n\n\nVisualization is a key part of understanding and communicating data. Python supports a wide range of basic plotting options, including bar charts, line graphs, and histograms. With AI assistance, users can describe the kind of graph they want and receive ready-to-run code. This enables quick generation of univariate and bivariate plots that explore single variables or pairwise relationships. Libraries such as matplotlib, seaborn, and plotnine provide powerful tools for crafting clear, insightful visualizations with minimal setup.\n\n\n\n\nWhen analysis involves more than two variables, visualization becomes more complex—and more powerful. Python offers tools to create multivariate plots like scatter matrices, heatmaps, and bubble charts. For dynamic exploration, libraries like plotly, altair, and holoviews enable interactive features such as zooming, filtering, and tooltip displays. AI support can refine these visualizations by adjusting parameters or recommending plot types based on data structure. This helps users engage deeply with high-dimensional data and uncover hidden patterns.",
    "crumbs": [
      "module 2",
      "lesson 2.2",
      "video lecture 2.2.1"
    ]
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture.html",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture.html",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "",
    "text": "To understand sustainability communication, we need to define content both formally and functionally. Formally, we can consider sustainability content as any material that aligns with themes of environmental or social responsibility, such as climate action, resource use, or equity. Functionally, sustainability communication serves organizational purposes—such as shaping brand identity, complying with regulations, or engaging stakeholders. These functions affect how audiences perceive credibility and authenticity. Analyzing content through both lenses helps clarify its structure, intent, and potential impact.\n\n\n\n\nWhen collecting online sustainability communication for research, sampling methods shape the validity and reliability of findings. Random sampling is considered the gold standard in social science, but it may be difficult to apply in digital contexts. Instead, purposive or strategic sampling can ensure the inclusion of relevant or contrasting cases. Whichever method is used, researchers must assess whether their sample supports generalizable claims. A strong sampling design ensures that observed patterns are not merely artifacts of selection bias.\n\n\n\n\nSustainability communication can be studied using qualitative, quantitative, or mixed methods. Qualitative approaches explore the deeper meanings behind content, while quantitative methods enable comparisons and statistical testing. Descriptive studies reveal patterns and practices, while explanatory studies focus on why these patterns occur. For example, a linguistic comparison between high-impact and low-impact organizations might reveal different rhetorical strategies or terminology use. The process begins by formulating clear research questions and operationalizing them into testable hypotheses or coding schemes.\n\n\n\n\nThe first step in collecting sustainability communication is to select which organizations to study, guided by your research questions. You may focus on a single organization for depth or compare contrasting organizations for breadth. Official websites are a primary source of curated, strategic communication. Within these sites, researchers can collect the URLs of sustainability-related pages, often found under sections like “Sustainability,” “CSR,” or “Environment.” These links will form the basis of your content collection and further analysis.\n\n\n\n\nTo capture complex sustainability content online, one effective method is to print web pages to PDF using a browser. This approach preserves multimodal elements such as layout, images, and embedded links, which are essential for analyzing how messages are visually and textually constructed. It’s a fast and accessible technique that requires no special tools. However, a limitation is that videos or dynamic elements are not captured, making this best suited for static page content.\n\n\n\n\nAfter collecting the content, it’s important to store it in a way that supports reproducibility and easy access. Saving files locally is straightforward and works offline, but can be harder to share or back up. Using cloud platforms like Google Drive or GitHub offers better collaboration, version control, and accessibility. Regardless of the method, the goal is to make communication content systematically available for analysis, and to ensure it can be traced back to its source.\n\n\n\n\nOrganizing files into folders by organization or theme helps compare sustainability messaging across cases. Consistent file path naming is important for automating the analysis and maintaining structure. If content is saved as rendered PDFs, it may need to be converted into plain text later for textual analysis or coding. A clear folder structure simplifies navigation, supports reproducibility, and ensures that data management doesn’t become a barrier to insight.",
    "crumbs": [
      "module 1",
      "lesson 1.1",
      "video lecture 1.1.1"
    ]
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture.html#lesson-1.1-sustainability-communication-content",
    "href": "module01-understanding-sustainability-communication-content-10pct/1-1-1-video-lecture.html#lesson-1.1-sustainability-communication-content",
    "title": "Module 1: Understanding sustainability communication content",
    "section": "",
    "text": "To understand sustainability communication, we need to define content both formally and functionally. Formally, we can consider sustainability content as any material that aligns with themes of environmental or social responsibility, such as climate action, resource use, or equity. Functionally, sustainability communication serves organizational purposes—such as shaping brand identity, complying with regulations, or engaging stakeholders. These functions affect how audiences perceive credibility and authenticity. Analyzing content through both lenses helps clarify its structure, intent, and potential impact.\n\n\n\n\nWhen collecting online sustainability communication for research, sampling methods shape the validity and reliability of findings. Random sampling is considered the gold standard in social science, but it may be difficult to apply in digital contexts. Instead, purposive or strategic sampling can ensure the inclusion of relevant or contrasting cases. Whichever method is used, researchers must assess whether their sample supports generalizable claims. A strong sampling design ensures that observed patterns are not merely artifacts of selection bias.\n\n\n\n\nSustainability communication can be studied using qualitative, quantitative, or mixed methods. Qualitative approaches explore the deeper meanings behind content, while quantitative methods enable comparisons and statistical testing. Descriptive studies reveal patterns and practices, while explanatory studies focus on why these patterns occur. For example, a linguistic comparison between high-impact and low-impact organizations might reveal different rhetorical strategies or terminology use. The process begins by formulating clear research questions and operationalizing them into testable hypotheses or coding schemes.\n\n\n\n\nThe first step in collecting sustainability communication is to select which organizations to study, guided by your research questions. You may focus on a single organization for depth or compare contrasting organizations for breadth. Official websites are a primary source of curated, strategic communication. Within these sites, researchers can collect the URLs of sustainability-related pages, often found under sections like “Sustainability,” “CSR,” or “Environment.” These links will form the basis of your content collection and further analysis.\n\n\n\n\nTo capture complex sustainability content online, one effective method is to print web pages to PDF using a browser. This approach preserves multimodal elements such as layout, images, and embedded links, which are essential for analyzing how messages are visually and textually constructed. It’s a fast and accessible technique that requires no special tools. However, a limitation is that videos or dynamic elements are not captured, making this best suited for static page content.\n\n\n\n\nAfter collecting the content, it’s important to store it in a way that supports reproducibility and easy access. Saving files locally is straightforward and works offline, but can be harder to share or back up. Using cloud platforms like Google Drive or GitHub offers better collaboration, version control, and accessibility. Regardless of the method, the goal is to make communication content systematically available for analysis, and to ensure it can be traced back to its source.\n\n\n\n\nOrganizing files into folders by organization or theme helps compare sustainability messaging across cases. Consistent file path naming is important for automating the analysis and maintaining structure. If content is saved as rendered PDFs, it may need to be converted into plain text later for textual analysis or coding. A clear folder structure simplifies navigation, supports reproducibility, and ensures that data management doesn’t become a barrier to insight.",
    "crumbs": [
      "module 1",
      "lesson 1.1",
      "video lecture 1.1.1"
    ]
  }
]