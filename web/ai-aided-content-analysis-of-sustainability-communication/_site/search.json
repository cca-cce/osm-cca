[
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-2-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-2-computer-lab.html",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-2-computer-lab.qmd",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\n# Load token_df from the TSV file\ninput_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\ntoken_df = pd.read_csv(input_file_path, sep='\\t')\n\n# Filter the DataFrame to keep only rows where the part of speech is 'NOUN'\nnoun_df = token_df[token_df['token_pos'] == 'NOUN']\n\n# Group by the lemma and count the occurrences of each lemma\nlemma_counts = noun_df['token_lemma'].value_counts().reset_index()\n\n# Rename the columns for clarity\nlemma_counts.columns = ['lemma', 'count']\n\n# Get the 20 most frequent lemmas\ntop_lemmas = lemma_counts.head(20)\n\n# Plot the 20 most frequent nouns using Seaborn\nplt.figure(figsize=(10, 8))\nsns.barplot(x='count', y='lemma', data=top_lemmas, palette='viridis')\nplt.title('Top 20 Most Frequent Nouns')\nplt.xlabel('Count')\nplt.ylabel('Lemma')\n\n# Save the figure to a PNG file\noutput_file_path = '/content/osm-cca-nlp/fig/token_noun.png'\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/fig/token_noun.png'\nplt.savefig(output_file_path)\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "module 3",
      "lesson 3.3",
      "computer lab 3.3.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-2-computer-lab.html#summarize-and-visualize-data",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-2-computer-lab.html#summarize-and-visualize-data",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-3-2-computer-lab.qmd",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\n# Load token_df from the TSV file\ninput_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\ntoken_df = pd.read_csv(input_file_path, sep='\\t')\n\n# Filter the DataFrame to keep only rows where the part of speech is 'NOUN'\nnoun_df = token_df[token_df['token_pos'] == 'NOUN']\n\n# Group by the lemma and count the occurrences of each lemma\nlemma_counts = noun_df['token_lemma'].value_counts().reset_index()\n\n# Rename the columns for clarity\nlemma_counts.columns = ['lemma', 'count']\n\n# Get the 20 most frequent lemmas\ntop_lemmas = lemma_counts.head(20)\n\n# Plot the 20 most frequent nouns using Seaborn\nplt.figure(figsize=(10, 8))\nsns.barplot(x='count', y='lemma', data=top_lemmas, palette='viridis')\nplt.title('Top 20 Most Frequent Nouns')\nplt.xlabel('Count')\nplt.ylabel('Lemma')\n\n# Save the figure to a PNG file\noutput_file_path = '/content/osm-cca-nlp/fig/token_noun.png'\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/fig/token_noun.png'\nplt.savefig(output_file_path)\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "module 3",
      "lesson 3.3",
      "computer lab 3.3.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/index.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/index.html",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/index.qmd",
    "section": "",
    "text": "video lecture:\ndifferent types of NLP, ai-aided NLU, pos, ner, lemma\ntext content features are associated with, causes cognitive effects\ntext as unstructured data, unit of analysis\ntext sampling and types of text pdf, html, text, languages (en/se)\nexplain three levels of text analysis, quantitative analysis\n3 levels: descriptive, explorative, inferential\ncomputer lab: read text into dataframe, text cleaning, stopword removal\ncomputer lab: describe text stats (NLTK)\nprepare quiz:\nwhich of these word is a NLTK stopword\n\n\n\n\n\n\nvideo lecture:\ndifferent types of NLP, ai-aided NLU, pos, ner, lemma\ntext content features are associated with, causes cognitive effects\nexplain three levels of text analysis, quantitative analysis\n3 levels: descriptive, explorative, inferential\ntokenization, normalization, sentence level named entity recognition\ncomputer lab: sentence and word tokenization, dataframe data structure\ncomputer lab: inferential analysis of text using spacy ai models\nprepare quiz:\n\n\n\n\n\n\nvideo lecture:\ndifferent types of NLP, ai-aided NLU, pos, ner, lemma\ntext content features are associated with, causes cognitive effects\nopen science methods, reproducibility, transparency, collaboration, github\nplug in your own text content data in this course module\ninterate through module 3, lessons 1-3\nnext steps, compare sustainability communication by organizations, vectorization\ncomputer lab: low-code ai-aided summarize in google colab\ncomputer lab: low-code ai-aided visualize in google colab, content word frequency\nprepare quiz:",
    "crumbs": [
      "module 3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#overview-of-module-3-ca-3x3-hours-work",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/index.html#overview-of-module-3-ca-3x3-hours-work",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/index.qmd",
    "section": "",
    "text": "video lecture:\ndifferent types of NLP, ai-aided NLU, pos, ner, lemma\ntext content features are associated with, causes cognitive effects\ntext as unstructured data, unit of analysis\ntext sampling and types of text pdf, html, text, languages (en/se)\nexplain three levels of text analysis, quantitative analysis\n3 levels: descriptive, explorative, inferential\ncomputer lab: read text into dataframe, text cleaning, stopword removal\ncomputer lab: describe text stats (NLTK)\nprepare quiz:\nwhich of these word is a NLTK stopword\n\n\n\n\n\n\nvideo lecture:\ndifferent types of NLP, ai-aided NLU, pos, ner, lemma\ntext content features are associated with, causes cognitive effects\nexplain three levels of text analysis, quantitative analysis\n3 levels: descriptive, explorative, inferential\ntokenization, normalization, sentence level named entity recognition\ncomputer lab: sentence and word tokenization, dataframe data structure\ncomputer lab: inferential analysis of text using spacy ai models\nprepare quiz:\n\n\n\n\n\n\nvideo lecture:\ndifferent types of NLP, ai-aided NLU, pos, ner, lemma\ntext content features are associated with, causes cognitive effects\nopen science methods, reproducibility, transparency, collaboration, github\nplug in your own text content data in this course module\ninterate through module 3, lessons 1-3\nnext steps, compare sustainability communication by organizations, vectorization\ncomputer lab: low-code ai-aided summarize in google colab\ncomputer lab: low-code ai-aided visualize in google colab, content word frequency\nprepare quiz:",
    "crumbs": [
      "module 3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.qmd",
    "section": "",
    "text": "This code chunk outlines a procedure for processing a collection of text files using Python, with a focus on content analysis and natural language processing (NLP). Here’s a detailed explanation of each step:\nimport os\nimport pandas as pd\nimport re\n\n# Function to clean text by removing non-ASCII characters\ndef clean_text(text):\n    # Remove non-ASCII characters\n    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    return cleaned_text\n\n# Directory containing text files\ndirectory_path = '/content/osm-cca-nlp/res'\ndirectory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n\nImporting Necessary Libraries:\n\nThe code begins by importing essential Python libraries: os, pandas, and re. The os library is used for interacting with the file system, pandas is a powerful data manipulation library used to manage and analyze data structures like DataFrames, and re is the regular expressions library used for pattern matching and text processing.\n\nDefining a Text Cleaning Function:\n\nA function clean_text(text) is defined to clean the text data by removing non-ASCII characters. This function uses the re.sub() method to search the text for any character that does not fall within the ASCII range ([^\\x00-\\x7F]) and replaces it with an empty string, effectively removing these characters. This step is crucial in NLP tasks to standardize text data, making it easier to analyze.\n\nSetting Up the Directory Path:\n\nThe directory containing the text files is specified with directory_path = '/content/osm-cca-nlp/res'. This path directs the script to the location of the text files that will be processed.\n\n\n# Initialize an empty list to store the data\ndata = []\n\n# Initialize a unique ID counter\nunique_id = 1\n\n# Iterate over the text files in the directory\nfor filename in os.listdir(directory_path):\n    # Consider only plain text files\n    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n        file_path = os.path.join(directory_path, filename)\n        \n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n        \n        # Clean the text\n        cleaned_text = clean_text(text)\n        \n        # Append the data as a dictionary with a unique ID\n        data.append({\n            'id': unique_id,\n            'filename': filename,\n            'original_text': text,\n            'cleaned_text': cleaned_text\n        })\n        \n        # Increment the unique ID\n        unique_id += 1\n\nInitializing Data Structures:\n\nAn empty list data is initialized to store the cleaned data, and a unique_id counter is set to 1 to uniquely identify each text file. These data structures are essential for organizing and managing the extracted and cleaned content.\n\nIterating Over Files in the Directory:\n\nThe script iterates over each file in the specified directory using os.listdir(directory_path). A conditional statement if filename.endswith(\".txt\") or filename.endswith(\".md\") ensures that only plain text files (.txt) and Markdown files (.md) are processed. This step is fundamental in content analysis as it focuses the analysis on relevant document types.\n\nReading and Cleaning Text Content:\n\nFor each text file, the file is opened and read into a string variable text using open(file_path, 'r', encoding='utf-8'). The content is then passed to the clean_text() function to remove non-ASCII characters, resulting in a cleaned version of the text stored in cleaned_text. This step is crucial for preparing the text data for further NLP tasks by ensuring it is in a consistent and analyzable format.\n\n\n# Create a Pandas DataFrame\ntext_df = pd.DataFrame(data)\n\n# Save the DataFrame as a TSV file in the 'csv' subdirectory\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\noutput_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\n\n# Save the DataFrame to a TSV file\ntext_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the DataFrame\nprint(text_df)\n\nStoring the Data:\n\nThe original and cleaned text, along with the filename and unique ID, are stored as a dictionary in the data list. This structured storage is essential for keeping track of each document’s metadata and content, facilitating easy access and manipulation for later analysis.\n\nCreating a DataFrame:\n\nOnce all files are processed, the list data is converted into a pandas DataFrame using pd.DataFrame(data). This DataFrame organizes the collected data into a tabular format, where each row corresponds to a file, and columns represent the unique ID, filename, original text, and cleaned text. This step is critical in content analysis and NLP as it allows for systematic exploration, manipulation, and analysis of the textual data.\n\nDisplaying the Data:\n\nFinally, the DataFrame is printed to the console using print(df), providing a visual representation of the processed data. This allows for a quick inspection of the results, ensuring that the text cleaning and data aggregation processes have been correctly executed.\n\n\nThis procedure exemplifies a typical workflow in content analysis and NLP, where raw text data is cleaned, organized, and prepared for more sophisticated analytical tasks such as tokenization, entity recognition, or sentiment analysis. The use of Python and its libraries like pandas and re streamlines these tasks, making it easier to manage and analyze large collections of text.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#prepare-text-content",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#prepare-text-content",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.qmd",
    "section": "",
    "text": "This code chunk outlines a procedure for processing a collection of text files using Python, with a focus on content analysis and natural language processing (NLP). Here’s a detailed explanation of each step:\nimport os\nimport pandas as pd\nimport re\n\n# Function to clean text by removing non-ASCII characters\ndef clean_text(text):\n    # Remove non-ASCII characters\n    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    return cleaned_text\n\n# Directory containing text files\ndirectory_path = '/content/osm-cca-nlp/res'\ndirectory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n\nImporting Necessary Libraries:\n\nThe code begins by importing essential Python libraries: os, pandas, and re. The os library is used for interacting with the file system, pandas is a powerful data manipulation library used to manage and analyze data structures like DataFrames, and re is the regular expressions library used for pattern matching and text processing.\n\nDefining a Text Cleaning Function:\n\nA function clean_text(text) is defined to clean the text data by removing non-ASCII characters. This function uses the re.sub() method to search the text for any character that does not fall within the ASCII range ([^\\x00-\\x7F]) and replaces it with an empty string, effectively removing these characters. This step is crucial in NLP tasks to standardize text data, making it easier to analyze.\n\nSetting Up the Directory Path:\n\nThe directory containing the text files is specified with directory_path = '/content/osm-cca-nlp/res'. This path directs the script to the location of the text files that will be processed.\n\n\n# Initialize an empty list to store the data\ndata = []\n\n# Initialize a unique ID counter\nunique_id = 1\n\n# Iterate over the text files in the directory\nfor filename in os.listdir(directory_path):\n    # Consider only plain text files\n    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n        file_path = os.path.join(directory_path, filename)\n        \n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n        \n        # Clean the text\n        cleaned_text = clean_text(text)\n        \n        # Append the data as a dictionary with a unique ID\n        data.append({\n            'id': unique_id,\n            'filename': filename,\n            'original_text': text,\n            'cleaned_text': cleaned_text\n        })\n        \n        # Increment the unique ID\n        unique_id += 1\n\nInitializing Data Structures:\n\nAn empty list data is initialized to store the cleaned data, and a unique_id counter is set to 1 to uniquely identify each text file. These data structures are essential for organizing and managing the extracted and cleaned content.\n\nIterating Over Files in the Directory:\n\nThe script iterates over each file in the specified directory using os.listdir(directory_path). A conditional statement if filename.endswith(\".txt\") or filename.endswith(\".md\") ensures that only plain text files (.txt) and Markdown files (.md) are processed. This step is fundamental in content analysis as it focuses the analysis on relevant document types.\n\nReading and Cleaning Text Content:\n\nFor each text file, the file is opened and read into a string variable text using open(file_path, 'r', encoding='utf-8'). The content is then passed to the clean_text() function to remove non-ASCII characters, resulting in a cleaned version of the text stored in cleaned_text. This step is crucial for preparing the text data for further NLP tasks by ensuring it is in a consistent and analyzable format.\n\n\n# Create a Pandas DataFrame\ntext_df = pd.DataFrame(data)\n\n# Save the DataFrame as a TSV file in the 'csv' subdirectory\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\noutput_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\n\n# Save the DataFrame to a TSV file\ntext_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the DataFrame\nprint(text_df)\n\nStoring the Data:\n\nThe original and cleaned text, along with the filename and unique ID, are stored as a dictionary in the data list. This structured storage is essential for keeping track of each document’s metadata and content, facilitating easy access and manipulation for later analysis.\n\nCreating a DataFrame:\n\nOnce all files are processed, the list data is converted into a pandas DataFrame using pd.DataFrame(data). This DataFrame organizes the collected data into a tabular format, where each row corresponds to a file, and columns represent the unique ID, filename, original text, and cleaned text. This step is critical in content analysis and NLP as it allows for systematic exploration, manipulation, and analysis of the textual data.\n\nDisplaying the Data:\n\nFinally, the DataFrame is printed to the console using print(df), providing a visual representation of the processed data. This allows for a quick inspection of the results, ensuring that the text cleaning and data aggregation processes have been correctly executed.\n\n\nThis procedure exemplifies a typical workflow in content analysis and NLP, where raw text data is cleaned, organized, and prepared for more sophisticated analytical tasks such as tokenization, entity recognition, or sentiment analysis. The use of Python and its libraries like pandas and re streamlines these tasks, making it easier to manage and analyze large collections of text.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#code-chunk",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#code-chunk",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.qmd",
    "section": "code chunk",
    "text": "code chunk\n\nimport os\nimport pandas as pd\nimport re\n\n# Function to clean text by removing non-ASCII characters\ndef clean_text(text):\n    # Remove non-ASCII characters\n    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    return cleaned_text\n\n# Directory containing text files\ndirectory_path = '/content/osm-cca-nlp/res'\ndirectory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n\n# Initialize an empty list to store the data\ndata = []\n\n# Initialize a unique ID counter\nunique_id = 1\n\n# Iterate over the text files in the directory\nfor filename in os.listdir(directory_path):\n    # Consider only plain text files\n    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n        file_path = os.path.join(directory_path, filename)\n        \n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n        \n        # Clean the text\n        cleaned_text = clean_text(text)\n        \n        # Append the data as a dictionary with a unique ID\n        data.append({\n            'id': unique_id,\n            'filename': filename,\n            'original_text': text,\n            'cleaned_text': cleaned_text\n        })\n        \n        # Increment the unique ID\n        unique_id += 1\n\n# Create a Pandas DataFrame\ntext_df = pd.DataFrame(data)\n\n# Save the DataFrame as a TSV file in the 'csv' subdirectory\noutput_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\n\n# Save the DataFrame to a TSV file\ntext_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the DataFrame\nprint(text_df)",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/index.html",
    "href": "module02-introduction-to-low-code-python-programming-20pct/index.html",
    "title": "module02-introduction-to-low-code-python-programming-20pct/index.qmd",
    "section": "",
    "text": "video lecture:\ndifferent python environments, local, cloud, notebooks\ndifferent python use cases, social science vs computer science\nopen science methods, reproducibility, transparency, collaboration, github\ncomputer lab: walk-trough of google colab notebook user interface\ncomputer lab: trying some basic python syntax\nprepare quiz:\nwhat is the result of this python command\n\n\n\n\n\n\nvideo lecture:\ndifferent python environments, local, cloud, notebooks\ndifferent python use cases, social science vs computer science\ncomputer lab: low-code cda, structured data analysis using pandas, built in data\ncomputer lab: low-code cda, data visualization using matplotlib, seaborn\nprepare quiz:\nwhat are the column names of dataset iris",
    "crumbs": [
      "module 2"
    ]
  },
  {
    "objectID": "module02-introduction-to-low-code-python-programming-20pct/index.html#overview-of-module-2-ca-2x3-hours-work",
    "href": "module02-introduction-to-low-code-python-programming-20pct/index.html#overview-of-module-2-ca-2x3-hours-work",
    "title": "module02-introduction-to-low-code-python-programming-20pct/index.qmd",
    "section": "",
    "text": "video lecture:\ndifferent python environments, local, cloud, notebooks\ndifferent python use cases, social science vs computer science\nopen science methods, reproducibility, transparency, collaboration, github\ncomputer lab: walk-trough of google colab notebook user interface\ncomputer lab: trying some basic python syntax\nprepare quiz:\nwhat is the result of this python command\n\n\n\n\n\n\nvideo lecture:\ndifferent python environments, local, cloud, notebooks\ndifferent python use cases, social science vs computer science\ncomputer lab: low-code cda, structured data analysis using pandas, built in data\ncomputer lab: low-code cda, data visualization using matplotlib, seaborn\nprepare quiz:\nwhat are the column names of dataset iris",
    "crumbs": [
      "module 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "web/ai-aided-content-analysis-of-sustainability-communication",
    "section": "",
    "text": "video lecture:\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google slides browser tab\nstart google slides in presenter view\nread presentation script, stop screen recording\ncomputer lab:\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google colab browser tab\nexecute jupyter notebook cells and explain\nprepare quiz:\n\n\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "index.html#overview-of-mooc-course-ca-10-lessons-x-3-hours-work",
    "href": "index.html#overview-of-mooc-course-ca-10-lessons-x-3-hours-work",
    "title": "web/ai-aided-content-analysis-of-sustainability-communication",
    "section": "",
    "text": "video lecture:\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google slides browser tab\nstart google slides in presenter view\nread presentation script, stop screen recording\ncomputer lab:\nopen google chrome, access google isk account\ngo to microsoft teams, stream web app\nstart screen recording of google colab browser tab\nexecute jupyter notebook cells and explain\nprepare quiz:\n\n\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.html",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.qmd",
    "section": "",
    "text": "3-1-1-video-lecture.qmd",
    "crumbs": [
      "module 3",
      "lesson 3.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.html#video-lecture",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.html#video-lecture",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.qmd",
    "section": "",
    "text": "3-1-1-video-lecture.qmd",
    "crumbs": [
      "module 3",
      "lesson 3.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.html#computer-lab",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.html#computer-lab",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.qmd",
    "section": "computer lab",
    "text": "computer lab\n\n3-1-2-computer-lab.qmd\n\n\n\n\nIMAGE ALT TEXT HERE",
    "crumbs": [
      "module 3",
      "lesson 3.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.html#computer-lab-1",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.html#computer-lab-1",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-lesson.qmd",
    "section": "computer lab",
    "text": "computer lab\n\n3-1-3-computer-lab.qmd",
    "crumbs": [
      "module 3",
      "lesson 3.1"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.html",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.qmd",
    "section": "",
    "text": "import pandas as pd\nimport spacy\nimport os\n\n# Load text_df from the TSV file\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\ninput_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\ntext_df = pd.read_csv(input_file_path, sep='\\t')\n\n# Load the spaCy model (small English model is used here)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize an empty list to store sentence data\nsentence_data = []\n\n# Iterate over the cleaned text in the DataFrame\nfor index, row in text_df.iterrows():\n    doc = nlp(row['cleaned_text'])  # Process the cleaned text with spaCy\n    \n    # Iterate over the sentences in the document\n    for i, sentence in enumerate(doc.sents):\n        sentence_data.append({\n            'id': row['id'],           # Original text ID\n            'sentence_number': i + 1,  # Sentence number (starting from 1)\n            'sentence_text': sentence.text.strip()  # Sentence text\n        })\n\n# Create a new DataFrame with the sentence data\nsentence_df = pd.DataFrame(sentence_data)\n\n# Save the sentence_df DataFrame as a TSV file\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\noutput_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\nsentence_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the sentence DataFrame\nprint(sentence_df)",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "computer lab 3.2.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.html#inferential-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.html#inferential-analysis",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.qmd",
    "section": "",
    "text": "import pandas as pd\nimport spacy\nimport os\n\n# Load text_df from the TSV file\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\ninput_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\ntext_df = pd.read_csv(input_file_path, sep='\\t')\n\n# Load the spaCy model (small English model is used here)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize an empty list to store sentence data\nsentence_data = []\n\n# Iterate over the cleaned text in the DataFrame\nfor index, row in text_df.iterrows():\n    doc = nlp(row['cleaned_text'])  # Process the cleaned text with spaCy\n    \n    # Iterate over the sentences in the document\n    for i, sentence in enumerate(doc.sents):\n        sentence_data.append({\n            'id': row['id'],           # Original text ID\n            'sentence_number': i + 1,  # Sentence number (starting from 1)\n            'sentence_text': sentence.text.strip()  # Sentence text\n        })\n\n# Create a new DataFrame with the sentence data\nsentence_df = pd.DataFrame(sentence_data)\n\n# Save the sentence_df DataFrame as a TSV file\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\noutput_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\nsentence_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the sentence DataFrame\nprint(sentence_df)",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "computer lab 3.2.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.html#code-discussion",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.html#code-discussion",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-2-computer-lab.qmd",
    "section": "code discussion",
    "text": "code discussion\nThis code chunk outlines a procedure for processing text data stored in a TSV file using Python, with a specific focus on sentence extraction and content analysis through natural language processing (NLP) techniques.\n\nLoading the Text Data:\n\nThe process begins by loading a TSV file containing text data into a pandas DataFrame using pd.read_csv(). The file is located at a specified path (input_file_path), and the sep='\\t' parameter indicates that the file is tab-separated. This DataFrame, text_df, holds the cleaned text data along with associated metadata like unique IDs.\n\nInitializing the NLP Model:\n\nThe spaCy library, a powerful NLP tool, is loaded using spacy.load(\"en_core_web_sm\"). This initializes a small English model that will be used to process the text data, allowing for sophisticated linguistic analysis such as tokenization, lemmatization, and sentence segmentation.\n\nProcessing Text and Extracting Sentences:\n\nThe core of the procedure involves iterating over each row of the text_df DataFrame. For each row, the cleaned text is passed through the spaCy model, which processes the text and divides it into sentences (doc.sents). Each sentence is then extracted and stored in a list called sentence_data along with its corresponding unique ID and sentence number. This step is crucial for breaking down the text into manageable units, facilitating more granular content analysis.\n\nCreating and Saving the Sentence Data:\n\nAfter all sentences have been extracted, the sentence_data list is converted into a new DataFrame, sentence_df. This DataFrame organizes the sentences with their associated metadata, making it easier to analyze or manipulate the content on a sentence-by-sentence basis.\nThe sentence_df DataFrame is then saved as a TSV file at a specified output path (output_file_path) using sentence_df.to_csv(), ensuring the data is stored in a structured and accessible format for future use.\n\nDisplaying the Result:\n\nFinally, the sentence_df DataFrame is printed to the console, allowing for a quick inspection of the extracted sentence data. This step helps verify that the sentence extraction and data storage processes have been executed correctly.\n\n\nThis procedure effectively leverages Python and NLP techniques to transform raw text data into a structured format, breaking it down into sentences that can be further analyzed for various content analysis tasks. The use of spaCy allows for accurate sentence segmentation, which is a foundational step in many NLP workflows.",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "computer lab 3.2.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-3-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-3-computer-lab.html",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-3-computer-lab.qmd",
    "section": "",
    "text": "import pandas as pd\nimport spacy\nimport os\n\n# Load sentence_df from the TSV file\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\ninput_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\nsentence_df = pd.read_csv(input_file_path, sep='\\t')\n\n# Load the spaCy model (small English model is used here)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize an empty list to store token data\ntoken_data = []\n\n# Iterate over the sentences in the sentence_df DataFrame\nfor index, row in sentence_df.iterrows():\n    doc = nlp(row['sentence_text'])  # Process the sentence text with spaCy\n    \n    # Iterate over the tokens in the sentence\n    for j, token in enumerate(doc):\n        token_data.append({\n            'id': row['id'],                    # Original text ID\n            'sentence_number': row['sentence_number'],  # Sentence number\n            'token_number': j + 1,              # Token number (starting from 1)\n            'token_text': token.text,           # Token text\n            'token_lemma': token.lemma_,        # Token lemma\n            'token_pos': token.pos_,            # Token part of speech\n            'token_entity': token.ent_type_     # Token entity type (if any)\n        })\n\n# Create a new DataFrame with the token data\ntoken_df = pd.DataFrame(token_data)\n\n# Save the token_df DataFrame as a TSV file\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\noutput_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\ntoken_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the token DataFrame\nprint(token_df)",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "computer lab 3.2.3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-3-computer-lab.html#inferential-analysis-part-2",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-3-computer-lab.html#inferential-analysis-part-2",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-2-3-computer-lab.qmd",
    "section": "",
    "text": "import pandas as pd\nimport spacy\nimport os\n\n# Load sentence_df from the TSV file\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\ninput_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\nsentence_df = pd.read_csv(input_file_path, sep='\\t')\n\n# Load the spaCy model (small English model is used here)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize an empty list to store token data\ntoken_data = []\n\n# Iterate over the sentences in the sentence_df DataFrame\nfor index, row in sentence_df.iterrows():\n    doc = nlp(row['sentence_text'])  # Process the sentence text with spaCy\n    \n    # Iterate over the tokens in the sentence\n    for j, token in enumerate(doc):\n        token_data.append({\n            'id': row['id'],                    # Original text ID\n            'sentence_number': row['sentence_number'],  # Sentence number\n            'token_number': j + 1,              # Token number (starting from 1)\n            'token_text': token.text,           # Token text\n            'token_lemma': token.lemma_,        # Token lemma\n            'token_pos': token.pos_,            # Token part of speech\n            'token_entity': token.ent_type_     # Token entity type (if any)\n        })\n\n# Create a new DataFrame with the token data\ntoken_df = pd.DataFrame(token_data)\n\n# Save the token_df DataFrame as a TSV file\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\noutput_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\ntoken_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the token DataFrame\nprint(token_df)",
    "crumbs": [
      "module 3",
      "lesson 3.2",
      "computer lab 3.2.3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-3-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-3-computer-lab.html",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-3-computer-lab.qmd",
    "section": "",
    "text": "The provided code chunk reads a collection of text files from a specified directory, cleans the text by removing non-ASCII characters, and then stores the cleaned and original text in a pandas DataFrame along with a unique ID and the filename. The DataFrame is then printed, displaying the organized data for further analysis.\n\n\nTo extend the analysis, the following code chunk will add two new columns to the DataFrame: one for the word count and another for the character count of the cleaned text.\n# Perform word count and character count on each cleaned text in the DataFrame\ndf['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\ndf['character_count'] = df['cleaned_text'].apply(lambda x: len(x))\n\n# Select and print all columns except 'original_text' and 'cleaned_text'\ncolumns_to_display = df.columns.difference(['original_text', 'cleaned_text'])\nprint(df[columns_to_display])\n\n\n\n\nWord Count:\n\nThe apply function is used on the cleaned_text column to calculate the word count. The lambda function splits each cleaned text string into words using split() and then calculates the length of the resulting list using len(x.split()). This word count is stored in a new column word_count.\n\nCharacter Count:\n\nSimilarly, the apply function calculates the character count by applying len(x) directly on the cleaned_text string. This counts the total number of characters (including spaces) and stores the result in a new column character_count.\n\nUpdated DataFrame:\n\nThe DataFrame is then printed again, now including the word_count and character_count columns, providing additional insights into the text length and structure.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.3"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-3-computer-lab.html#descriptive-analysis",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-3-computer-lab.html#descriptive-analysis",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-3-computer-lab.qmd",
    "section": "",
    "text": "The provided code chunk reads a collection of text files from a specified directory, cleans the text by removing non-ASCII characters, and then stores the cleaned and original text in a pandas DataFrame along with a unique ID and the filename. The DataFrame is then printed, displaying the organized data for further analysis.\n\n\nTo extend the analysis, the following code chunk will add two new columns to the DataFrame: one for the word count and another for the character count of the cleaned text.\n# Perform word count and character count on each cleaned text in the DataFrame\ndf['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\ndf['character_count'] = df['cleaned_text'].apply(lambda x: len(x))\n\n# Select and print all columns except 'original_text' and 'cleaned_text'\ncolumns_to_display = df.columns.difference(['original_text', 'cleaned_text'])\nprint(df[columns_to_display])\n\n\n\n\nWord Count:\n\nThe apply function is used on the cleaned_text column to calculate the word count. The lambda function splits each cleaned text string into words using split() and then calculates the length of the resulting list using len(x.split()). This word count is stored in a new column word_count.\n\nCharacter Count:\n\nSimilarly, the apply function calculates the character count by applying len(x) directly on the cleaned_text string. This counts the total number of characters (including spaces) and stores the result in a new column character_count.\n\nUpdated DataFrame:\n\nThe DataFrame is then printed again, now including the word_count and character_count columns, providing additional insights into the text length and structure.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.3"
    ]
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/index.html",
    "href": "module01-understanding-sustainability-communication-content-10pct/index.html",
    "title": "module01-understanding-sustainability-communication-content-10pct/index.qmd",
    "section": "",
    "text": "video lecture:\ndefinition and relevance of sustainability communication (suscomm)\nbackground in quantitative content analysis, content sampling\ncomputational content analysis (cca), easier to scale up than manual\ncomputer lab: find, download suscomm on organizational websites\ncomputer lab: setting up a minimal, simple content database\nprepare quiz:\nreference book (Weder, Krainer, and Karmasin 2021)",
    "crumbs": [
      "module 1"
    ]
  },
  {
    "objectID": "module01-understanding-sustainability-communication-content-10pct/index.html#overview-of-module-1-ca-1x3-hours-work",
    "href": "module01-understanding-sustainability-communication-content-10pct/index.html#overview-of-module-1-ca-1x3-hours-work",
    "title": "module01-understanding-sustainability-communication-content-10pct/index.qmd",
    "section": "",
    "text": "video lecture:\ndefinition and relevance of sustainability communication (suscomm)\nbackground in quantitative content analysis, content sampling\ncomputational content analysis (cca), easier to scale up than manual\ncomputer lab: find, download suscomm on organizational websites\ncomputer lab: setting up a minimal, simple content database\nprepare quiz:\nreference book (Weder, Krainer, and Karmasin 2021)",
    "crumbs": [
      "module 1"
    ]
  }
]