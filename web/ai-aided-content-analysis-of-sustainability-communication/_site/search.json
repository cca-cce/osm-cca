[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "web/ai-aided-content-analysis-of-sustainability-communication",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "course start"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.qmd",
    "section": "",
    "text": "This code chunk outlines a procedure for processing a collection of text files using Python, with a focus on content analysis and natural language processing (NLP). Here’s a detailed explanation of each step:\n\nimport os\nimport pandas as pd\nimport re\n\n# Function to clean text by removing non-ASCII characters\ndef clean_text(text):\n    # Remove non-ASCII characters\n    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    return cleaned_text\n\n# Directory containing text files\ndirectory_path = '/content/osm-cca-nlp/res'\ndirectory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n\n\nImporting Necessary Libraries:\n\nThe code begins by importing essential Python libraries: os, pandas, and re. The os library is used for interacting with the file system, pandas is a powerful data manipulation library used to manage and analyze data structures like DataFrames, and re is the regular expressions library used for pattern matching and text processing.\n\nDefining a Text Cleaning Function:\n\nA function clean_text(text) is defined to clean the text data by removing non-ASCII characters. This function uses the re.sub() method to search the text for any character that does not fall within the ASCII range ([^\\x00-\\x7F]) and replaces it with an empty string, effectively removing these characters. This step is crucial in NLP tasks to standardize text data, making it easier to analyze.\n\nSetting Up the Directory Path:\n\nThe directory containing the text files is specified with directory_path = '/content/osm-cca-nlp/res'. This path directs the script to the location of the text files that will be processed.\n\n\n\n# Initialize an empty list to store the data\ndata = []\n\n# Initialize a unique ID counter\nunique_id = 1\n\n# Iterate over the text files in the directory\nfor filename in os.listdir(directory_path):\n    # Consider only plain text files\n    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n        file_path = os.path.join(directory_path, filename)\n        \n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n        \n        # Clean the text\n        cleaned_text = clean_text(text)\n        \n        # Append the data as a dictionary with a unique ID\n        data.append({\n            'id': unique_id,\n            'filename': filename,\n            'original_text': text,\n            'cleaned_text': cleaned_text\n        })\n        \n        # Increment the unique ID\n        unique_id += 1\n\n\nInitializing Data Structures:\n\nAn empty list data is initialized to store the cleaned data, and a unique_id counter is set to 1 to uniquely identify each text file. These data structures are essential for organizing and managing the extracted and cleaned content.\n\nIterating Over Files in the Directory:\n\nThe script iterates over each file in the specified directory using os.listdir(directory_path). A conditional statement if filename.endswith(\".txt\") or filename.endswith(\".md\") ensures that only plain text files (.txt) and Markdown files (.md) are processed. This step is fundamental in content analysis as it focuses the analysis on relevant document types.\n\nReading and Cleaning Text Content:\n\nFor each text file, the file is opened and read into a string variable text using open(file_path, 'r', encoding='utf-8'). The content is then passed to the clean_text() function to remove non-ASCII characters, resulting in a cleaned version of the text stored in cleaned_text. This step is crucial for preparing the text data for further NLP tasks by ensuring it is in a consistent and analyzable format.\n\n\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n\n   id                                           filename  \\\n0   1  strategy-sustainable-development-lund-universi...   \n1   2          lu-sustainability-communication-01-min.md   \n\n                                       original_text  \\\n0  Page 1\\n\\nSTRATEGY  \\n\\n19 September 2019  \\n\\...   \n1  Your browser has javascript turned off or bloc...   \n\n                                        cleaned_text  \n0  Page 1\\n\\nSTRATEGY  \\n\\n19 September 2019  \\n\\...  \n1  Your browser has javascript turned off or bloc...  \n\n\n\nStoring the Data:\n\nThe original and cleaned text, along with the filename and unique ID, are stored as a dictionary in the data list. This structured storage is essential for keeping track of each document’s metadata and content, facilitating easy access and manipulation for later analysis.\n\nCreating a DataFrame:\n\nOnce all files are processed, the list data is converted into a pandas DataFrame using pd.DataFrame(data). This DataFrame organizes the collected data into a tabular format, where each row corresponds to a file, and columns represent the unique ID, filename, original text, and cleaned text. This step is critical in content analysis and NLP as it allows for systematic exploration, manipulation, and analysis of the textual data.\n\nDisplaying the Data:\n\nFinally, the DataFrame is printed to the console using print(df), providing a visual representation of the processed data. This allows for a quick inspection of the results, ensuring that the text cleaning and data aggregation processes have been correctly executed.\n\n\nThis procedure exemplifies a typical workflow in content analysis and NLP, where raw text data is cleaned, organized, and prepared for more sophisticated analytical tasks such as tokenization, entity recognition, or sentiment analysis. The use of Python and its libraries like pandas and re streamlines these tasks, making it easier to manage and analyze large collections of text.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#prepare-text-content",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#prepare-text-content",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.qmd",
    "section": "",
    "text": "This code chunk outlines a procedure for processing a collection of text files using Python, with a focus on content analysis and natural language processing (NLP). Here’s a detailed explanation of each step:\n\nimport os\nimport pandas as pd\nimport re\n\n# Function to clean text by removing non-ASCII characters\ndef clean_text(text):\n    # Remove non-ASCII characters\n    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    return cleaned_text\n\n# Directory containing text files\ndirectory_path = '/content/osm-cca-nlp/res'\ndirectory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n\n\nImporting Necessary Libraries:\n\nThe code begins by importing essential Python libraries: os, pandas, and re. The os library is used for interacting with the file system, pandas is a powerful data manipulation library used to manage and analyze data structures like DataFrames, and re is the regular expressions library used for pattern matching and text processing.\n\nDefining a Text Cleaning Function:\n\nA function clean_text(text) is defined to clean the text data by removing non-ASCII characters. This function uses the re.sub() method to search the text for any character that does not fall within the ASCII range ([^\\x00-\\x7F]) and replaces it with an empty string, effectively removing these characters. This step is crucial in NLP tasks to standardize text data, making it easier to analyze.\n\nSetting Up the Directory Path:\n\nThe directory containing the text files is specified with directory_path = '/content/osm-cca-nlp/res'. This path directs the script to the location of the text files that will be processed.\n\n\n\n# Initialize an empty list to store the data\ndata = []\n\n# Initialize a unique ID counter\nunique_id = 1\n\n# Iterate over the text files in the directory\nfor filename in os.listdir(directory_path):\n    # Consider only plain text files\n    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n        file_path = os.path.join(directory_path, filename)\n        \n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n        \n        # Clean the text\n        cleaned_text = clean_text(text)\n        \n        # Append the data as a dictionary with a unique ID\n        data.append({\n            'id': unique_id,\n            'filename': filename,\n            'original_text': text,\n            'cleaned_text': cleaned_text\n        })\n        \n        # Increment the unique ID\n        unique_id += 1\n\n\nInitializing Data Structures:\n\nAn empty list data is initialized to store the cleaned data, and a unique_id counter is set to 1 to uniquely identify each text file. These data structures are essential for organizing and managing the extracted and cleaned content.\n\nIterating Over Files in the Directory:\n\nThe script iterates over each file in the specified directory using os.listdir(directory_path). A conditional statement if filename.endswith(\".txt\") or filename.endswith(\".md\") ensures that only plain text files (.txt) and Markdown files (.md) are processed. This step is fundamental in content analysis as it focuses the analysis on relevant document types.\n\nReading and Cleaning Text Content:\n\nFor each text file, the file is opened and read into a string variable text using open(file_path, 'r', encoding='utf-8'). The content is then passed to the clean_text() function to remove non-ASCII characters, resulting in a cleaned version of the text stored in cleaned_text. This step is crucial for preparing the text data for further NLP tasks by ensuring it is in a consistent and analyzable format.\n\n\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n\n   id                                           filename  \\\n0   1  strategy-sustainable-development-lund-universi...   \n1   2          lu-sustainability-communication-01-min.md   \n\n                                       original_text  \\\n0  Page 1\\n\\nSTRATEGY  \\n\\n19 September 2019  \\n\\...   \n1  Your browser has javascript turned off or bloc...   \n\n                                        cleaned_text  \n0  Page 1\\n\\nSTRATEGY  \\n\\n19 September 2019  \\n\\...  \n1  Your browser has javascript turned off or bloc...  \n\n\n\nStoring the Data:\n\nThe original and cleaned text, along with the filename and unique ID, are stored as a dictionary in the data list. This structured storage is essential for keeping track of each document’s metadata and content, facilitating easy access and manipulation for later analysis.\n\nCreating a DataFrame:\n\nOnce all files are processed, the list data is converted into a pandas DataFrame using pd.DataFrame(data). This DataFrame organizes the collected data into a tabular format, where each row corresponds to a file, and columns represent the unique ID, filename, original text, and cleaned text. This step is critical in content analysis and NLP as it allows for systematic exploration, manipulation, and analysis of the textual data.\n\nDisplaying the Data:\n\nFinally, the DataFrame is printed to the console using print(df), providing a visual representation of the processed data. This allows for a quick inspection of the results, ensuring that the text cleaning and data aggregation processes have been correctly executed.\n\n\nThis procedure exemplifies a typical workflow in content analysis and NLP, where raw text data is cleaned, organized, and prepared for more sophisticated analytical tasks such as tokenization, entity recognition, or sentiment analysis. The use of Python and its libraries like pandas and re streamlines these tasks, making it easier to manage and analyze large collections of text.",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  },
  {
    "objectID": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#code-chunk",
    "href": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.html#code-chunk",
    "title": "module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.qmd",
    "section": "code chunk",
    "text": "code chunk\n\nimport os\nimport pandas as pd\nimport re\n\n# Function to clean text by removing non-ASCII characters\ndef clean_text(text):\n    # Remove non-ASCII characters\n    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    return cleaned_text\n\n# Directory containing text files\ndirectory_path = '/content/osm-cca-nlp/res'\ndirectory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n\n# Initialize an empty list to store the data\ndata = []\n\n# Initialize a unique ID counter\nunique_id = 1\n\n# Iterate over the text files in the directory\nfor filename in os.listdir(directory_path):\n    # Consider only plain text files\n    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n        file_path = os.path.join(directory_path, filename)\n        \n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n        \n        # Clean the text\n        cleaned_text = clean_text(text)\n        \n        # Append the data as a dictionary with a unique ID\n        data.append({\n            'id': unique_id,\n            'filename': filename,\n            'original_text': text,\n            'cleaned_text': cleaned_text\n        })\n        \n        # Increment the unique ID\n        unique_id += 1\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n\n   id                                           filename  \\\n0   1  strategy-sustainable-development-lund-universi...   \n1   2          lu-sustainability-communication-01-min.md   \n\n                                       original_text  \\\n0  Page 1\\n\\nSTRATEGY  \\n\\n19 September 2019  \\n\\...   \n1  Your browser has javascript turned off or bloc...   \n\n                                        cleaned_text  \n0  Page 1\\n\\nSTRATEGY  \\n\\n19 September 2019  \\n\\...  \n1  Your browser has javascript turned off or bloc...",
    "crumbs": [
      "module 3",
      "lesson 3.1",
      "computer lab 3.1.2"
    ]
  }
]