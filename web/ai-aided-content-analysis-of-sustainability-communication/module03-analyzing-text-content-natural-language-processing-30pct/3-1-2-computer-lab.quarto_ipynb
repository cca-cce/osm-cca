{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"module03-analyzing-text-content-natural-language-processing-30pct/3-1-2-computer-lab.qmd\"\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## prepare text content\n",
        "\n",
        "This code chunk outlines a procedure for processing a collection of text files using Python, with a focus on content analysis and natural language processing (NLP). Hereâ€™s a detailed explanation of each step:\n",
        "\n",
        "```python\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to clean text by removing non-ASCII characters\n",
        "def clean_text(text):\n",
        "    # Remove non-ASCII characters\n",
        "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Directory containing text files\n",
        "directory_path = '/content/osm-cca-nlp/res'\n",
        "directory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n",
        "```\n",
        "\n",
        "1. **Importing Necessary Libraries**:\n",
        "   - The code begins by importing essential Python libraries: `os`, `pandas`, and `re`. The `os` library is used for interacting with the file system, `pandas` is a powerful data manipulation library used to manage and analyze data structures like DataFrames, and `re` is the regular expressions library used for pattern matching and text processing.\n",
        "\n",
        "2. **Defining a Text Cleaning Function**:\n",
        "   - A function `clean_text(text)` is defined to clean the text data by removing non-ASCII characters. This function uses the `re.sub()` method to search the text for any character that does not fall within the ASCII range (`[^\\x00-\\x7F]`) and replaces it with an empty string, effectively removing these characters. This step is crucial in NLP tasks to standardize text data, making it easier to analyze.\n",
        "\n",
        "3. **Setting Up the Directory Path**:\n",
        "   - The directory containing the text files is specified with `directory_path = '/content/osm-cca-nlp/res'`. This path directs the script to the location of the text files that will be processed.\n",
        "\n",
        "```python\n",
        "# Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Initialize a unique ID counter\n",
        "unique_id = 1\n",
        "\n",
        "# Iterate over the text files in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    # Consider only plain text files\n",
        "    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        \n",
        "        # Read the file content\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        \n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(text)\n",
        "        \n",
        "        # Append the data as a dictionary with a unique ID\n",
        "        data.append({\n",
        "            'id': unique_id,\n",
        "            'filename': filename,\n",
        "            'original_text': text,\n",
        "            'cleaned_text': cleaned_text\n",
        "        })\n",
        "        \n",
        "        # Increment the unique ID\n",
        "        unique_id += 1\n",
        "```\n",
        "\n",
        "4. **Initializing Data Structures**:\n",
        "   - An empty list `data` is initialized to store the cleaned data, and a `unique_id` counter is set to `1` to uniquely identify each text file. These data structures are essential for organizing and managing the extracted and cleaned content.\n",
        "\n",
        "5. **Iterating Over Files in the Directory**:\n",
        "   - The script iterates over each file in the specified directory using `os.listdir(directory_path)`. A conditional statement `if filename.endswith(\".txt\") or filename.endswith(\".md\")` ensures that only plain text files (`.txt`) and Markdown files (`.md`) are processed. This step is fundamental in content analysis as it focuses the analysis on relevant document types.\n",
        "\n",
        "6. **Reading and Cleaning Text Content**:\n",
        "   - For each text file, the file is opened and read into a string variable `text` using `open(file_path, 'r', encoding='utf-8')`. The content is then passed to the `clean_text()` function to remove non-ASCII characters, resulting in a cleaned version of the text stored in `cleaned_text`. This step is crucial for preparing the text data for further NLP tasks by ensuring it is in a consistent and analyzable format.\n",
        "\n",
        "```python\n",
        "# Create a Pandas DataFrame\n",
        "text_df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame as a TSV file in the 'csv' subdirectory\n",
        "output_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\n",
        "output_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\n",
        "\n",
        "# Save the DataFrame to a TSV file\n",
        "text_df.to_csv(output_file_path, sep='\\t', index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(text_df)\n",
        "```\n",
        "\n",
        "7. **Storing the Data**:\n",
        "   - The original and cleaned text, along with the filename and unique ID, are stored as a dictionary in the `data` list. This structured storage is essential for keeping track of each document's metadata and content, facilitating easy access and manipulation for later analysis.\n",
        "\n",
        "8. **Creating a DataFrame**:\n",
        "   - Once all files are processed, the list `data` is converted into a `pandas` DataFrame using `pd.DataFrame(data)`. This DataFrame organizes the collected data into a tabular format, where each row corresponds to a file, and columns represent the unique ID, filename, original text, and cleaned text. This step is critical in content analysis and NLP as it allows for systematic exploration, manipulation, and analysis of the textual data.\n",
        "\n",
        "9. **Displaying the Data**:\n",
        "   - Finally, the DataFrame is printed to the console using `print(df)`, providing a visual representation of the processed data. This allows for a quick inspection of the results, ensuring that the text cleaning and data aggregation processes have been correctly executed.\n",
        "\n",
        "This procedure exemplifies a typical workflow in content analysis and NLP, where raw text data is cleaned, organized, and prepared for more sophisticated analytical tasks such as tokenization, entity recognition, or sentiment analysis. The use of Python and its libraries like `pandas` and `re` streamlines these tasks, making it easier to manage and analyze large collections of text.\n",
        "\n",
        "## code chunk\n"
      ],
      "id": "b50092dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to clean text by removing non-ASCII characters\n",
        "def clean_text(text):\n",
        "    # Remove non-ASCII characters\n",
        "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Directory containing text files\n",
        "directory_path = '/content/osm-cca-nlp/res'\n",
        "directory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Initialize a unique ID counter\n",
        "unique_id = 1\n",
        "\n",
        "# Iterate over the text files in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    # Consider only plain text files\n",
        "    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        \n",
        "        # Read the file content\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        \n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(text)\n",
        "        \n",
        "        # Append the data as a dictionary with a unique ID\n",
        "        data.append({\n",
        "            'id': unique_id,\n",
        "            'filename': filename,\n",
        "            'original_text': text,\n",
        "            'cleaned_text': cleaned_text\n",
        "        })\n",
        "        \n",
        "        # Increment the unique ID\n",
        "        unique_id += 1\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "text_df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame as a TSV file in the 'csv' subdirectory\n",
        "output_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\n",
        "output_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\n",
        "\n",
        "# Save the DataFrame to a TSV file\n",
        "text_df.to_csv(output_file_path, sep='\\t', index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(text_df)"
      ],
      "id": "390fd58b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}